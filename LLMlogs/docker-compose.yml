services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - log-streaming
      
  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "logs:1:1,raw-logs:1:1,classified-logs:1:1,anomalies:1:1"
    depends_on:
      - zookeeper
    networks:
      - log-streaming
      
  victoria-metrics:
    image: victoriametrics/victoria-metrics:latest
    ports:
      - "8428:8428"
    volumes:
      - victoria-metrics-data:/victoria-metrics-data
    networks:
      - log-streaming
      
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - victoria-metrics
    networks:
      - log-streaming
      
  log-producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    command: python log_producer.py
    environment:
      - PYTHONUNBUFFERED=1
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9093}
    depends_on:
      - kafka
    networks:
      - log-streaming
      
  log-consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    restart: on-failure
    volumes:
      - ./xgboost_model.pkl:/app/xgboost_model.pkl
      - ./vectorizer.pkl:/app/vectorizer.pkl
      - ./label_encoder.pkl:/app/label_encoder.pkl
    depends_on:
      - kafka
      - victoria-metrics
    networks:
      - log-streaming
    environment:
      - PYTHONUNBUFFERED=1
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9093}
      - RAW_LOGS_TOPIC=${RAW_LOGS_TOPIC:-raw-logs}
      - CLASSIFIED_LOGS_TOPIC=${CLASSIFIED_LOGS_TOPIC:-classified-logs}
      
  anomaly-detector:
    build:
      context: .
      dockerfile: Dockerfile.anomaly_detector
    depends_on:
      - victoria-metrics
    networks:
      - log-streaming
    environment:
      - PYTHONUNBUFFERED=1
      - GRAFANA_API_KEY=${GRAFANA_API_KEY}
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9093}
      - ANOMALIES_TOPIC=${ANOMALIES_TOPIC:-anomalies}
      
  aws-log-poller:
    build:
      context: .
      dockerfile: Dockerfile.aws_log_poller
    depends_on:
      - kafka
    networks:
      - log-streaming
    volumes:
      - aws-checkpoints:/app/checkpoints
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9093}
      - RAW_LOGS_TOPIC=${RAW_LOGS_TOPIC:-raw-logs}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - AWS_LOG_GROUPS=${AWS_LOG_GROUPS}
      - AWS_POLL_INTERVAL_SECONDS=${AWS_POLL_INTERVAL_SECONDS:-30}
      - MAX_RETRIES=${MAX_RETRIES:-5}
      - RETRY_DELAY_SECONDS=${RETRY_DELAY_SECONDS:-5}
    profiles:
      - aws
      
  azure-log-poller:
    build:
      context: .
      dockerfile: Dockerfile.azure_log_poller
    depends_on:
      - kafka
    networks:
      - log-streaming
    volumes:
      - azure-checkpoints:/app/checkpoints
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9093}
      - RAW_LOGS_TOPIC=${RAW_LOGS_TOPIC:-raw-logs}
      - AZURE_TENANT_ID=${AZURE_TENANT_ID}
      - AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
      - AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
      - AZURE_WORKSPACE_ID=${AZURE_WORKSPACE_ID}
      - AZURE_QUERY=${AZURE_QUERY:-union * | where TimeGenerated > ago(1h) | order by TimeGenerated desc | limit 100}
      - AZURE_POLL_INTERVAL_SECONDS=${AZURE_POLL_INTERVAL_SECONDS:-60}
      - MAX_RETRIES=${MAX_RETRIES:-5}
      - RETRY_DELAY_SECONDS=${RETRY_DELAY_SECONDS:-5}
    profiles:
      - azure
      
  notifier:
    build:
      context: .
      dockerfile: Dockerfile.notifier
    depends_on:
      - kafka
    networks:
      - log-streaming
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9093}
      - CLASSIFIED_LOGS_TOPIC=${CLASSIFIED_LOGS_TOPIC:-classified-logs}
      - ANOMALIES_TOPIC=${ANOMALIES_TOPIC:-anomalies}
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SMTP_USE_TLS=${SMTP_USE_TLS:-true}
      - EMAIL_FROM=${EMAIL_FROM}
      - EMAIL_RECIPIENTS=${EMAIL_RECIPIENTS}
      - TEAMS_WEBHOOK_URL=${TEAMS_WEBHOOK_URL}
      - ANOMALY_THRESHOLD=${ANOMALY_THRESHOLD:-0.8}
      - DEDUP_WINDOW_MINUTES=${DEDUP_WINDOW_MINUTES:-15}
      - SEND_PREVENTIVE_EMAILS=${SEND_PREVENTIVE_EMAILS:-false}
      - GRAFANA_BASE_URL=${GRAFANA_BASE_URL:-http://localhost:3000}

  # === LOKI INTEGRATION SERVICES ===
  
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - log-streaming
    profiles:
      - aws-loki
      - azure-loki
      - loki

  promtail:
    image: grafana/promtail:2.9.0
    ports:
      - "9080:9080"
    volumes:
      - ./promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - log-streaming
    profiles:
      - aws-loki
      - azure-loki
      - loki

  loki-kafka-forwarder:
    build:
      context: .
      dockerfile: Dockerfile.loki_forwarder
    depends_on:
      - loki
      - kafka
    networks:
      - log-streaming
    environment:
      - LOKI_URL=http://loki:3100
      - KAFKA_SERVERS=kafka:9093
      - KAFKA_TOPIC=raw-logs
      - BATCH_SIZE=${LOKI_BATCH_SIZE:-100}
      - RATE_LIMIT=${LOKI_RATE_LIMIT:-1000}
      - QUERY_INTERVAL=${LOKI_QUERY_INTERVAL:-10}
    profiles:
      - aws-loki
      - azure-loki
      - loki

  aws-log-poller-loki:
    build:
      context: .
      dockerfile: Dockerfile.aws_log_poller_loki
    depends_on:
      - loki
    networks:
      - log-streaming
    volumes:
      - aws-loki-checkpoints:/tmp
    environment:
      - LOKI_URL=http://loki:3100
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - AWS_LOG_GROUPS=${AWS_LOG_GROUPS}
      - AWS_POLL_INTERVAL_SECONDS=${AWS_POLL_INTERVAL_SECONDS:-30}
      - AWS_BATCH_SIZE=${AWS_BATCH_SIZE:-100}
    profiles:
      - aws-loki

  # === MONITORING SERVICES ===
  
  prometheus:
    image: prom/prometheus:v2.45.0
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus-alert-rules.yml:/etc/prometheus/alert_rules.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    networks:
      - log-streaming
    profiles:
      - monitoring
      - aws-loki
      - azure-loki

  alertmanager:
    image: prom/alertmanager:v0.25.0
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
    networks:
      - log-streaming
    profiles:
      - monitoring
      - aws-loki
      - azure-loki

networks:
  log-streaming:
    driver: bridge

volumes:
  victoria-metrics-data:
  grafana-data:
  aws-checkpoints:
  azure-checkpoints:
  loki-data:
  aws-loki-checkpoints:
  prometheus-data: