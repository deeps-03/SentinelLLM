{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt2LpbIKB2KB",
        "outputId": "2232393f-6b43-4cf0-9635-3b6b7160c238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SentinelLLM'...\n",
            "remote: Enumerating objects: 155, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 155 (delta 78), reused 131 (delta 55), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (155/155), 375.17 KiB | 7.36 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b LLMtest https://github.com/deeps-03/SentinelLLM.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoDNChDfDYkG",
        "outputId": "75304205-3280-4b89-ce59-9d40ed84d44c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SentinelLLM\n"
          ]
        }
      ],
      "source": [
        "%cd SentinelLLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueKuflGoD1H7",
        "outputId": "f3e01b0b-84db-43bd-a912-4d4a1cc7fe6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kafka-python (from -r LLMlogs/requirements.txt (line 1))\n",
            "  Using cached kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 5)) (1.1.1)\n",
            "Collecting boto3 (from -r LLMlogs/requirements.txt (line 6))\n",
            "  Using cached boto3-1.40.25-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting azure-monitor-query (from -r LLMlogs/requirements.txt (line 7))\n",
            "  Using cached azure_monitor_query-2.0.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting azure-identity (from -r LLMlogs/requirements.txt (line 8))\n",
            "  Using cached azure_identity-1.24.0-py3-none-any.whl.metadata (86 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 9)) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 10)) (4.56.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 11)) (0.3.27)\n",
            "Collecting langchain-community (from -r LLMlogs/requirements.txt (line 12))\n",
            "  Using cached langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting llama-cpp-python (from -r LLMlogs/requirements.txt (line 13))\n",
            "  Using cached llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 14)) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 15)) (1.6.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 16)) (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 17)) (0.17.1)\n",
            "Collecting trl (from -r LLMlogs/requirements.txt (line 18))\n",
            "  Using cached trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r LLMlogs/requirements.txt (line 19)) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r LLMlogs/requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r LLMlogs/requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r LLMlogs/requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r LLMlogs/requirements.txt (line 3)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r LLMlogs/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r LLMlogs/requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r LLMlogs/requirements.txt (line 3)) (2025.8.3)\n",
            "Collecting botocore<1.41.0,>=1.40.25 (from boto3->-r LLMlogs/requirements.txt (line 6))\n",
            "  Using cached botocore-1.40.25-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->-r LLMlogs/requirements.txt (line 6))\n",
            "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->-r LLMlogs/requirements.txt (line 6))\n",
            "  Using cached s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-monitor-query->-r LLMlogs/requirements.txt (line 7))\n",
            "  Using cached isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core>=1.30.0 (from azure-monitor-query->-r LLMlogs/requirements.txt (line 7))\n",
            "  Using cached azure_core-1.35.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-monitor-query->-r LLMlogs/requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.12/dist-packages (from azure-identity->-r LLMlogs/requirements.txt (line 8)) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity->-r LLMlogs/requirements.txt (line 8))\n",
            "  Using cached msal-1.33.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity->-r LLMlogs/requirements.txt (line 8))\n",
            "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r LLMlogs/requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r LLMlogs/requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain->-r LLMlogs/requirements.txt (line 11)) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain->-r LLMlogs/requirements.txt (line 11)) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain->-r LLMlogs/requirements.txt (line 11)) (0.4.23)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain->-r LLMlogs/requirements.txt (line 11)) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain->-r LLMlogs/requirements.txt (line 11)) (2.0.43)\n",
            "Collecting requests (from -r LLMlogs/requirements.txt (line 3))\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r LLMlogs/requirements.txt (line 12)) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r LLMlogs/requirements.txt (line 12)) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community->-r LLMlogs/requirements.txt (line 12))\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r LLMlogs/requirements.txt (line 12)) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r LLMlogs/requirements.txt (line 12)) (0.4.1)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python->-r LLMlogs/requirements.txt (line 13))\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost->-r LLMlogs/requirements.txt (line 14)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r LLMlogs/requirements.txt (line 15)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r LLMlogs/requirements.txt (line 15)) (3.6.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r LLMlogs/requirements.txt (line 16)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r LLMlogs/requirements.txt (line 16)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r LLMlogs/requirements.txt (line 16)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r LLMlogs/requirements.txt (line 16)) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft->-r LLMlogs/requirements.txt (line 17)) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r LLMlogs/requirements.txt (line 12)) (1.20.1)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.0->azure-monitor-query->-r LLMlogs/requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.5->azure-identity->-r LLMlogs/requirements.txt (line 8)) (1.17.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community->-r LLMlogs/requirements.txt (line 12))\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community->-r LLMlogs/requirements.txt (line 12))\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r LLMlogs/requirements.txt (line 10)) (1.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r LLMlogs/requirements.txt (line 9)) (3.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r LLMlogs/requirements.txt (line 11)) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (0.24.0)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r LLMlogs/requirements.txt (line 8)) (2.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r LLMlogs/requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r LLMlogs/requirements.txt (line 11)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r LLMlogs/requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r LLMlogs/requirements.txt (line 11)) (3.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r LLMlogs/requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->-r LLMlogs/requirements.txt (line 8)) (2.22)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain->-r LLMlogs/requirements.txt (line 11)) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community->-r LLMlogs/requirements.txt (line 12))\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain->-r LLMlogs/requirements.txt (line 11)) (1.3.1)\n",
            "Using cached kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
            "Using cached boto3-1.40.25-py3-none-any.whl (139 kB)\n",
            "Using cached azure_monitor_query-2.0.0-py3-none-any.whl (71 kB)\n",
            "Using cached azure_identity-1.24.0-py3-none-any.whl (187 kB)\n",
            "Using cached langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached trl-0.22.2-py3-none-any.whl (544 kB)\n",
            "Using cached azure_core-1.35.0-py3-none-any.whl (210 kB)\n",
            "Using cached botocore-1.40.25-py3-none-any.whl (14.0 MB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Using cached isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Using cached msal-1.33.0-py3-none-any.whl (116 kB)\n",
            "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Using cached s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r LLMlogs/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzK3pmiVElD3",
        "outputId": "77ddd0c1-72d5-4bdc-cf77-e099c22ce737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.47.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m0BHPN6LEwnK",
        "outputId": "7741c995-f83f-43c8-fc2a-5d6251f08ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.42.4\n",
            "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (0.34.4)\n",
            "Collecting numpy<2.0,>=1.17 (from transformers==4.42.4)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (0.6.2)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.4)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (2025.8.3)\n",
            "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tokenizers, transformers\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.0\n",
            "    Uninstalling tokenizers-0.22.0:\n",
            "      Successfully uninstalled tokenizers-0.22.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.0\n",
            "    Uninstalling transformers-4.56.0:\n",
            "      Successfully uninstalled transformers-4.56.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 tokenizers-0.19.1 transformers-4.42.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "57678306698c4f6587b0edcf67b5d91a",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets==2.20.0\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.70.16)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 fsspec-2024.5.0 pyarrow-hotfix-0.7\n",
            "Collecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (0.6.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2024.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.11.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.11.1) (0.19.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2025.8.3)\n",
            "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "Successfully installed peft-0.11.1\n",
            "Collecting trl==0.9.6\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (4.42.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (1.26.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (1.10.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.9.6) (2.20.0)\n",
            "Collecting tyro>=0.5.11 (from trl==0.9.6)\n",
            "  Downloading tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2024.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->trl==0.9.6) (4.67.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.6)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->trl==0.9.6) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.9.6) (3.12.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.31.0->trl==0.9.6) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.9.6) (2025.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.17.0)\n",
            "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.31-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro, trl\n",
            "Successfully installed shtab-1.7.2 trl-0.9.6 tyro-0.9.31\n",
            "Collecting accelerate==0.33.0\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2025.8.3)\n",
            "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.1\n",
            "    Uninstalling accelerate-1.10.1:\n",
            "      Successfully uninstalled accelerate-1.10.1\n",
            "Successfully installed accelerate-0.33.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.42.4\n",
        "!pip install datasets==2.20.0\n",
        "!pip install peft==0.11.1\n",
        "!pip install trl==0.9.6\n",
        "!pip install accelerate==0.33.0\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yYn_FvWFBy5",
        "outputId": "1705708d-599e-4d40-9a74-745ff4f20560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-06 08:11:36.046619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757146296.067934    3543 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757146296.074309    3543 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757146296.091025    3543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757146296.091049    3543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757146296.091053    3543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757146296.091056    3543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-06 08:11:36.096116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading dataset from generated_logs.jsonl...\n",
            "Generating train split: 10000 examples [00:00, 203076.63 examples/s]\n",
            "Map: 100% 10000/10000 [00:00<00:00, 10615.13 examples/s]\n",
            "Loading model and tokenizer for 'Qwen/Qwen2-1.5B-Instruct'...\n",
            "tokenizer_config.json: 1.29kB [00:00, 2.82MB/s]\n",
            "vocab.json: 2.78MB [00:00, 88.1MB/s]\n",
            "merges.txt: 1.67MB [00:00, 129MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 181MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 660/660 [00:00<00:00, 5.39MB/s]\n",
            "model.safetensors: 100% 3.09G/3.09G [00:51<00:00, 59.8MB/s]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 2.30MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Map: 100% 10000/10000 [00:03<00:00, 2608.17 examples/s]\n",
            "🚀 Starting fresh training...\n",
            "  0% 0/5000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "{'loss': 0.7502, 'grad_norm': 0.2119140625, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
            "{'loss': 0.2225, 'grad_norm': 0.1884765625, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
            "{'loss': 0.2407, 'grad_norm': 0.16015625, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
            "{'loss': 0.1357, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
            "{'loss': 0.1463, 'grad_norm': 0.1328125, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
            "{'loss': 0.1072, 'grad_norm': 0.1181640625, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
            "{'loss': 0.1664, 'grad_norm': 0.12109375, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
            "{'loss': 0.0833, 'grad_norm': 0.2314453125, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
            "{'loss': 0.1318, 'grad_norm': 0.1103515625, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
            "{'loss': 0.084, 'grad_norm': 0.10693359375, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
            "{'loss': 0.1173, 'grad_norm': 0.09130859375, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
            "{'loss': 0.0711, 'grad_norm': 0.126953125, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
            "{'loss': 0.1073, 'grad_norm': 0.064453125, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
            "{'loss': 0.075, 'grad_norm': 0.1279296875, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
            "{'loss': 0.0992, 'grad_norm': 0.0615234375, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
            "{'loss': 0.0644, 'grad_norm': 0.09814453125, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
            "{'loss': 0.1018, 'grad_norm': 0.0869140625, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
            "{'loss': 0.066, 'grad_norm': 0.087890625, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
            "{'loss': 0.102, 'grad_norm': 0.0751953125, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
            "{'loss': 0.0654, 'grad_norm': 0.076171875, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
            "{'loss': 0.1008, 'grad_norm': 0.08544921875, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
            "{'loss': 0.0708, 'grad_norm': 0.0732421875, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
            "{'loss': 0.0953, 'grad_norm': 0.09033203125, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
            "{'loss': 0.065, 'grad_norm': 0.072265625, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
            "{'loss': 0.0956, 'grad_norm': 0.1943359375, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
            "{'loss': 0.0609, 'grad_norm': 0.07177734375, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
            "{'loss': 0.0879, 'grad_norm': 0.06298828125, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
            "{'loss': 0.0641, 'grad_norm': 0.05810546875, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
            "{'loss': 0.1019, 'grad_norm': 0.04833984375, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
            "{'loss': 0.0621, 'grad_norm': 0.062255859375, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
            "{'loss': 0.0901, 'grad_norm': 0.056640625, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
            "{'loss': 0.0656, 'grad_norm': 0.060791015625, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
            "{'loss': 0.0964, 'grad_norm': 0.0888671875, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
            "{'loss': 0.0583, 'grad_norm': 0.054443359375, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
            "{'loss': 0.0895, 'grad_norm': 0.044189453125, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
            "{'loss': 0.0588, 'grad_norm': 0.0791015625, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
            "{'loss': 0.0928, 'grad_norm': 0.050048828125, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
            "{'loss': 0.0612, 'grad_norm': 0.0771484375, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
            "{'loss': 0.0959, 'grad_norm': 0.062255859375, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
            "{'loss': 0.0602, 'grad_norm': 0.068359375, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
            "{'loss': 0.087, 'grad_norm': 0.06396484375, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
            "{'loss': 0.0596, 'grad_norm': 0.061279296875, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
            "{'loss': 0.0827, 'grad_norm': 0.04638671875, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
            "{'loss': 0.062, 'grad_norm': 0.06884765625, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
            "{'loss': 0.0932, 'grad_norm': 0.05322265625, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
            "{'loss': 0.0574, 'grad_norm': 0.06640625, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
            "{'loss': 0.1006, 'grad_norm': 0.07080078125, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
            "{'loss': 0.0583, 'grad_norm': 0.06298828125, 'learning_rate': 0.0002, 'epoch': 0.48}\n",
            "{'loss': 0.0882, 'grad_norm': 0.048095703125, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
            "{'loss': 0.06, 'grad_norm': 0.06689453125, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
            "{'loss': 0.0937, 'grad_norm': 0.058837890625, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
            "{'loss': 0.0619, 'grad_norm': 0.05859375, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
            "{'loss': 0.0935, 'grad_norm': 0.05224609375, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
            "{'loss': 0.0597, 'grad_norm': 0.04638671875, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
            "{'loss': 0.0919, 'grad_norm': 0.052978515625, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
            "{'loss': 0.0589, 'grad_norm': 0.03857421875, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
            "{'loss': 0.0865, 'grad_norm': 0.038330078125, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
            "{'loss': 0.056, 'grad_norm': 0.06494140625, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
            "{'loss': 0.0906, 'grad_norm': 0.042724609375, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
            "{'loss': 0.0594, 'grad_norm': 0.048828125, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
            "{'loss': 0.0875, 'grad_norm': 0.05859375, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
            "{'loss': 0.0587, 'grad_norm': 0.060546875, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
            "{'loss': 0.0935, 'grad_norm': 0.041748046875, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
            "{'loss': 0.0586, 'grad_norm': 0.06982421875, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
            "{'loss': 0.0862, 'grad_norm': 0.052490234375, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
            "{'loss': 0.0578, 'grad_norm': 0.06787109375, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
            "{'loss': 0.0871, 'grad_norm': 0.04248046875, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
            "{'loss': 0.0581, 'grad_norm': 0.057861328125, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
            "{'loss': 0.0794, 'grad_norm': 0.036376953125, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
            "{'loss': 0.0589, 'grad_norm': 0.052490234375, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
            "{'loss': 0.0902, 'grad_norm': 0.0419921875, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
            "{'loss': 0.0562, 'grad_norm': 0.068359375, 'learning_rate': 0.0002, 'epoch': 0.72}\n",
            "{'loss': 0.0895, 'grad_norm': 0.043212890625, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
            "{'loss': 0.0605, 'grad_norm': 0.049072265625, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
            "{'loss': 0.082, 'grad_norm': 0.039794921875, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
            "{'loss': 0.0571, 'grad_norm': 0.047119140625, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
            "{'loss': 0.0844, 'grad_norm': 0.037109375, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
            "{'loss': 0.0562, 'grad_norm': 0.04541015625, 'learning_rate': 0.0002, 'epoch': 0.78}\n",
            "{'loss': 0.0902, 'grad_norm': 0.043212890625, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
            "{'loss': 0.0578, 'grad_norm': 0.05810546875, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
            "{'loss': 0.0878, 'grad_norm': 0.04248046875, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
            "{'loss': 0.058, 'grad_norm': 0.053955078125, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
            "{'loss': 0.0819, 'grad_norm': 0.06689453125, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
            "{'loss': 0.0574, 'grad_norm': 0.049072265625, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
            "{'loss': 0.0935, 'grad_norm': 0.031494140625, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
            "{'loss': 0.0569, 'grad_norm': 0.042236328125, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
            "{'loss': 0.0912, 'grad_norm': 0.03955078125, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
            "{'loss': 0.0571, 'grad_norm': 0.043701171875, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
            "{'loss': 0.0881, 'grad_norm': 0.035400390625, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
            "{'loss': 0.059, 'grad_norm': 0.044921875, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
            "{'loss': 0.0846, 'grad_norm': 0.037841796875, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
            "{'loss': 0.0564, 'grad_norm': 0.042236328125, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
            "{'loss': 0.0964, 'grad_norm': 0.03955078125, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
            "{'loss': 0.0591, 'grad_norm': 0.048095703125, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
            "{'loss': 0.0901, 'grad_norm': 0.0419921875, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
            "{'loss': 0.06, 'grad_norm': 0.052734375, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
            "{'loss': 0.1006, 'grad_norm': 0.1591796875, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
            "{'loss': 0.0648, 'grad_norm': 0.0654296875, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
            "{'loss': 0.1039, 'grad_norm': 0.123046875, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
            "{'loss': 0.0682, 'grad_norm': 0.049560546875, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
            "{'loss': 0.1126, 'grad_norm': 0.050537109375, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
            "{'loss': 0.0674, 'grad_norm': 0.054443359375, 'learning_rate': 0.0002, 'epoch': 1.02}\n",
            "{'loss': 0.1045, 'grad_norm': 0.1728515625, 'learning_rate': 0.0002, 'epoch': 1.03}\n",
            "{'loss': 0.0668, 'grad_norm': 0.0712890625, 'learning_rate': 0.0002, 'epoch': 1.04}\n",
            "{'loss': 0.0974, 'grad_norm': 0.043701171875, 'learning_rate': 0.0002, 'epoch': 1.05}\n",
            "{'loss': 0.0588, 'grad_norm': 0.1767578125, 'learning_rate': 0.0002, 'epoch': 1.06}\n",
            "{'loss': 0.0918, 'grad_norm': 0.04931640625, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
            "{'loss': 0.0615, 'grad_norm': 0.045166015625, 'learning_rate': 0.0002, 'epoch': 1.08}\n",
            "{'loss': 0.0931, 'grad_norm': 0.052978515625, 'learning_rate': 0.0002, 'epoch': 1.09}\n",
            "{'loss': 0.0591, 'grad_norm': 0.059326171875, 'learning_rate': 0.0002, 'epoch': 1.1}\n",
            "{'loss': 0.0893, 'grad_norm': 0.045654296875, 'learning_rate': 0.0002, 'epoch': 1.11}\n",
            "{'loss': 0.0586, 'grad_norm': 0.05224609375, 'learning_rate': 0.0002, 'epoch': 1.12}\n",
            "{'loss': 0.0883, 'grad_norm': 0.057373046875, 'learning_rate': 0.0002, 'epoch': 1.13}\n",
            "{'loss': 0.058, 'grad_norm': 0.06396484375, 'learning_rate': 0.0002, 'epoch': 1.14}\n",
            "{'loss': 0.0884, 'grad_norm': 0.042236328125, 'learning_rate': 0.0002, 'epoch': 1.15}\n",
            "{'loss': 0.0553, 'grad_norm': 0.044677734375, 'learning_rate': 0.0002, 'epoch': 1.16}\n",
            "{'loss': 0.0915, 'grad_norm': 0.03955078125, 'learning_rate': 0.0002, 'epoch': 1.17}\n",
            "{'loss': 0.0534, 'grad_norm': 0.048583984375, 'learning_rate': 0.0002, 'epoch': 1.18}\n",
            "{'loss': 0.0863, 'grad_norm': 0.041748046875, 'learning_rate': 0.0002, 'epoch': 1.19}\n",
            "{'loss': 0.0554, 'grad_norm': 0.04443359375, 'learning_rate': 0.0002, 'epoch': 1.2}\n",
            "{'loss': 0.0843, 'grad_norm': 0.04248046875, 'learning_rate': 0.0002, 'epoch': 1.21}\n",
            "{'loss': 0.0589, 'grad_norm': 0.047607421875, 'learning_rate': 0.0002, 'epoch': 1.22}\n",
            "{'loss': 0.0909, 'grad_norm': 0.03564453125, 'learning_rate': 0.0002, 'epoch': 1.23}\n",
            "{'loss': 0.0556, 'grad_norm': 0.0654296875, 'learning_rate': 0.0002, 'epoch': 1.24}\n",
            "{'loss': 0.083, 'grad_norm': 0.036376953125, 'learning_rate': 0.0002, 'epoch': 1.25}\n",
            "{'loss': 0.0554, 'grad_norm': 0.04052734375, 'learning_rate': 0.0002, 'epoch': 1.26}\n",
            "{'loss': 0.0934, 'grad_norm': 0.044189453125, 'learning_rate': 0.0002, 'epoch': 1.27}\n",
            "{'loss': 0.0546, 'grad_norm': 0.040771484375, 'learning_rate': 0.0002, 'epoch': 1.28}\n",
            "{'loss': 0.0935, 'grad_norm': 0.047607421875, 'learning_rate': 0.0002, 'epoch': 1.29}\n",
            "{'loss': 0.0577, 'grad_norm': 0.0390625, 'learning_rate': 0.0002, 'epoch': 1.3}\n",
            "{'loss': 0.0922, 'grad_norm': 0.046630859375, 'learning_rate': 0.0002, 'epoch': 1.31}\n",
            "{'loss': 0.0562, 'grad_norm': 0.042236328125, 'learning_rate': 0.0002, 'epoch': 1.32}\n",
            "{'loss': 0.0801, 'grad_norm': 0.032958984375, 'learning_rate': 0.0002, 'epoch': 1.33}\n",
            "{'loss': 0.0537, 'grad_norm': 0.037109375, 'learning_rate': 0.0002, 'epoch': 1.34}\n",
            "{'loss': 0.0839, 'grad_norm': 0.0306396484375, 'learning_rate': 0.0002, 'epoch': 1.35}\n",
            "{'loss': 0.0538, 'grad_norm': 0.040771484375, 'learning_rate': 0.0002, 'epoch': 1.36}\n",
            "{'loss': 0.0846, 'grad_norm': 0.04443359375, 'learning_rate': 0.0002, 'epoch': 1.37}\n",
            "{'loss': 0.0569, 'grad_norm': 0.0439453125, 'learning_rate': 0.0002, 'epoch': 1.38}\n",
            "{'loss': 0.0854, 'grad_norm': 0.0380859375, 'learning_rate': 0.0002, 'epoch': 1.39}\n",
            "{'loss': 0.0531, 'grad_norm': 0.04736328125, 'learning_rate': 0.0002, 'epoch': 1.4}\n",
            "{'loss': 0.0815, 'grad_norm': 0.031982421875, 'learning_rate': 0.0002, 'epoch': 1.41}\n",
            "{'loss': 0.0541, 'grad_norm': 0.0458984375, 'learning_rate': 0.0002, 'epoch': 1.42}\n",
            "{'loss': 0.0831, 'grad_norm': 0.03466796875, 'learning_rate': 0.0002, 'epoch': 1.43}\n",
            "{'loss': 0.0567, 'grad_norm': 0.06201171875, 'learning_rate': 0.0002, 'epoch': 1.44}\n",
            "{'loss': 0.0875, 'grad_norm': 0.03173828125, 'learning_rate': 0.0002, 'epoch': 1.45}\n",
            "{'loss': 0.058, 'grad_norm': 0.042236328125, 'learning_rate': 0.0002, 'epoch': 1.46}\n",
            "{'loss': 0.0839, 'grad_norm': 0.046875, 'learning_rate': 0.0002, 'epoch': 1.47}\n",
            "{'loss': 0.0544, 'grad_norm': 0.040283203125, 'learning_rate': 0.0002, 'epoch': 1.48}\n",
            "{'loss': 0.0875, 'grad_norm': 0.044921875, 'learning_rate': 0.0002, 'epoch': 1.49}\n",
            "{'loss': 0.0564, 'grad_norm': 0.043212890625, 'learning_rate': 0.0002, 'epoch': 1.5}\n",
            "{'loss': 0.0819, 'grad_norm': 0.040283203125, 'learning_rate': 0.0002, 'epoch': 1.51}\n",
            "{'loss': 0.0536, 'grad_norm': 0.044677734375, 'learning_rate': 0.0002, 'epoch': 1.52}\n",
            "{'loss': 0.0883, 'grad_norm': 0.05078125, 'learning_rate': 0.0002, 'epoch': 1.53}\n",
            "{'loss': 0.0532, 'grad_norm': 0.0390625, 'learning_rate': 0.0002, 'epoch': 1.54}\n",
            "{'loss': 0.094, 'grad_norm': 0.0458984375, 'learning_rate': 0.0002, 'epoch': 1.55}\n",
            "{'loss': 0.0533, 'grad_norm': 0.046875, 'learning_rate': 0.0002, 'epoch': 1.56}\n",
            "{'loss': 0.0868, 'grad_norm': 0.03515625, 'learning_rate': 0.0002, 'epoch': 1.57}\n",
            "{'loss': 0.0544, 'grad_norm': 0.03857421875, 'learning_rate': 0.0002, 'epoch': 1.58}\n",
            "{'loss': 0.0851, 'grad_norm': 0.03125, 'learning_rate': 0.0002, 'epoch': 1.59}\n",
            "{'loss': 0.0549, 'grad_norm': 0.043701171875, 'learning_rate': 0.0002, 'epoch': 1.6}\n",
            "{'loss': 0.0928, 'grad_norm': 0.04052734375, 'learning_rate': 0.0002, 'epoch': 1.61}\n",
            "{'loss': 0.0531, 'grad_norm': 0.0419921875, 'learning_rate': 0.0002, 'epoch': 1.62}\n",
            "{'loss': 0.08, 'grad_norm': 0.0311279296875, 'learning_rate': 0.0002, 'epoch': 1.63}\n",
            "{'loss': 0.0551, 'grad_norm': 0.0556640625, 'learning_rate': 0.0002, 'epoch': 1.64}\n",
            "{'loss': 0.0835, 'grad_norm': 0.0299072265625, 'learning_rate': 0.0002, 'epoch': 1.65}\n",
            "{'loss': 0.0557, 'grad_norm': 0.03662109375, 'learning_rate': 0.0002, 'epoch': 1.66}\n",
            "{'loss': 0.0862, 'grad_norm': 0.04931640625, 'learning_rate': 0.0002, 'epoch': 1.67}\n",
            "{'loss': 0.0591, 'grad_norm': 0.040283203125, 'learning_rate': 0.0002, 'epoch': 1.68}\n",
            "{'loss': 0.0886, 'grad_norm': 0.0291748046875, 'learning_rate': 0.0002, 'epoch': 1.69}\n",
            "{'loss': 0.0569, 'grad_norm': 0.05126953125, 'learning_rate': 0.0002, 'epoch': 1.7}\n",
            "{'loss': 0.0828, 'grad_norm': 0.037353515625, 'learning_rate': 0.0002, 'epoch': 1.71}\n",
            "{'loss': 0.0564, 'grad_norm': 0.050048828125, 'learning_rate': 0.0002, 'epoch': 1.72}\n",
            "{'loss': 0.0823, 'grad_norm': 0.04248046875, 'learning_rate': 0.0002, 'epoch': 1.73}\n",
            "{'loss': 0.054, 'grad_norm': 0.04443359375, 'learning_rate': 0.0002, 'epoch': 1.74}\n",
            "{'loss': 0.0811, 'grad_norm': 0.037841796875, 'learning_rate': 0.0002, 'epoch': 1.75}\n",
            "{'loss': 0.0584, 'grad_norm': 0.04150390625, 'learning_rate': 0.0002, 'epoch': 1.76}\n",
            "{'loss': 0.0906, 'grad_norm': 0.0390625, 'learning_rate': 0.0002, 'epoch': 1.77}\n",
            "{'loss': 0.0554, 'grad_norm': 0.03759765625, 'learning_rate': 0.0002, 'epoch': 1.78}\n",
            "{'loss': 0.0897, 'grad_norm': 0.039306640625, 'learning_rate': 0.0002, 'epoch': 1.79}\n",
            "{'loss': 0.056, 'grad_norm': 0.046142578125, 'learning_rate': 0.0002, 'epoch': 1.8}\n",
            "{'loss': 0.0842, 'grad_norm': 0.032470703125, 'learning_rate': 0.0002, 'epoch': 1.81}\n",
            "{'loss': 0.0531, 'grad_norm': 0.034912109375, 'learning_rate': 0.0002, 'epoch': 1.82}\n",
            "{'loss': 0.0899, 'grad_norm': 0.037353515625, 'learning_rate': 0.0002, 'epoch': 1.83}\n",
            "{'loss': 0.0586, 'grad_norm': 0.0400390625, 'learning_rate': 0.0002, 'epoch': 1.84}\n",
            "{'loss': 0.09, 'grad_norm': 0.0311279296875, 'learning_rate': 0.0002, 'epoch': 1.85}\n",
            "{'loss': 0.0554, 'grad_norm': 0.048095703125, 'learning_rate': 0.0002, 'epoch': 1.86}\n",
            "{'loss': 0.0876, 'grad_norm': 0.027587890625, 'learning_rate': 0.0002, 'epoch': 1.87}\n",
            "{'loss': 0.0568, 'grad_norm': 0.0390625, 'learning_rate': 0.0002, 'epoch': 1.88}\n",
            "{'loss': 0.0797, 'grad_norm': 0.032958984375, 'learning_rate': 0.0002, 'epoch': 1.89}\n",
            "{'loss': 0.0575, 'grad_norm': 0.0380859375, 'learning_rate': 0.0002, 'epoch': 1.9}\n",
            "{'loss': 0.0782, 'grad_norm': 0.03955078125, 'learning_rate': 0.0002, 'epoch': 1.91}\n",
            "{'loss': 0.056, 'grad_norm': 0.0390625, 'learning_rate': 0.0002, 'epoch': 1.92}\n",
            "{'loss': 0.0929, 'grad_norm': 0.03271484375, 'learning_rate': 0.0002, 'epoch': 1.93}\n",
            "{'loss': 0.0572, 'grad_norm': 0.033935546875, 'learning_rate': 0.0002, 'epoch': 1.94}\n",
            "{'loss': 0.0929, 'grad_norm': 0.036865234375, 'learning_rate': 0.0002, 'epoch': 1.95}\n",
            "{'loss': 0.0557, 'grad_norm': 0.03955078125, 'learning_rate': 0.0002, 'epoch': 1.96}\n",
            "{'loss': 0.088, 'grad_norm': 0.0267333984375, 'learning_rate': 0.0002, 'epoch': 1.97}\n",
            "{'loss': 0.0541, 'grad_norm': 0.044921875, 'learning_rate': 0.0002, 'epoch': 1.98}\n",
            "{'loss': 0.0825, 'grad_norm': 0.027587890625, 'learning_rate': 0.0002, 'epoch': 1.99}\n",
            "{'loss': 0.0576, 'grad_norm': 0.049072265625, 'learning_rate': 0.0002, 'epoch': 2.0}\n",
            "{'train_runtime': 13779.5394, 'train_samples_per_second': 1.451, 'train_steps_per_second': 0.363, 'train_loss': 0.08098026595115662, 'epoch': 2.0}\n",
            "100% 5000/5000 [3:49:39<00:00,  2.76s/it]\n",
            "💾 Saving LoRA fine-tuned model to ./fine-tuned-model...\n",
            "🔗 Merging LoRA with base model...\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "☁️ Pushing merged model to Hugging Face repo Deeps03/qwen2-1.5b-log-classifier...\n",
            "README.md: 100% 31.0/31.0 [00:00<00:00, 244kB/s]\n",
            "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
            "New Data Upload                         : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    :   1% 33.5M/3.09G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :   1% 33.5M/3.09G [00:01<02:13, 22.9MB/s, 27.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :   2% 67.0M/3.09G [00:01<01:04, 46.5MB/s, 47.9MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :   3% 101M/3.09G [00:01<00:43, 69.4MB/s, 62.8MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)                :   4% 134M/3.09G [00:02<00:32, 90.4MB/s, 74.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :   5% 168M/3.09G [00:02<00:26, 108MB/s, 83.9MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)                :   7% 201M/3.09G [00:02<00:23, 123MB/s, 91.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :   8% 235M/3.09G [00:02<00:21, 135MB/s, 97.8MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :   9% 268M/3.09G [00:02<00:19, 144MB/s,  103MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  10% 302M/3.09G [00:03<00:18, 150MB/s,  108MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  10% 319M/3.09G [00:03<00:21, 132MB/s,  106MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  11% 344M/3.09G [00:03<00:21, 130MB/s,  107MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  12% 369M/3.09G [00:03<00:21, 129MB/s,  109MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  13% 386M/3.09G [00:03<00:23, 116MB/s,  107MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  13% 411M/3.09G [00:04<00:22, 118MB/s,  108MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  14% 436M/3.09G [00:04<00:21, 121MB/s,  109MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  15% 453M/3.09G [00:04<00:24, 110MB/s,  108MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  15% 461M/3.09G [00:04<00:29, 89.5MB/s,  105MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)                :  15% 466M/3.09G [00:04<00:37, 69.5MB/s,  101MB/s  ]\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    :  15% 466M/3.09G [00:03<00:21, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    :  15% 466M/3.09G [00:03<00:23, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    :  15% 466M/3.09G [00:03<00:24, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    :  15% 466M/3.09G [00:04<00:25, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  15% 466M/3.09G [00:05<01:56, 22.4MB/s, 83.3MB/s  ]\n",
            "New Data Upload                         :   1% 606k/67.1M [00:05<10:44, 103kB/s,  108kB/s  ]\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    :  15% 466M/3.09G [00:04<00:27, 94.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  15% 470M/3.09G [00:06<02:22, 18.4MB/s, 78.2MB/s  ]\n",
            "New Data Upload                         :   3% 3.64M/134M [00:06<02:50, 767kB/s,  606kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  15% 471M/3.09G [00:06<02:35, 16.8MB/s, 76.0MB/s  ]\n",
            "New Data Upload                         :   4% 5.45M/134M [00:06<01:41, 1.26MB/s,  879kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  16% 479M/3.09G [00:06<02:03, 21.2MB/s, 74.9MB/s  ]\n",
            "New Data Upload                         :  10% 13.3M/134M [00:06<00:28, 4.26MB/s, 2.08MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  16% 489M/3.09G [00:06<01:35, 27.1MB/s, 74.1MB/s  ]\n",
            "New Data Upload                         :  17% 23.0M/134M [00:06<00:12, 8.72MB/s, 3.49MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  16% 502M/3.09G [00:07<01:12, 35.7MB/s, 73.8MB/s  ]\n",
            "New Data Upload                         :  18% 35.7M/201M [00:07<00:10, 15.6MB/s, 5.26MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  17% 513M/3.09G [00:07<01:03, 40.4MB/s, 73.2MB/s  ]\n",
            "New Data Upload                         :  23% 46.6M/201M [00:07<00:07, 21.6MB/s, 6.66MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  17% 529M/3.09G [00:07<00:49, 52.1MB/s, 73.5MB/s  ]\n",
            "New Data Upload                         :  32% 63.6M/201M [00:07<00:04, 32.8MB/s, 8.83MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  18% 548M/3.09G [00:07<00:40, 62.8MB/s, 74.0MB/s  ]\n",
            "New Data Upload                         :  41% 81.8M/201M [00:07<00:02, 44.7MB/s, 11.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  18% 566M/3.09G [00:07<00:35, 71.5MB/s, 74.5MB/s  ]\n",
            "New Data Upload                         :  37% 101M/268M [00:07<00:03, 55.8MB/s, 13.2MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  19% 578M/3.09G [00:08<00:37, 67.6MB/s, 74.1MB/s  ]\n",
            "New Data Upload                         :  42% 112M/268M [00:08<00:02, 56.2MB/s, 14.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  19% 586M/3.09G [00:08<00:42, 59.4MB/s, 73.2MB/s  ]\n",
            "New Data Upload                         :  45% 120M/268M [00:08<00:02, 51.9MB/s, 15.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  19% 597M/3.09G [00:08<00:43, 57.9MB/s, 72.8MB/s  ]\n",
            "New Data Upload                         :  49% 131M/268M [00:08<00:02, 52.5MB/s, 16.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  20% 607M/3.09G [00:08<00:44, 56.0MB/s, 72.3MB/s  ]\n",
            "New Data Upload                         :  53% 141M/268M [00:08<00:02, 52.3MB/s, 16.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  20% 613M/3.09G [00:08<00:51, 48.1MB/s, 71.3MB/s  ]\n",
            "New Data Upload                         :  44% 147M/335M [00:08<00:04, 45.8MB/s, 17.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  20% 623M/3.09G [00:09<00:51, 48.2MB/s, 70.7MB/s  ]\n",
            "New Data Upload                         :  47% 157M/335M [00:09<00:03, 46.5MB/s, 17.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  21% 639M/3.09G [00:09<00:41, 59.1MB/s, 71.1MB/s  ]\n",
            "New Data Upload                         :  52% 174M/335M [00:09<00:02, 57.6MB/s, 19.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  21% 658M/3.09G [00:09<00:35, 69.2MB/s, 71.5MB/s  ]\n",
            "New Data Upload                         :  48% 192M/402M [00:09<00:03, 68.0MB/s, 20.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  22% 674M/3.09G [00:09<00:33, 72.9MB/s, 71.7MB/s  ]\n",
            "New Data Upload                         :  52% 208M/402M [00:09<00:02, 72.0MB/s, 22.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  23% 699M/3.09G [00:09<00:27, 88.0MB/s, 72.8MB/s  ]\n",
            "New Data Upload                         :  58% 233M/402M [00:09<00:01, 87.3MB/s, 24.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  23% 720M/3.09G [00:10<00:25, 92.6MB/s, 73.4MB/s  ]\n",
            "New Data Upload                         :  63% 254M/402M [00:10<00:01, 92.0MB/s, 25.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  24% 741M/3.09G [00:10<00:24, 97.5MB/s, 74.1MB/s  ]\n",
            "New Data Upload                         :  59% 276M/469M [00:10<00:01, 97.1MB/s, 27.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  25% 764M/3.09G [00:10<00:23, 101MB/s, 74.8MB/s  ] \n",
            "New Data Upload                         :  63% 298M/469M [00:10<00:01, 101MB/s, 29.2MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  25% 783M/3.09G [00:10<00:23, 99.6MB/s, 76.7MB/s  ]\n",
            "New Data Upload                         :  67% 317M/469M [00:10<00:01, 99.4MB/s, 31.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  26% 793M/3.09G [00:10<00:26, 86.0MB/s, 77.8MB/s  ]\n",
            "New Data Upload                         :  70% 328M/469M [00:10<00:01, 85.9MB/s, 32.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  26% 806M/3.09G [00:11<00:28, 79.2MB/s, 79.0MB/s  ]\n",
            "New Data Upload                         :  63% 340M/537M [00:11<00:02, 79.1MB/s, 33.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  27% 821M/3.09G [00:11<00:29, 78.1MB/s, 80.5MB/s  ]\n",
            "New Data Upload                         :  66% 355M/537M [00:11<00:02, 78.0MB/s, 34.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  27% 847M/3.09G [00:11<00:24, 92.5MB/s, 83.1MB/s  ]\n",
            "New Data Upload                         :  71% 381M/537M [00:11<00:01, 92.5MB/s, 37.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  28% 870M/3.09G [00:11<00:22, 100MB/s, 82.0MB/s  ] \n",
            "New Data Upload                         :  75% 404M/537M [00:11<00:01, 100MB/s, 39.6MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  29% 898M/3.09G [00:11<00:19, 112MB/s, 81.5MB/s  ]\n",
            "New Data Upload                         :  72% 432M/604M [00:11<00:01, 112MB/s, 42.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  30% 925M/3.09G [00:12<00:18, 119MB/s, 80.8MB/s  ]\n",
            "New Data Upload                         :  76% 459M/604M [00:12<00:01, 119MB/s, 45.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  31% 950M/3.09G [00:12<00:17, 121MB/s, 80.0MB/s  ]\n",
            "New Data Upload                         :  80% 484M/604M [00:12<00:00, 121MB/s, 47.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  31% 966M/3.09G [00:12<00:19, 108MB/s, 78.2MB/s  ]\n",
            "New Data Upload                         :  83% 500M/604M [00:12<00:00, 108MB/s, 49.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  32% 984M/3.09G [00:12<00:20, 103MB/s, 76.8MB/s  ]\n",
            "New Data Upload                         :  77% 518M/671M [00:12<00:01, 103MB/s, 50.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  32% 997M/3.09G [00:12<00:22, 91.6MB/s, 74.7MB/s  ]\n",
            "New Data Upload                         :  79% 531M/671M [00:12<00:01, 91.6MB/s, 52.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  33% 1.01G/3.09G [00:13<00:24, 86.0MB/s, 72.9MB/s  ]\n",
            "New Data Upload                         :  81% 546M/671M [00:13<00:01, 86.0MB/s, 53.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  33% 1.02G/3.09G [00:13<00:27, 75.2MB/s, 70.6MB/s  ]\n",
            "New Data Upload                         :  83% 556M/671M [00:13<00:01, 75.2MB/s, 54.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  33% 1.03G/3.09G [00:13<00:32, 63.5MB/s, 69.6MB/s  ]\n",
            "New Data Upload                         :  76% 563M/738M [00:13<00:02, 63.5MB/s, 55.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  34% 1.04G/3.09G [00:13<00:35, 58.2MB/s, 68.0MB/s  ]\n",
            "New Data Upload                         :  78% 572M/738M [00:13<00:02, 58.2MB/s, 56.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  34% 1.05G/3.09G [00:13<00:32, 63.4MB/s, 67.1MB/s  ]\n",
            "New Data Upload                         :  80% 587M/738M [00:13<00:02, 63.4MB/s, 57.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  35% 1.07G/3.09G [00:14<00:27, 72.5MB/s, 67.3MB/s  ]\n",
            "New Data Upload                         :  82% 606M/738M [00:14<00:01, 72.5MB/s, 59.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  35% 1.09G/3.09G [00:14<00:26, 75.2MB/s, 66.4MB/s  ]\n",
            "New Data Upload                         :  77% 622M/805M [00:14<00:02, 75.2MB/s, 61.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  36% 1.11G/3.09G [00:14<00:24, 80.7MB/s, 65.8MB/s  ]\n",
            "New Data Upload                         :  80% 641M/805M [00:14<00:02, 80.7MB/s, 62.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  37% 1.13G/3.09G [00:14<00:21, 91.9MB/s, 66.4MB/s  ]\n",
            "New Data Upload                         :  83% 665M/805M [00:14<00:01, 91.9MB/s, 65.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  37% 1.15G/3.09G [00:14<00:21, 88.8MB/s, 67.2MB/s  ]\n",
            "New Data Upload                         :  78% 681M/872M [00:14<00:02, 88.8MB/s, 66.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  38% 1.16G/3.09G [00:15<00:22, 84.7MB/s, 68.2MB/s  ]\n",
            "New Data Upload                         :  80% 696M/872M [00:15<00:02, 84.7MB/s, 68.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  38% 1.19G/3.09G [00:15<00:19, 96.4MB/s, 70.7MB/s  ]\n",
            "New Data Upload                         :  83% 721M/872M [00:15<00:01, 96.4MB/s, 70.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  39% 1.20G/3.09G [00:15<00:19, 94.7MB/s, 72.4MB/s  ]\n",
            "New Data Upload                         :  85% 739M/872M [00:15<00:01, 94.7MB/s, 72.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  40% 1.22G/3.09G [00:15<00:20, 89.8MB/s, 74.0MB/s  ]\n",
            "New Data Upload                         :  80% 754M/939M [00:15<00:02, 89.8MB/s, 74.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  40% 1.24G/3.09G [00:15<00:19, 95.5MB/s, 76.1MB/s  ]\n",
            "New Data Upload                         :  83% 776M/939M [00:15<00:01, 95.5MB/s, 76.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  41% 1.27G/3.09G [00:16<00:17, 105MB/s, 78.6MB/s  ] \n",
            "New Data Upload                         :  85% 802M/939M [00:16<00:01, 105MB/s, 78.6MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  42% 1.29G/3.09G [00:16<00:15, 113MB/s, 81.1MB/s  ]\n",
            "New Data Upload                         :  88% 828M/939M [00:16<00:00, 113MB/s, 81.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  43% 1.32G/3.09G [00:16<00:15, 113MB/s, 83.0MB/s  ]\n",
            "New Data Upload                         :  91% 850M/939M [00:16<00:00, 113MB/s, 83.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  43% 1.34G/3.09G [00:16<00:15, 111MB/s, 85.0MB/s  ]\n",
            "New Data Upload                         :  87% 872M/1.01G [00:16<00:01, 111MB/s, 85.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  44% 1.35G/3.09G [00:16<00:16, 102MB/s, 85.8MB/s  ]\n",
            "New Data Upload                         :  88% 888M/1.01G [00:16<00:01, 102MB/s, 85.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  44% 1.37G/3.09G [00:17<00:17, 96.1MB/s, 86.4MB/s  ]\n",
            "New Data Upload                         :  90% 905M/1.01G [00:17<00:01, 96.1MB/s, 86.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  45% 1.39G/3.09G [00:17<00:18, 92.6MB/s, 86.8MB/s  ]\n",
            "New Data Upload                         :  92% 921M/1.01G [00:17<00:00, 92.5MB/s, 86.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  45% 1.40G/3.09G [00:17<00:20, 81.4MB/s, 86.9MB/s  ]\n",
            "New Data Upload                         :  93% 933M/1.01G [00:17<00:00, 81.4MB/s, 86.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  46% 1.41G/3.09G [00:17<00:20, 81.4MB/s, 86.8MB/s  ]\n",
            "New Data Upload                         :  94% 949M/1.01G [00:17<00:00, 81.4MB/s, 86.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  46% 1.43G/3.09G [00:17<00:22, 75.1MB/s, 86.2MB/s  ]\n",
            "New Data Upload                         :  90% 961M/1.07G [00:17<00:01, 75.1MB/s, 86.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  47% 1.44G/3.09G [00:18<00:25, 65.5MB/s, 85.2MB/s  ]\n",
            "New Data Upload                         :  90% 970M/1.07G [00:18<00:01, 65.5MB/s, 85.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  47% 1.45G/3.09G [00:18<00:24, 65.8MB/s, 85.4MB/s  ]\n",
            "New Data Upload                         :  92% 983M/1.07G [00:18<00:01, 65.8MB/s, 85.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  47% 1.46G/3.09G [00:18<00:24, 65.0MB/s, 85.8MB/s  ]\n",
            "New Data Upload                         :  93% 996M/1.07G [00:18<00:01, 65.0MB/s, 85.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  48% 1.48G/3.09G [00:18<00:22, 72.0MB/s, 86.5MB/s  ]\n",
            "New Data Upload                         :  94% 1.01G/1.07G [00:18<00:00, 72.0MB/s, 86.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  48% 1.49G/3.09G [00:18<00:22, 71.2MB/s, 86.9MB/s  ]\n",
            "New Data Upload                         :  90% 1.03G/1.14G [00:18<00:01, 71.2MB/s, 86.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  49% 1.51G/3.09G [00:19<00:22, 69.0MB/s, 87.5MB/s  ]\n",
            "New Data Upload                         :  91% 1.04G/1.14G [00:19<00:01, 69.0MB/s, 87.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  49% 1.52G/3.09G [00:19<00:23, 66.4MB/s, 87.8MB/s  ]\n",
            "New Data Upload                         :  92% 1.05G/1.14G [00:19<00:01, 66.4MB/s, 87.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  50% 1.53G/3.09G [00:19<00:23, 66.3MB/s, 87.4MB/s  ]\n",
            "New Data Upload                         :  93% 1.07G/1.14G [00:19<00:01, 66.3MB/s, 87.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  50% 1.55G/3.09G [00:19<00:21, 72.7MB/s, 87.3MB/s  ]\n",
            "New Data Upload                         :  90% 1.08G/1.21G [00:19<00:01, 72.7MB/s, 87.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  51% 1.56G/3.09G [00:19<00:20, 75.4MB/s, 87.3MB/s  ]\n",
            "New Data Upload                         :  91% 1.10G/1.21G [00:19<00:01, 75.4MB/s, 87.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  51% 1.58G/3.09G [00:20<00:20, 72.9MB/s, 86.2MB/s  ]\n",
            "New Data Upload                         :  92% 1.11G/1.21G [00:20<00:01, 72.9MB/s, 86.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  52% 1.59G/3.09G [00:20<00:20, 73.6MB/s, 85.6MB/s  ]\n",
            "New Data Upload                         :  93% 1.13G/1.21G [00:20<00:01, 73.6MB/s, 85.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  52% 1.61G/3.09G [00:20<00:18, 79.7MB/s, 85.4MB/s  ]\n",
            "New Data Upload                         :  90% 1.15G/1.27G [00:20<00:01, 79.7MB/s, 85.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  53% 1.63G/3.09G [00:20<00:16, 87.1MB/s, 85.2MB/s  ]\n",
            "New Data Upload                         :  92% 1.17G/1.27G [00:20<00:01, 87.1MB/s, 85.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  54% 1.65G/3.09G [00:20<00:15, 92.0MB/s, 85.4MB/s  ]\n",
            "New Data Upload                         :  93% 1.19G/1.27G [00:20<00:00, 92.0MB/s, 85.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  54% 1.67G/3.09G [00:21<00:16, 88.4MB/s, 85.9MB/s  ]\n",
            "New Data Upload                         :  94% 1.20G/1.27G [00:21<00:00, 88.4MB/s, 85.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  55% 1.69G/3.09G [00:21<00:16, 87.1MB/s, 86.3MB/s  ]\n",
            "New Data Upload                         :  91% 1.22G/1.34G [00:21<00:01, 87.1MB/s, 86.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  55% 1.71G/3.09G [00:21<00:14, 92.9MB/s, 86.9MB/s  ]\n",
            "New Data Upload                         :  93% 1.24G/1.34G [00:21<00:01, 92.9MB/s, 86.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  56% 1.72G/3.09G [00:21<00:17, 80.4MB/s, 85.4MB/s  ]\n",
            "New Data Upload                         :  93% 1.25G/1.34G [00:21<00:01, 80.4MB/s, 85.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  56% 1.73G/3.09G [00:21<00:20, 67.1MB/s, 83.8MB/s  ]\n",
            "New Data Upload                         :  89% 1.26G/1.41G [00:21<00:02, 67.1MB/s, 83.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  56% 1.73G/3.09G [00:22<00:22, 61.4MB/s, 82.0MB/s  ]\n",
            "New Data Upload                         :  90% 1.27G/1.41G [00:22<00:02, 61.4MB/s, 82.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  57% 1.75G/3.09G [00:22<00:19, 68.4MB/s, 81.0MB/s  ]\n",
            "New Data Upload                         :  91% 1.29G/1.41G [00:22<00:01, 68.4MB/s, 81.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  57% 1.76G/3.09G [00:22<00:21, 63.0MB/s, 79.6MB/s  ]\n",
            "New Data Upload                         :  88% 1.30G/1.48G [00:22<00:02, 63.0MB/s, 79.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  58% 1.78G/3.09G [00:22<00:19, 67.6MB/s, 79.6MB/s  ]\n",
            "New Data Upload                         :  89% 1.31G/1.48G [00:22<00:02, 67.6MB/s, 79.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  58% 1.80G/3.09G [00:22<00:16, 78.9MB/s, 79.8MB/s  ]\n",
            "New Data Upload                         :  90% 1.33G/1.48G [00:22<00:01, 78.9MB/s, 79.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  59% 1.82G/3.09G [00:23<00:15, 80.6MB/s, 80.2MB/s  ]\n",
            "New Data Upload                         :  91% 1.35G/1.48G [00:23<00:01, 80.6MB/s, 80.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  60% 1.84G/3.09G [00:23<00:13, 90.8MB/s, 81.0MB/s  ]\n",
            "New Data Upload                         :  89% 1.37G/1.54G [00:23<00:01, 90.8MB/s, 81.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  60% 1.86G/3.09G [00:23<00:12, 99.7MB/s, 82.4MB/s  ]\n",
            "New Data Upload                         :  91% 1.40G/1.54G [00:23<00:01, 99.7MB/s, 82.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  61% 1.89G/3.09G [00:23<00:10, 113MB/s, 84.6MB/s  ] \n",
            "New Data Upload                         :  92% 1.43G/1.54G [00:23<00:01, 113MB/s, 84.6MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  62% 1.91G/3.09G [00:23<00:10, 109MB/s, 85.6MB/s  ]\n",
            "New Data Upload                         :  94% 1.45G/1.54G [00:23<00:00, 109MB/s, 85.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  62% 1.93G/3.09G [00:24<00:11, 98.1MB/s, 85.6MB/s  ]\n",
            "New Data Upload                         :  91% 1.46G/1.61G [00:24<00:01, 98.1MB/s, 85.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  63% 1.94G/3.09G [00:24<00:13, 83.1MB/s, 84.7MB/s  ]\n",
            "New Data Upload                         :  91% 1.47G/1.61G [00:24<00:01, 83.1MB/s, 84.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  63% 1.95G/3.09G [00:24<00:14, 76.3MB/s, 84.3MB/s  ]\n",
            "New Data Upload                         :  92% 1.48G/1.61G [00:24<00:01, 76.3MB/s, 84.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  63% 1.96G/3.09G [00:24<00:15, 71.0MB/s, 83.6MB/s  ]\n",
            "New Data Upload                         :  93% 1.49G/1.61G [00:24<00:01, 71.0MB/s, 83.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  64% 1.97G/3.09G [00:24<00:17, 65.4MB/s, 82.3MB/s  ]\n",
            "New Data Upload                         :  93% 1.50G/1.61G [00:24<00:01, 65.4MB/s, 82.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  64% 1.98G/3.09G [00:25<00:17, 63.0MB/s, 81.8MB/s  ]\n",
            "New Data Upload                         :  90% 1.52G/1.68G [00:25<00:02, 63.0MB/s, 81.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  65% 2.00G/3.09G [00:25<00:15, 71.0MB/s, 82.1MB/s  ]\n",
            "New Data Upload                         :  91% 1.53G/1.68G [00:25<00:02, 71.0MB/s, 82.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  65% 2.02G/3.09G [00:25<00:12, 82.4MB/s, 81.8MB/s  ]\n",
            "New Data Upload                         :  93% 1.56G/1.68G [00:25<00:01, 82.4MB/s, 81.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  66% 2.05G/3.09G [00:25<00:11, 93.9MB/s, 82.4MB/s  ]\n",
            "New Data Upload                         :  91% 1.58G/1.74G [00:25<00:01, 93.9MB/s, 82.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  67% 2.07G/3.09G [00:25<00:10, 96.5MB/s, 82.9MB/s  ]\n",
            "New Data Upload                         :  92% 1.60G/1.74G [00:25<00:01, 96.5MB/s, 82.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  68% 2.08G/3.09G [00:26<00:10, 95.7MB/s, 82.6MB/s  ]\n",
            "New Data Upload                         :  93% 1.62G/1.74G [00:26<00:01, 95.7MB/s, 82.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  68% 2.10G/3.09G [00:26<00:10, 91.4MB/s, 81.7MB/s  ]\n",
            "New Data Upload                         :  94% 1.63G/1.74G [00:26<00:01, 91.4MB/s, 81.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  69% 2.12G/3.09G [00:26<00:10, 91.1MB/s, 80.9MB/s  ]\n",
            "New Data Upload                         :  91% 1.65G/1.81G [00:26<00:01, 91.1MB/s, 80.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  69% 2.14G/3.09G [00:26<00:10, 93.0MB/s, 80.6MB/s  ]\n",
            "New Data Upload                         :  92% 1.67G/1.81G [00:26<00:01, 93.0MB/s, 80.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  70% 2.16G/3.09G [00:26<00:10, 91.1MB/s, 80.2MB/s  ]\n",
            "New Data Upload                         :  93% 1.69G/1.81G [00:26<00:01, 91.1MB/s, 80.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  70% 2.17G/3.09G [00:27<00:10, 86.6MB/s, 80.0MB/s  ]\n",
            "New Data Upload                         :  94% 1.70G/1.81G [00:27<00:01, 86.6MB/s, 80.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  71% 2.19G/3.09G [00:27<00:10, 86.8MB/s, 80.2MB/s  ]\n",
            "New Data Upload                         :  92% 1.72G/1.88G [00:27<00:01, 86.8MB/s, 80.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  71% 2.20G/3.09G [00:27<00:10, 85.2MB/s, 80.1MB/s  ]\n",
            "New Data Upload                         :  93% 1.74G/1.88G [00:27<00:01, 85.3MB/s, 80.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  72% 2.23G/3.09G [00:27<00:09, 91.4MB/s, 81.1MB/s  ]\n",
            "New Data Upload                         :  94% 1.76G/1.88G [00:27<00:01, 91.4MB/s, 81.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  73% 2.25G/3.09G [00:27<00:08, 96.1MB/s, 81.6MB/s  ]\n",
            "New Data Upload                         :  95% 1.78G/1.88G [00:27<00:01, 96.1MB/s, 81.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  74% 2.27G/3.09G [00:28<00:07, 104MB/s, 82.8MB/s  ] \n",
            "New Data Upload                         :  93% 1.81G/1.94G [00:28<00:01, 104MB/s, 82.8MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  74% 2.29G/3.09G [00:28<00:07, 103MB/s, 83.9MB/s  ]\n",
            "New Data Upload                         :  94% 1.83G/1.94G [00:28<00:01, 103MB/s, 83.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  75% 2.31G/3.09G [00:28<00:07, 104MB/s, 84.7MB/s  ]\n",
            "New Data Upload                         :  95% 1.85G/1.94G [00:28<00:00, 104MB/s, 84.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  76% 2.33G/3.09G [00:28<00:07, 99.4MB/s, 85.2MB/s  ]\n",
            "New Data Upload                         :  96% 1.86G/1.94G [00:28<00:00, 99.4MB/s, 85.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  76% 2.35G/3.09G [00:28<00:08, 92.1MB/s, 85.0MB/s  ]\n",
            "New Data Upload                         :  93% 1.88G/2.01G [00:28<00:01, 92.1MB/s, 85.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  77% 2.36G/3.09G [00:29<00:08, 89.0MB/s, 85.2MB/s  ]\n",
            "New Data Upload                         :  94% 1.90G/2.01G [00:29<00:01, 89.0MB/s, 85.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  77% 2.38G/3.09G [00:29<00:07, 91.1MB/s, 85.8MB/s  ]\n",
            "New Data Upload                         :  95% 1.92G/2.01G [00:29<00:01, 91.1MB/s, 85.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  78% 2.39G/3.09G [00:29<00:08, 80.3MB/s, 85.7MB/s  ]\n",
            "New Data Upload                         :  96% 1.93G/2.01G [00:29<00:01, 80.3MB/s, 85.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  78% 2.41G/3.09G [00:29<00:08, 81.6MB/s, 86.1MB/s  ]\n",
            "New Data Upload                         :  97% 1.94G/2.01G [00:29<00:00, 81.6MB/s, 86.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  79% 2.42G/3.09G [00:29<00:08, 79.7MB/s, 85.9MB/s  ]\n",
            "New Data Upload                         :  94% 1.96G/2.08G [00:29<00:01, 79.7MB/s, 85.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  79% 2.44G/3.09G [00:30<00:08, 80.4MB/s, 85.9MB/s  ]\n",
            "New Data Upload                         :  95% 1.97G/2.08G [00:30<00:01, 80.4MB/s, 85.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  79% 2.45G/3.09G [00:30<00:09, 69.9MB/s, 85.4MB/s  ]\n",
            "New Data Upload                         :  95% 1.98G/2.08G [00:30<00:01, 69.9MB/s, 85.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  80% 2.46G/3.09G [00:30<00:09, 67.0MB/s, 85.1MB/s  ]\n",
            "New Data Upload                         :  96% 2.00G/2.08G [00:30<00:01, 67.0MB/s, 85.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  80% 2.47G/3.09G [00:30<00:10, 60.6MB/s, 84.2MB/s  ]\n",
            "New Data Upload                         :  96% 2.00G/2.08G [00:30<00:01, 60.6MB/s, 84.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  81% 2.49G/3.09G [00:30<00:09, 64.2MB/s, 83.6MB/s  ]\n",
            "New Data Upload                         :  97% 2.02G/2.08G [00:30<00:00, 64.2MB/s, 83.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  81% 2.50G/3.09G [00:31<00:09, 64.0MB/s, 82.8MB/s  ]\n",
            "New Data Upload                         :  95% 2.03G/2.15G [00:31<00:01, 64.0MB/s, 82.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  81% 2.51G/3.09G [00:31<00:10, 57.5MB/s, 82.1MB/s  ]\n",
            "New Data Upload                         :  95% 2.04G/2.15G [00:31<00:01, 57.5MB/s, 82.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  82% 2.52G/3.09G [00:31<00:09, 58.4MB/s, 81.6MB/s  ]\n",
            "New Data Upload                         :  96% 2.05G/2.15G [00:31<00:01, 58.4MB/s, 81.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  82% 2.53G/3.09G [00:31<00:09, 57.2MB/s, 80.6MB/s  ]\n",
            "New Data Upload                         :  96% 2.06G/2.15G [00:31<00:01, 57.2MB/s, 80.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  82% 2.54G/3.09G [00:31<00:08, 62.8MB/s, 81.1MB/s  ]\n",
            "New Data Upload                         :  94% 2.08G/2.21G [00:31<00:02, 62.8MB/s, 81.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  83% 2.57G/3.09G [00:32<00:06, 74.8MB/s, 82.4MB/s  ]\n",
            "New Data Upload                         :  95% 2.10G/2.21G [00:32<00:01, 74.8MB/s, 82.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  84% 2.58G/3.09G [00:32<00:06, 73.0MB/s, 82.8MB/s  ]\n",
            "New Data Upload                         :  96% 2.11G/2.21G [00:32<00:01, 73.0MB/s, 82.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  84% 2.60G/3.09G [00:32<00:06, 80.2MB/s, 83.0MB/s  ]\n",
            "New Data Upload                         :  94% 2.13G/2.28G [00:32<00:01, 80.2MB/s, 83.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  85% 2.61G/3.09G [00:32<00:05, 79.8MB/s, 83.6MB/s  ]\n",
            "New Data Upload                         :  94% 2.15G/2.28G [00:32<00:01, 79.8MB/s, 83.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  85% 2.63G/3.09G [00:32<00:05, 86.7MB/s, 84.1MB/s  ]\n",
            "New Data Upload                         :  95% 2.17G/2.28G [00:32<00:01, 86.7MB/s, 84.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  86% 2.65G/3.09G [00:33<00:05, 84.0MB/s, 83.5MB/s  ]\n",
            "New Data Upload                         :  96% 2.18G/2.28G [00:33<00:01, 84.0MB/s, 83.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  86% 2.66G/3.09G [00:33<00:05, 78.7MB/s, 83.2MB/s  ]\n",
            "New Data Upload                         :  94% 2.20G/2.35G [00:33<00:01, 78.7MB/s, 83.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  87% 2.67G/3.09G [00:33<00:06, 68.8MB/s, 81.8MB/s  ]\n",
            "New Data Upload                         :  94% 2.21G/2.35G [00:33<00:02, 68.8MB/s, 81.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  87% 2.69G/3.09G [00:33<00:05, 75.5MB/s, 81.3MB/s  ]\n",
            "New Data Upload                         :  95% 2.23G/2.35G [00:33<00:01, 75.4MB/s, 81.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  88% 2.70G/3.09G [00:33<00:05, 68.1MB/s, 79.4MB/s  ]\n",
            "New Data Upload                         :  95% 2.24G/2.35G [00:33<00:01, 68.1MB/s, 79.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  88% 2.71G/3.09G [00:34<00:05, 63.9MB/s, 78.5MB/s  ]\n",
            "New Data Upload                         :  93% 2.25G/2.41G [00:34<00:02, 64.0MB/s, 78.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  88% 2.73G/3.09G [00:34<00:05, 68.3MB/s, 78.6MB/s  ]\n",
            "New Data Upload                         :  94% 2.26G/2.41G [00:34<00:02, 68.3MB/s, 78.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  89% 2.74G/3.09G [00:34<00:04, 71.4MB/s, 79.2MB/s  ]\n",
            "New Data Upload                         :  94% 2.28G/2.41G [00:34<00:01, 71.4MB/s, 79.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  90% 2.77G/3.09G [00:34<00:03, 83.6MB/s, 80.2MB/s  ]\n",
            "New Data Upload                         :  93% 2.30G/2.48G [00:34<00:02, 83.6MB/s, 80.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  90% 2.79G/3.09G [00:34<00:03, 92.0MB/s, 81.3MB/s  ]\n",
            "New Data Upload                         :  94% 2.32G/2.48G [00:34<00:01, 92.0MB/s, 81.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  91% 2.81G/3.09G [00:35<00:02, 101MB/s, 82.7MB/s  ] \n",
            "New Data Upload                         :  95% 2.35G/2.48G [00:35<00:01, 101MB/s, 82.7MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  92% 2.83G/3.09G [00:35<00:02, 101MB/s, 83.5MB/s  ]\n",
            "New Data Upload                         :  95% 2.37G/2.48G [00:35<00:01, 101MB/s, 83.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  93% 2.86G/3.09G [00:35<00:02, 108MB/s, 84.2MB/s  ]\n",
            "New Data Upload                         :  94% 2.39G/2.55G [00:35<00:01, 108MB/s, 84.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  93% 2.88G/3.09G [00:35<00:01, 113MB/s, 84.5MB/s  ]\n",
            "New Data Upload                         :  95% 2.42G/2.55G [00:35<00:01, 113MB/s, 84.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  94% 2.91G/3.09G [00:35<00:01, 118MB/s, 84.6MB/s  ]\n",
            "New Data Upload                         :  96% 2.44G/2.55G [00:35<00:00, 118MB/s, 84.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  95% 2.93G/3.09G [00:36<00:01, 115MB/s, 84.8MB/s  ]\n",
            "New Data Upload                         :  97% 2.46G/2.55G [00:36<00:00, 115MB/s, 84.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  95% 2.95G/3.09G [00:36<00:01, 104MB/s, 84.5MB/s  ]\n",
            "New Data Upload                         :  95% 2.48G/2.62G [00:36<00:01, 104MB/s, 84.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  96% 2.96G/3.09G [00:36<00:01, 100MB/s, 84.6MB/s  ]\n",
            "New Data Upload                         :  96% 2.50G/2.62G [00:36<00:01, 100MB/s, 84.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  97% 2.99G/3.09G [00:36<00:00, 108MB/s, 85.4MB/s  ]\n",
            "New Data Upload                         :  96% 2.52G/2.62G [00:36<00:00, 108MB/s, 85.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  97% 3.00G/3.09G [00:36<00:00, 91.0MB/s, 84.4MB/s  ]\n",
            "New Data Upload                         :  97% 2.53G/2.62G [00:36<00:00, 91.0MB/s, 84.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  97% 3.01G/3.09G [00:37<00:00, 78.1MB/s, 83.7MB/s  ]\n",
            "New Data Upload                         :  97% 2.54G/2.62G [00:37<00:00, 78.1MB/s, 83.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  98% 3.02G/3.09G [00:37<00:00, 71.0MB/s, 83.3MB/s  ]\n",
            "New Data Upload                         :  97% 2.55G/2.62G [00:37<00:00, 71.0MB/s, 83.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  98% 3.03G/3.09G [00:37<00:00, 71.5MB/s, 83.0MB/s  ]\n",
            "New Data Upload                         :  98% 2.57G/2.62G [00:37<00:00, 71.5MB/s, 83.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  99% 3.04G/3.09G [00:37<00:00, 60.1MB/s, 82.0MB/s  ]\n",
            "New Data Upload                         :  98% 2.58G/2.62G [00:37<00:00, 60.1MB/s, 82.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  99% 3.05G/3.09G [00:37<00:00, 52.9MB/s, 80.7MB/s  ]\n",
            "New Data Upload                         :  99% 2.58G/2.62G [00:37<00:00, 52.9MB/s, 80.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  99% 3.06G/3.09G [00:38<00:00, 47.9MB/s, 79.2MB/s  ]\n",
            "New Data Upload                         :  99% 2.59G/2.62G [00:38<00:00, 47.9MB/s, 79.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                :  99% 3.06G/3.09G [00:38<00:00, 47.1MB/s, 77.8MB/s  ]\n",
            "New Data Upload                         :  99% 2.60G/2.62G [00:38<00:00, 47.1MB/s, 77.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                : 100% 3.07G/3.09G [00:38<00:00, 46.6MB/s, 76.7MB/s  ]\n",
            "New Data Upload                         : 100% 2.61G/2.62G [00:38<00:00, 46.6MB/s, 76.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                : 100% 3.08G/3.09G [00:38<00:00, 42.6MB/s, 75.2MB/s  ]\n",
            "New Data Upload                         : 100% 2.61G/2.62G [00:38<00:00, 42.6MB/s, 75.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)                : 100% 3.09G/3.09G [00:38<00:00, 38.8MB/s, 74.1MB/s  ]\n",
            "New Data Upload                         : 100% 2.62G/2.62G [00:38<00:00, 38.8MB/s, 74.1MB/s  ]\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:37<00:00, 81.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:37<00:00, 80.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:37<00:00, 80.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)                : 100% 3.09G/3.09G [00:39<00:00, 14.4MB/s, 68.1MB/s  ]\n",
            "New Data Upload                         : 100% 2.62G/2.62G [00:39<00:00, 14.4MB/s, 68.1MB/s  ]\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:38<00:00, 79.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:38<00:00, 79.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:38<00:00, 78.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)                : 100% 3.09G/3.09G [00:40<00:00, 76.7MB/s, 63.7MB/s  ]\n",
            "New Data Upload                         : 100% 2.62G/2.62G [00:40<00:00, 65.1MB/s, 63.7MB/s  ]\n",
            "  /tmp/tmpz9efx8wn/model.safetensors    : 100% 3.09G/3.09G [00:38<00:00, 78.7MB/s]\n",
            "✅ Fine-tuning complete and model uploaded to Hugging Face!\n"
          ]
        }
      ],
      "source": [
        "!python LLMlogs/fine_tune.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkWUvYj1pHYk"
      },
      "outputs": [],
      "source": [
        "# ✅ Install requirements\n",
        "!apt-get -qq install cmake build-essential git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# ✅ Clone your fine-tuned model repo\n",
        "!git clone https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier\n",
        "%cd qwen2-1.5b-log-classifier\n",
        "\n",
        "# ✅ Clone llama.cpp (latest)\n",
        "%cd /content\n",
        "!git clone https://github.com/ggml-org/llama.cpp\n",
        "%cd llama.cpp\n",
        "\n",
        "# ✅ Build llama.cpp\n",
        "!cmake -B build\n",
        "!cmake --build build -j\n",
        "\n",
        "# ✅ Convert Hugging Face model → GGUF (FP16)\n",
        "!python3 convert_hf_to_gguf.py \\\n",
        "    /content/qwen2-1.5b-log-classifier \\\n",
        "    --outfile /content/qwen2-1.5b-log-classifier-f16.gguf \\\n",
        "    --model-type qwen2\n",
        "\n",
        "# ✅ Quantize model (Q4_K_M)\n",
        "!./build/bin/llama-quantize \\\n",
        "    /content/qwen2-1.5b-log-classifier-f16.gguf \\\n",
        "    /content/qwen2-1.5b-log-classifier-Q4_K_M.gguf \\\n",
        "    Q4_K_M\n",
        "\n",
        "# ✅ Organize files inside repo\n",
        "import os, shutil\n",
        "os.makedirs(\"/content/qwen2-1.5b-log-classifier/gguf\", exist_ok=True)\n",
        "shutil.move(\"/content/qwen2-1.5b-log-classifier-f16.gguf\", \"/content/qwen2-1.5b-log-classifier/gguf/\")\n",
        "shutil.move(\"/content/qwen2-1.5b-log-classifier-Q4_K_M.gguf\", \"/content/qwen2-1.5b-log-classifier/gguf/\")\n",
        "\n",
        "# ✅ Push GGUF files back to Hugging Face\n",
        "%cd /content/qwen2-1.5b-log-classifier\n",
        "!git lfs track \"*.gguf\"\n",
        "!git add .\n",
        "!git commit -m \"Added GGUF (f16 + Q4_K_M) versions\"\n",
        "!git push\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzMRMLClsdVR",
        "outputId": "f01177f7-d93f-417e-9af3-ebce9fcf6a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mistral_common\n",
            "  Downloading mistral_common-1.8.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral_common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral_common\n",
            "Successfully installed mistral_common-1.8.4 pycountry-24.6.1 pydantic-extra-types-2.10.5\n",
            "INFO:hf-to-gguf:Loading model: qwen2-1.5b-log-classifier\n",
            "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151646}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 1536\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8960\n",
            "INFO:hf-to-gguf:gguf: head count = 12\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151645\n",
            "INFO:gguf.vocab:Setting special token type bos to 151643\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/qwen2-1.5b-log-classifier-f16.gguf: n_tensors = 338, total_size = 3.1G\n",
            "Writing: 100% 3.09G/3.09G [00:38<00:00, 81.0Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/qwen2-1.5b-log-classifier-f16.gguf\n",
            "/bin/bash: line 1: ./content/llama.cpp/build/bin/llama-quantize: No such file or directory\n",
            "/bin/bash: line 1: ./content/llama.cpp/build/bin/llama-quantize: No such file or directory\n",
            "mv: cannot stat '/content/qwen2-1.5b-log-classifier-Q4_K_M.gguf': No such file or directory\n",
            "mv: cannot stat '/content/qwen2-1.5b-log-classifier-Q5_K_M.gguf': No such file or directory\n",
            "Tracking \"*.gguf\"\n",
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@e2ef6e2390a0.(none)')\n"
          ]
        }
      ],
      "source": [
        "!pip install mistral_common\n",
        "\n",
        "!cd /content/llama.cpp && python3 convert_hf_to_gguf.py /content/qwen2-1.5b-log-classifier --outfile /content/qwen2-1.5b-log-classifier-f16.gguf\n",
        "\n",
        "!./content/llama.cpp/build/bin/llama-quantize /content/qwen2-1.5b-log-classifier-f16.gguf /content/qwen2-1.5b-log-classifier-Q4_K_M.gguf Q4_K_M\n",
        "!./content/llama.cpp/build/bin/llama-quantize /content/qwen2-1.5b-log-classifier-f16.gguf /content/qwen2-1.5b-log-classifier-Q5_K_M.gguf Q5_K_M\n",
        "\n",
        "!mkdir -p /content/qwen2-1.5b-log-classifier/gguf\n",
        "!mv /content/qwen2-1.5b-log-classifier-f16.gguf /content/qwen2-1.5b-log-classifier/gguf/\n",
        "!mv /content/qwen2-1.5b-log-classifier-Q4_K_M.gguf /content/qwen2-1.5b-log-classifier/gguf/\n",
        "!mv /content/qwen2-1.5b-log-classifier-Q5_K_M.gguf /content/qwen2-1.5b-log-classifier/gguf/\n",
        "\n",
        "!cd /content/qwen2-1.5b-log-classifier && git lfs track \"*.gguf\" && git add . && git commit -m \"Added GGUF models\" && git push\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "8b8fa1d870954e3db10e08d061cf07ee",
            "f4d642a23965459191622f7acd8c69f9"
          ]
        },
        "id": "x9CmCYEDuYTR",
        "outputId": "5661d4d4-ed39-4f36-e484-f3f7b982ef10"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b8fa1d870954e3db10e08d061cf07ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wk9YNqcug5w",
        "outputId": "e9f6cf33-c533-458e-dff0-609a011c963b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-llama-spm.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-starcoder.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-qwen2.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-phi-3.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-bert-bge.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-baichuan.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-command-r.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-gpt-2.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-aquila.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-refact.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-mpt.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-falcon.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-nomic-bert-moe.gguf\n",
            "/content/llama.cpp/models/ggml-vocab-gpt-neox.gguf\n",
            "/content/qwen2-1.5b-log-classifier/gguf/qwen2-1.5b-log-classifier-f16.gguf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content\"):\n",
        "    for f in files:\n",
        "        if f.endswith(\".gguf\"):\n",
        "            print(os.path.join(root, f))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "9d05b8e4797c4f3b8a04fb4184054145",
            "b5f493fc09ef498386a72980abb8cba3",
            "bf92d69172a04066a25a4c300f7514a0",
            "81c97deb048647beab4e5450e9380b31",
            "169bb1693d914e6e84b46b21b334bff6",
            "74b589d83f2a4599b12a61d94e98a0e2",
            "e901cfef3bc14c7397d9f0010a15a470",
            "35e81869d55545c1a223da69937f28a9",
            "0f5ecc38f9c34056afebc11cd2ba0751",
            "6ed150eff3fe456da7ddab6b62dcccbb",
            "3f0aaf6fe22d49989ff765fd28a3c2f6",
            "fb21570567c94da19c3940fde0efc669",
            "2e0c9a7240834b8696e92dcf683efb75",
            "975e235f9caf4adb8fc01e8262d2d12a",
            "69fb510685a1486490bd8996440e211f",
            "5c1ee615b716443b8855c8ff84431d6f",
            "140e59a01e84407ca9542f8728a738b1",
            "a68155d675474b7c84153852bf825e04",
            "1b38335e9f8c4b24a092bf1b6df0bc58",
            "011f6b2593b14eadab1f4f2a5edb3a82",
            "7cd001676ab4443a9822ba4e00b3ee46",
            "bf9f85df75f1454eb40f04db5102e9eb",
            "8b5356452a8343849a959e76c6815e33",
            "c5996ac4b209474ca1f1e18530e0dcb2",
            "77087f5e92624d158822106e6031c451",
            "855015fb93a34503b08016ebcf4eac42",
            "73bcb76d98e84793879a91504933c3c7",
            "c546f6ed3e2f4f12ad4b2aa97c1783e2",
            "8fcc73d4c7a7439fabc0e91efbcfc300",
            "79f3d1a30e3748a3b43769e2dd2b2b96",
            "fd6a1ba75d5741249c08818fd61f301b",
            "9748f6b870024ac6a5d1fa5b58d55a9a",
            "73fdf897095e4d81916126600b1cb866"
          ]
        },
        "id": "yc8Aw8tJu5sv",
        "outputId": "36c1bef2-af74-4eac-9b61-c55964321a0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d05b8e4797c4f3b8a04fb4184054145",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb21570567c94da19c3940fde0efc669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b5356452a8343849a959e76c6815e33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  .../qwen2-1.5b-log-classifier-f16.gguf:   1%|1         | 33.6MB / 3.09GB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Upload complete!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login, upload_file\n",
        "\n",
        "# Login with your token\n",
        "login(\"hf token\")\n",
        "\n",
        "# Path to your model file\n",
        "local_file = \"/content/qwen2-1.5b-log-classifier/gguf/qwen2-1.5b-log-classifier-f16.gguf\"\n",
        "path_in_repo = \"qwen2-1.5b-log-classifier-f16.gguf\"  # filename on HF\n",
        "\n",
        "# Upload the file\n",
        "upload_file(\n",
        "    path_or_fileobj=local_file,\n",
        "    path_in_repo=path_in_repo,\n",
        "    repo_id=\"Deeps03/qwen2-1.5b-log-classifier\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"✅ Upload complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "q1cZKZS1ePPI",
        "outputId": "2bf3fe70-e305-4e6b-9758-d31e4fe997bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Git LFS initialized.\n",
            "Cloning into 'qwen2-1.5b-log-classifier'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 33 (delta 8), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (33/33), 3.60 MiB | 2.97 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 5.75 GiB | 32.53 MiB/s, done.\n",
            "/content/qwen2-1.5b-log-classifier\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ls' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-908349828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Verify your f16 file is there\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mls\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ls' is not defined"
          ]
        }
      ],
      "source": [
        "# Install git-lfs to handle large files\n",
        "!apt-get install -y git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# Clone your repo\n",
        "!git clone https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier\n",
        "%cd qwen2-1.5b-log-classifier\n",
        "\n",
        "# Verify your f16 file is there\n",
        "ls -lh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go to /content\n",
        "%cd /content\n",
        "\n",
        "# Clone llama.cpp if not already there\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "\n",
        "# Build llama.cpp with CMake\n",
        "!cmake -B build\n",
        "!cmake --build build -j\n",
        "\n",
        "# Now quantize your f16 model into Q4_K_M\n",
        "!./build/bin/llama-quantize \\\n",
        "    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf \\\n",
        "    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q4_K_M.gguf Q4_K_M\n",
        "\n",
        "# And into Q5_K_M\n",
        "!./build/bin/llama-quantize \\\n",
        "    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf \\\n",
        "    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q5_K_M.gguf Q5_K_M\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4VFmQMwOQ0n",
        "outputId": "fc1753f9-4415-4876-d762-40104b643fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 61227, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 61227 (delta 106), reused 74 (delta 65), pack-reused 61059 (from 3)\u001b[K\n",
            "Receiving objects: 100% (61227/61227), 151.91 MiB | 22.99 MiB/s, done.\n",
            "Resolving deltas: 100% (44471/44471), done.\n",
            "/content/llama.cpp\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6405\n",
            "-- ggml commit:  c97b5e58\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.5s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  7%] Built target build_info\n",
            "[  7%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  9%] Built target sha1\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  9%] Built target sha256\n",
            "[  9%] Built target llama-gemma3-cli\n",
            "[  9%] Built target llama-qwen2vl-cli\n",
            "[  9%] Built target llama-llava-cli\n",
            "[  9%] Built target llama-minicpmv-cli\n",
            "[  9%] Built target xxhash\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  9%] Built target ggml-base\n",
            "[  9%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 15%] Built target ggml-cpu\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 15%] Built target ggml\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 27%] Built target llama-gguf\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 27%] Built target llama-gguf-hash\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 27%] Built target llama\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 35%] Built target test-c\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 35%] Built target llama-simple\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 36%] Built target llama-simple-chat\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 37%] Built target mtmd\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 38%] Built target common\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 71%] Built target test-mtmd-c-api\n",
            "[ 71%] Built target test-model-load-cancel\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 71%] Built target test-log\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 71%] Built target test-rope\n",
            "[ 71%] Built target test-autorelease\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 72%] Built target test-barrier\n",
            "[ 72%] Built target test-quantize-fns\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 72%] Built target llama-q8dot\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 74%] Built target llama-logits\n",
            "[ 74%] Built target llama-lookup-merge\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 74%] Built target test-gbnf-validator\n",
            "[ 74%] Built target llama-vdot\n",
            "[ 74%] Built target test-tokenizer-1-spm\n",
            "[ 74%] Built target test-tokenizer-1-bpe\n",
            "[ 74%] Built target llama-tokenize\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 75%] Built target llama-gguf-split\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 76%] Built target test-grammar-parser\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 78%] Built target test-sampling\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 78%] Built target test-tokenizer-0\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 81%] Built target test-llama-grammar\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 82%] Built target llama-finetune\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 83%] Built target llama-lookup-create\n",
            "[ 83%] Built target test-quantize-perf\n",
            "[ 83%] Built target llama-gen-docs\n",
            "[ 83%] Built target llama-eval-callback\n",
            "[ 83%] Built target llama-save-load-state\n",
            "[ 83%] Built target test-regex-partial\n",
            "[ 83%] Built target llama-batched\n",
            "[ 83%] Built target test-thread-safety\n",
            "[ 83%] Built target llama-lookup-stats\n",
            "[ 83%] Built target llama-gritlm\n",
            "[ 83%] Built target llama-batched-bench\n",
            "[ 83%] Built target llama-passkey\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 84%] Built target llama-speculative-simple\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 85%] Built target llama-lookup\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 86%] Built target test-opt\n",
            "[ 86%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 88%] Built target llama-lookahead\n",
            "[ 88%] Built target test-arg-parser\n",
            "[ 88%] Built target llama-embedding\n",
            "[ 88%] Built target test-gguf\n",
            "[ 88%] Built target llama-parallel\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 90%] Built target llama-quantize\n",
            "[ 90%] Built target llama-retrieval\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 92%] Built target llama-export-lora\n",
            "[ 92%] Built target llama-mtmd-cli\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 92%] Built target test-chat-template\n",
            "[ 92%] Built target llama-diffusion-cli\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 93%] Built target llama-cvector-generator\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 93%] Built target llama-speculative\n",
            "[ 93%] Built target test-json-partial\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 94%] Built target llama-cli\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 95%] Built target test-quantize-stats\n",
            "[ 95%] Built target llama-perplexity\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 96%] Built target test-grammar-integration\n",
            "[ 96%] Built target llama-run\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 97%] Built target test-chat-parser\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 99%] Built target test-json-schema-to-grammar\n",
            "[ 99%] Built target llama-imatrix\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[100%] Built target llama-bench\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[100%] Built target llama-tts\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[100%] Built target test-chat\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[100%] Built target test-backend-ops\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "main: build = 6405 (c97b5e58)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf' to '/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   2/ 338]                    token_embd.weight - [ 1536, 151646,     1,     1], type =    f16, converting to q6_K .. size =   444.28 MiB ->   182.22 MiB\n",
            "[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "llama_model_quantize_impl: model size  =  2943.83 MB\n",
            "llama_model_quantize_impl: quant size  =   934.35 MB\n",
            "\n",
            "main: quantize time = 150335.28 ms\n",
            "main:    total time = 150335.28 ms\n",
            "main: build = 6405 (c97b5e58)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf' to '/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q5_K_M.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   2/ 338]                    token_embd.weight - [ 1536, 151646,     1,     1], type =    f16, converting to q6_K .. size =   444.28 MiB ->   182.22 MiB\n",
            "[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q5_K .. size =     0.75 MiB ->     0.26 MiB\n",
            "[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q5_K .. size =     4.50 MiB ->     1.55 MiB\n",
            "[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q5_K .. size =    26.25 MiB ->     9.02 MiB\n",
            "llama_model_quantize_impl: model size  =  2943.83 MB\n",
            "llama_model_quantize_impl: quant size  =  1066.91 MB\n",
            "\n",
            "main: quantize time = 127790.09 ms\n",
            "main:    total time = 127790.09 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "token = \"hf token\"\n",
        "repo_id = \"Deeps03/qwen2-1.5b-log-classifier\"\n",
        "\n",
        "files_to_upload = [\n",
        "    \"/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q4_K_M.gguf\",\n",
        "    \"/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q5_K_M.gguf\"\n",
        "]\n",
        "\n",
        "for file_path in files_to_upload:\n",
        "    upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=\"gguf/\" + file_path.split(\"/\")[-1],\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        token=token,\n",
        "        commit_message=f\"Added {file_path.split('/')[-1]} quantized version\"\n",
        "    )\n",
        "    print(f\"✅ Uploaded {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "dd083d2acb634fc29d31bc093d7dc813",
            "4ee1f3567f1f4c46873d012aedff8b16",
            "24ef2e926494418d897ab037f7410ba5",
            "a67c601fe7b64d8b99b9542fc053d4ad",
            "ba6b991cfce2459f92506310387f56fb",
            "be78d2448c9c4eb79e14f4761d49c5f9",
            "671d3702c76e446f8f11d812fea4e601",
            "0b3bb09691284ea88fdff5041c28915b",
            "9be2ae82a9be4774ac030f8d0c4b29b4",
            "00f2ff84f2444f96bb555953c96840f2",
            "fb3b5a7b85a740299287302abe79a1f4",
            "1030038e01a64eab8abb9ffeee322e44",
            "d2f8be7bef104952a20ac6bb28c6bfbf",
            "9f36ac51148743e2b62e22f55c280a8e",
            "6394a7c28f09401e83b70ead3f853a77",
            "6559a5d0439c4a38a09f0ad464670c61",
            "445c2bb6334f45b0a69ed9c34191370e",
            "891e1eaa8ba641a28d64c19a8817cfa4",
            "22c7ee1055614c0198e483ae386d9e9a",
            "96d06d68094a4962874ef06ae83f9b12",
            "103eedcf7bd14d48b41c201a0dcb5052",
            "f43b07b3cf234a60913b358d7eb79338",
            "c9b1ff094619421ea7b901358c9c4cda",
            "f2d20bf33e12437ea4066b280268f20a",
            "6c5357ab3c7f430ba0e49ad1febc1c47",
            "a204b2cfccb6441790cc40aeb45136de",
            "0b463277a2fe4752a08568c538caafd5",
            "2dab92f0ec0b4866a8e4d7347ad66bab",
            "ce8c694c1022476f85a7f3a9dc259941",
            "95c70187ee264bfba60b5ef0e7858a5e",
            "b746067d2d6842d4b7eefc43d73c31ca",
            "f0fa38a122e64a37b94df8ac84af1c4b",
            "9e511939d2884ed0b04df4acb5a16b41",
            "cd97b06a31684702a4781dd20a177135",
            "eda7f5f926734cb4909f0dbfff03a871",
            "0208eef0a5c54d1b80247a6609f6427e",
            "b184145a1e3f40de9e45449206603249",
            "a1b1a285d723454eb76785359be51484",
            "7cb4840351d845be8277296e8b587bd5",
            "79caa7d5ba704103930c19657db45094",
            "04adbfab563a4e8cb4c683ed94922e36",
            "a5109298cc8e49febed16e1dfc680604",
            "a952be2fcc6f4a2fa3f30c35e45f175b",
            "e35b707785e545bc947578e75f98724a",
            "4d52801dd9a04a0d910b405a15df98f1",
            "9db0a720e9154993a7a1f8382ddccb73",
            "ebf375ed6c2843aeab49f9127adeeccc",
            "5d9e9ed9600f4c228f0b4f58d39a3b58",
            "197d5cb732e945988c249bd0300c90ea",
            "b50de80bc8cc487584dcee2cdabac5d6",
            "725f0a5c628d4d79a98aa59d5e81be09",
            "90e9e4402dc3481d9f8f3a7f2189ce6e",
            "efcb58f091fb4984bdeabd58f8298d84",
            "0a42f526fbc74dc49045cf14b5370474",
            "6e3b5e0dec3d4ab0adf2effd120348d8",
            "9495de38f2124d589adc651a19943e85",
            "6409acb798fe43548c200f62c16a1039",
            "3a69cc09452b48faa7eeb30a6f038642",
            "e249a795288b439c948928b80e2e812a",
            "503eba291702442d9485419a166ad661",
            "1fea2ccf76c94e5c9868a9080350db35",
            "8264fca6b24241dca84cda5176bc99d1",
            "259800acb5ea4f1ea2c198e01e95d50c",
            "a969b8e0a64b446eab7b84d16ad6bfae",
            "0d7304e2e5794b85bc8bfda0e85db7b4",
            "315e73d44860463eb3afae40e8a89175"
          ]
        },
        "id": "w7EDc8YsRwmz",
        "outputId": "d04a0f1e-0cc6-46df-b24a-e62090746a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd083d2acb634fc29d31bc093d7dc813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1030038e01a64eab8abb9ffeee322e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...en2-1.5b-log-classifier-Q4_K_M.gguf:   3%|2         | 25.2MB /  986MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9b1ff094619421ea7b901358c9c4cda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Uploaded /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q4_K_M.gguf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd97b06a31684702a4781dd20a177135"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d52801dd9a04a0d910b405a15df98f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...en2-1.5b-log-classifier-Q5_K_M.gguf:   3%|2         | 33.5MB / 1.12GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9495de38f2124d589adc651a19943e85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Uploaded /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q5_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 0: Reset environment ===\n",
        "%cd /content\n",
        "!rm -rf llama.cpp qwen2-1.5b-log-classifier\n",
        "\n",
        "# === Step 1: Clone your repo with f16 file ===\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier\n",
        "\n",
        "# === Step 2: Clone llama.cpp and build ===\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd /content/llama.cpp\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release -j 8\n",
        "\n",
        "# === Step 3: Go back to your repo folder ===\n",
        "%cd /content/qwen2-1.5b-log-classifier\n",
        "\n",
        "# === Step 4: Quantize Q8_0 ===\n",
        "!/content/llama.cpp/build/bin/llama-quantize \\\n",
        "    qwen2-1.5b-log-classifier-f16.gguf \\\n",
        "    qwen2-1.5b-log-classifier-Q8_0.gguf \\\n",
        "    Q8_0\n",
        "\n",
        "# === Step 5: Configure Git identity ===\n",
        "!git config --global user.email \"deepaksuresh698@gmail.com\"\n",
        "!git config --global user.name \"Deepak\"\n",
        "\n",
        "# === Step 6: Push quantized model to HF ===\n",
        "!git lfs track \"*.gguf\"\n",
        "!git add qwen2-1.5b-log-classifier-Q8_0.gguf\n",
        "!git commit -m \"Add Q8_0 quantization\"\n",
        "!git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiVh9sfBT1Md",
        "outputId": "54fb73bf-7f40-4baf-a991-3cb1ed18dfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Git LFS initialized.\n",
            "Cloning into 'qwen2-1.5b-log-classifier'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 43 (delta 13), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (43/43), 3.60 MiB | 4.21 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 7.72 GiB | 35.96 MiB/s, done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 61227, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 61227 (delta 97), reused 63 (delta 63), pack-reused 61080 (from 4)\u001b[K\n",
            "Receiving objects: 100% (61227/61227), 151.88 MiB | 23.16 MiB/s, done.\n",
            "Resolving deltas: 100% (44475/44475), done.\n",
            "/content/llama.cpp\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6405\n",
            "-- ggml commit:  c97b5e58\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.1s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  4%] Built target build_info\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  4%] Built target sha1\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  4%] Built target sha256\n",
            "[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  6%] Built target llama-llava-cli\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  7%] Built target llama-gemma3-cli\n",
            "[  7%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  8%] Built target llama-minicpmv-cli\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  9%] Built target llama-qwen2vl-cli\n",
            "[  9%] Built target xxhash\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  9%] Built target ggml-base\n",
            "[  9%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 15%] Built target ggml-cpu\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 15%] Built target ggml\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 19%] Built target llama-gguf\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 20%] Built target llama-gguf-hash\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 27%] Built target llama\n",
            "[ 27%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 31%] Built target test-c\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 31%] Built target llama-simple\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 33%] Built target llama-simple-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 37%] Built target mtmd\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 38%] Built target common\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 42%] Built target test-sampling\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] Built target test-tokenizer-0\n",
            "[ 42%] Built target test-grammar-parser\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 45%] Built target test-llama-grammar\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 47%] Built target test-gbnf-validator\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 47%] Built target test-tokenizer-1-bpe\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 48%] Built target test-tokenizer-1-spm\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 49%] Built target test-log\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 50%] Built target test-regex-partial\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 53%] Built target test-chat-template\n",
            "[ 53%] Built target test-quantize-stats\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 54%] Built target test-json-partial\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 55%] Built target test-grammar-integration\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 56%] Built target test-thread-safety\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 57%] Built target test-model-load-cancel\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 59%] Built target test-opt\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 61%] Built target test-autorelease\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 61%] Built target test-arg-parser\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 61%] Built target test-barrier\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 62%] Built target test-quantize-fns\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] Built target test-rope\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 65%] Built target test-mtmd-c-api\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 67%] Built target test-gguf\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 69%] Built target test-json-schema-to-grammar\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 69%] Built target test-quantize-perf\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 71%] Built target test-chat-parser\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 71%] Built target llama-batched\n",
            "[ 71%] Built target llama-eval-callback\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 73%] Built target llama-embedding\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 74%] Built target llama-gritlm\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 74%] Built target llama-lookup-merge\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 75%] Built target llama-lookahead\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 75%] Built target llama-lookup\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 76%] Built target llama-lookup-create\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 76%] Built target llama-lookup-stats\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 78%] Built target llama-parallel\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 79%] Built target llama-passkey\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 80%] Built target llama-save-load-state\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 81%] Built target llama-retrieval\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 82%] Built target llama-speculative-simple\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 84%] Built target llama-finetune\n",
            "[ 84%] Built target llama-gen-docs\n",
            "[ 84%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 86%] Built target llama-logits\n",
            "[ 86%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 86%] Built target llama-speculative\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 87%] Built target llama-q8dot\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 88%] Built target llama-vdot\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 89%] Built target llama-diffusion-cli\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 89%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 90%] Built target llama-gguf-split\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 91%] Built target llama-batched-bench\n",
            "[ 91%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 91%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 91%] Built target test-chat\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 91%] Built target llama-quantize\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 92%] Built target llama-cli\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 93%] Built target llama-tokenize\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 94%] Built target llama-perplexity\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 96%] Built target llama-mtmd-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 97%] Built target llama-cvector-generator\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 99%] Built target llama-imatrix\n",
            "[ 99%] Built target llama-run\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 99%] Built target llama-export-lora\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 99%] Built target test-backend-ops\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[100%] Built target llama-bench\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[100%] Built target llama-tts\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "/content/qwen2-1.5b-log-classifier\n",
            "main: build = 6405 (c97b5e58)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'qwen2-1.5b-log-classifier-f16.gguf' to 'qwen2-1.5b-log-classifier-Q8_0.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from qwen2-1.5b-log-classifier-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   2/ 338]                    token_embd.weight - [ 1536, 151646,     1,     1], type =    f16, converting to q8_0 .. size =   444.28 MiB ->   236.02 MiB\n",
            "[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
            "[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
            "[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
            "[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
            "[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
            "llama_model_quantize_impl: model size  =  2943.83 MB\n",
            "llama_model_quantize_impl: quant size  =  1564.17 MB\n",
            "\n",
            "main: quantize time = 26033.19 ms\n",
            "main:    total time = 26033.20 ms\n",
            "Tracking \"*.gguf\"\n",
            "[main e4873f8] Add Q8_0 quantization\n",
            " 1 file changed, 3 insertions(+)\n",
            " create mode 100644 qwen2-1.5b-log-classifier-Q8_0.gguf\n",
            "fatal: could not read Username for 'https://huggingface.co': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder, upload_file\n",
        "\n",
        "# Your repo and access token\n",
        "repo_id = \"Deeps03/qwen2-1.5b-log-classifier\"\n",
        "token = \"hf token\"\n",
        "\n",
        "# Save the token\n",
        "HfFolder.save_token(token)\n",
        "api = HfApi()\n",
        "\n",
        "# Local file path (make sure it exists after quantization)\n",
        "local_file = \"/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q8_0.gguf\"\n",
        "\n",
        "# Path inside repo\n",
        "path_in_repo = \"gguf/qwen2-1.5b-log-classifier-Q8_0.gguf\"\n",
        "\n",
        "# Upload file to repo\n",
        "upload_file(\n",
        "    path_or_fileobj=local_file,\n",
        "    path_in_repo=path_in_repo,\n",
        "    repo_id=repo_id,\n",
        "    token=token\n",
        ")\n",
        "\n",
        "print(f\"✅ Uploaded {local_file} to https://huggingface.co/{repo_id}/blob/main/{path_in_repo}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "99642e3d8808478eaaff4e8d13be4d96",
            "46263d6c2281469cb849f3ba281bcbbd",
            "a9039d53836942248473b38f5227c4a6",
            "36fe90d5450a41c0a647317393085d73",
            "2c0c8d4607574c0ca86576d847d204ab",
            "140270866d3849cc91816af26a09e77e",
            "19b186ce4098436fbb916580caf54ed7",
            "e64051683f3c45c2ad7643a0a3a40478",
            "3da183ffeeaf40598d86d522c734012c",
            "52c68fc4fc2b40f98a4b73e6e234da11",
            "5d5c32e2aed24fe5836916cd97ee0a32",
            "9114414e9abb44d6a73d0faa3ff853e7",
            "1ee8cb3e34524c8794a2113bdc87533a",
            "f7c35fac8e5b473393e5e5b9d1017ad7",
            "7c0d1d1aeff54e1e888fc53b9c892f19",
            "1d970099496e40e98e8873e4fdc49301",
            "156ee39402034e4a83de4358e6c6083b",
            "3e41198392274a8798065e430106a6d1",
            "01e07d313f81448489390aa96e137ae2",
            "4afd0fdefc7848aabcc08f1563b7d1c8",
            "b821c49a931f47959df53fb571371142",
            "b1825741773a46029ba1bc6e8890746f",
            "f2c617d0c7074ea2917060c7d156d001",
            "26874fb011af48b497e2239a9951f341",
            "6513c4d787df47289e4a8fa021108de1",
            "8877b86e57ae447a97562472d882b465",
            "d8a541eee92a446da0553eb348fdb91e",
            "96e1bcd7edbf44fd95a211b4dc2857dc",
            "c8e797e7ba9a446db656fbecf1a6a67d",
            "b5a25a99347a46b8abe5e3208f299900",
            "0829df8f57a4495096f4a80ca44c8c12",
            "b2d1ef62c7b34f99ab7e1669f0234c36",
            "86e3dd20ba99403485cef3bc2dba5c92"
          ]
        },
        "id": "ZGWc9lxrYZD3",
        "outputId": "d0a7c040-09c9-4d0a-9210-0f94cb829a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99642e3d8808478eaaff4e8d13be4d96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9114414e9abb44d6a73d0faa3ff853e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...qwen2-1.5b-log-classifier-Q8_0.gguf:   0%|          | 5.90MB / 1.65GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2c617d0c7074ea2917060c7d156d001"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Uploaded /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q8_0.gguf to https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier/blob/main/gguf/qwen2-1.5b-log-classifier-Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder, upload_file, delete_folder\n",
        "import os\n",
        "\n",
        "repo_id = \"Deeps03/qwen2-1.5b-log-classifier\"\n",
        "token = \"hf token\"\n",
        "\n",
        "HfFolder.save_token(token)\n",
        "api = HfApi()\n",
        "\n",
        "gguf_dir = \"/content/qwen2-1.5b-log-classifier/gguf\"\n",
        "for f in os.listdir(gguf_dir):\n",
        "    local_path = os.path.join(gguf_dir, f)\n",
        "    upload_file(\n",
        "        path_or_fileobj=local_path,\n",
        "        path_in_repo=f,  # root of repo\n",
        "        repo_id=repo_id,\n",
        "        token=token\n",
        "    )\n",
        "    print(f\"✅ Uploaded {f} to root of {repo_id}\")\n",
        "\n",
        "# Delete gguf folder from repo\n",
        "delete_folder(\n",
        "    repo_id=repo_id,\n",
        "    path_in_repo=\"gguf\",\n",
        "    token=token,\n",
        "    commit_message=\"Remove gguf folder after moving files to root\"\n",
        ")\n",
        "print(\"🗑️ Removed old gguf folder\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "c8e9d4e8e7b944aba0add70258bb0516",
            "c422c54bef1a427392214c6edf30788d",
            "defdf8861e404da583030d66d33c9226",
            "03a87c0aaca34661b79e06f4a1bbafe5",
            "83860255962f4392b756470f31432c15",
            "93e3edd7d9a3475bb5e0c3b36169b135",
            "ef4a375c72ff4079a243decfdb7dbecf",
            "b068bf0e57974af88aee7c0309412d22",
            "ddea06cf5ed740dca9e29f303f5abb7c",
            "0e55de40347e45fcb29c595f7bfa5541",
            "c7152704ab4c4e1fad2ff79085ec7356",
            "1b8c11cb4538436e93deb7b84983ecd2",
            "d70de06933864b1cb7521dfb8b58326e",
            "3bbb65e59d8441b89956f49a856969f4",
            "071c62f506054079a9384c4222c99511",
            "4fc881395e9c4f9795e16b23465795e8",
            "ca7fbafdecf143a4bd863d768d5986c0",
            "f44ebdbfec01422e9a1fcfd5ff980233",
            "7b47515426014b14bae5f89cdf22188c",
            "4102751f6e904c06946e07738c1a5474",
            "79ab8c8bbc0942758d97c28a79680f5a",
            "3f6dc962837d4c2190f9d58e9c5bc965",
            "2a6bbf4737f641788b7c4188f7cdfb0c",
            "5faede1031d143d5b1d5aabfe938e17c",
            "33e72c8a60f64bc2b56e5b870c5f5684",
            "4e8f8e5af3e342b39f577cce037f1154",
            "ef8571f7a39340fea435cc68b2017b8c",
            "a88c274a802040ca9f49814a1bb7d998",
            "8c19764515ca4c7e999da773838e7c3c",
            "88107ded2bff459fb6b13c13bd12ea37",
            "2a4e848f947f44369425a7f32e6e48e8",
            "88f56b7744fa48a49b7622e91086d682",
            "f69ace2927394b44a8bd633d30e32bad",
            "08c8c7fb3f3e455a808171d1d134b8d3",
            "6c81af35b0914a7e8765fef2aed60c84",
            "adaeb7f2a92944769e969dff848f9060",
            "4797f23dd1124445b6ab99216f98f4f3",
            "9938f9c5cd08427a813f3a35177b5526",
            "544516bd6b18415fb871bd391760b3bb",
            "4b62d92bd3a74e308de82dc09477fbe0",
            "0283636eaa2c4f2e8001216e6099fe91",
            "86a17c58db2243f5afeb405e013a1b0d",
            "e9a240763da04853a9bec67d20fb7400",
            "e491c6c87a234e26ac6abc50008f59cc",
            "fbc87f1e26834242a390b95350ea140a",
            "3f39aa1c84fd4465980811107cea5695",
            "7f9609d8e77c43648a8cc45da423d978",
            "84b2b97266c14b329664a4ef30fe647a",
            "2a99d1157be149858c15a1378193930c",
            "1ba97dc1aadc42bebd107c8dc10f3290",
            "f91443cd9c4544f59f4896c6597bf95b",
            "43d7907de3cf4f3d9b6f25d80c9ce56c",
            "8eb3a5fb56614e63b8a515d0183df0f4",
            "7b021e588e1c49cab692d7accf030448",
            "c1c172899b03424aa4237ccfdbcabab2",
            "d846bd21c4cb4dfb80c669b8d41b5291",
            "d7be8d9688da4c09a39b4d660dcecef8",
            "419fb3007c404b8c9a1a74cb82ca8492",
            "9fcd0ffda1844aa0b171780218b3ee08",
            "729f77f633104adea0e99b170ab7b0e5",
            "06a08a347fd24b219ac14a4df4ef94e7",
            "f0e711039f264d868b5a5d73f3896165",
            "3890db83da5d44b594b51e49b0fff942",
            "9bacd20155eb4b37a426a5c950639840",
            "f8f9f1d6fda44adeb6e2ebdc27c91a39",
            "fd53d8c63edb4ab09b6d743dc078e6d5"
          ]
        },
        "id": "klu1i6_VZUiH",
        "outputId": "598c5aa6-bcb4-4088-fbea-68a656c1adca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8e9d4e8e7b944aba0add70258bb0516"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b8c11cb4538436e93deb7b84983ecd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...en2-1.5b-log-classifier-Q4_K_M.gguf:   3%|3         | 33.5MB /  986MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a6bbf4737f641788b7c4188f7cdfb0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Uploaded qwen2-1.5b-log-classifier-Q4_K_M.gguf to root of Deeps03/qwen2-1.5b-log-classifier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08c8c7fb3f3e455a808171d1d134b8d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbc87f1e26834242a390b95350ea140a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...en2-1.5b-log-classifier-Q5_K_M.gguf:   2%|2         | 25.2MB / 1.12GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d846bd21c4cb4dfb80c669b8d41b5291"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Uploaded qwen2-1.5b-log-classifier-Q5_K_M.gguf to root of Deeps03/qwen2-1.5b-log-classifier\n",
            "🗑️ Removed old gguf folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "repo_id = \"Deeps03/qwen2-1.5b-log-classifier\"\n",
        "token = \"hf token\"\n",
        "\n",
        "# Local path of your file in root\n",
        "local_file = \"/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q8_0.gguf\"\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=local_file,\n",
        "    path_in_repo=\"qwen2-1.5b-log-classifier-Q8_0.gguf\",  # filename in repo root\n",
        "    repo_id=repo_id,\n",
        "    token=token\n",
        ")\n",
        "\n",
        "print(\"✅ Uploaded qwen2-1.5b-log-classifier-Q8_0.gguf to repo root\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "c4227de293cf4d548664fdb001411b3b",
            "71b8ba4ccdf2491fabbe44c03528772c",
            "98dd91b945164dce883f80619bbb98ef",
            "c24c988cfffe4e38a6140e9f3b623297",
            "a465c4f0c25a458b96b62dd0c68d96f7",
            "2ddc6645c6f44dbc8d0af49a800b5b2f",
            "f928f926b8154a8a9e50315adfe9b3b8",
            "a64049984318414a9bd2d4620dddf428",
            "1440373c747b4fe3b73898cd1de3c756",
            "a12d074c5680490aaace82abe9813a4c",
            "55b5e0ef35624154ac79c38a477a9cbb",
            "a8ae93b74912470495237a7fbe20a518",
            "1268f268191447ea9661b65123da70b8",
            "71ad96da77974c3bbd906609143360ba",
            "5ec9bb51921c4ee6871c0b7d7a401b22",
            "ebaa7d1c7e8b412cb9147efc9ffab0d2",
            "fa6ab1774a7542a1b643cbf2cd95e3c0",
            "de8205ce748f4b06b48513094892167a",
            "ecf005a9e3474d25bc2c682dd857586a",
            "e4503aa8ffff4de69ff247328b3ec65d",
            "5336822427954df69d8e5d0676b47c73",
            "9d89813dace847b695b1bb4449624691",
            "c944135c074b4741859e00d6bdaac0fb",
            "dbd59d13422c436bac71d470b1fd17b5",
            "e2f5fc9839dc4a0889650c51697d1828",
            "7a020e67c06e4b8e963d0a779ab70ae2",
            "d0e58731659c453f92179c2dbef6a1d5",
            "9345f4c9c2ad478fa1a56017fae4aac8",
            "0c4a21a6c2f945559fe1f762b6e81754",
            "509c3a732b8b4b21bbbb2e6b02225f00",
            "3cba5b423937467baef903dca256a1dd",
            "53cccabc2bec4305a5b0255c300172cd",
            "aee7d59a7bc34ecfba64c4a6116b859a"
          ]
        },
        "id": "ycZQXehUazL6",
        "outputId": "acffdf20-f8a3-461c-e2be-74efbe76c46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4227de293cf4d548664fdb001411b3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8ae93b74912470495237a7fbe20a518"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...qwen2-1.5b-log-classifier-Q8_0.gguf:   2%|2         | 33.5MB / 1.65GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c944135c074b4741859e00d6bdaac0fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Uploaded qwen2-1.5b-log-classifier-Q8_0.gguf to repo root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzPhhDLW3eNw",
        "outputId": "96cf3656-d3f0-4aef-aef7-45d6383e1430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"Deeps03/qwen2-1.5b-log-classifier\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",        # handles GPU/CPU automatically\n",
        "    torch_dtype=torch.float16 # saves VRAM\n",
        ")\n",
        "\n",
        "# No `device=` here\n",
        "clf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm5-Eek44NHU",
        "outputId": "d87bf2b2-ae4a-4294-94c5-cce94f0cb604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python huggingface_hub datasets scikit-learn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRduSUTuVIto",
        "outputId": "104c63bb-46b6-4503-f155-e2a6495e154a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.9)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4422284 sha256=c78925fe14fbf971025f06b112754328e771a20ad56d8a466d67aed1d05d6e59\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import load_dataset\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Download GGUF model from Hugging Face\n",
        "# ----------------------------\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"Deeps03/qwen2-1.5b-log-classifier\",  # your repo\n",
        "    filename=\"qwen2-1.5b-log-classifier-Q4_K_M.gguf\"  # pick Q4, Q5, or Q8\n",
        ")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Load dataset (small subset)\n",
        "# ----------------------------\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/generated_logs.jsonl\")\n",
        "test_data = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)[\"test\"]\n",
        "\n",
        "small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Inference loop\n",
        "# ----------------------------\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for ex in small_test:\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert log analysis assistant. Your task is to classify log messages into one of:\n",
        "'incident', 'preventive_action', or 'normal'.\n",
        "Provide only the classification word.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Classify the following log message:\n",
        "Log Entry: {ex['message']}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=5,\n",
        "        stop=[\"<|im_end|>\", \"\\n\"],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    # Extract model response\n",
        "    pred = output[\"choices\"][0][\"text\"].strip().lower()\n",
        "\n",
        "    # fallback if model returns junk\n",
        "    if pred not in [\"incident\", \"preventive_action\", \"normal\"]:\n",
        "        pred = \"normal\"\n",
        "\n",
        "    y_pred.append(pred)\n",
        "    y_true.append(ex[\"label\"].lower())\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Confusion Matrix\n",
        "# ----------------------------\n",
        "labels = [\"incident\", \"preventive_action\", \"normal\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix - Qwen2 Log Classifier (Q4_K_M)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7658740947614a21aa0b230fbb69c7d3",
            "052da233e89343f6b6907019acddbe37",
            "8ab6ddcf242f4690991aa596aed0ccc0",
            "c19c97e9a9c7489cbc7721c48ccbd6ed",
            "d92a36756e5749e6895a21b887236dec",
            "b61682bb9d1b465e82635816f2ca4c32",
            "d15d3bdaf61940f7831393ce9a34c888",
            "2fde3c46218f4a6d9f5e74069cffa9fc",
            "859f046dd7c04c4f87cec15fea9e541e",
            "c0f2356bab6d420fadf3d92f16848e68",
            "aeab7d05b22a444ba63ac6d1011f4794",
            "c91d5e6dce5746508c992661842beb7a",
            "8c781740f15d40e890a60428a1fa047f",
            "32d7ccb857f84414b5b25e65cb5b652b",
            "e5515495e03540aea4c7e933e2d6bbe8",
            "38afc26170ca44a9801647cbc32e20b6",
            "fe142085bcb248079de34f037b89b7bf",
            "7cc21eff528547dcbd0797183c90eef3",
            "55826a1429a940eb9b51eb4ced884da3",
            "6b1d0d7985ff48ceba69a674167f2f69",
            "8d0b0d7bdd254483a32d9abcc51f949a",
            "e38e932e1b3e4615bb58e900e964cf22"
          ]
        },
        "id": "nmJ-sp314NWJ",
        "outputId": "3f211512-a186-4163-8f0c-d5fc0158a7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "qwen2-1.5b-log-classifier-Q4_K_M.gguf:   0%|          | 0.00/986M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7658740947614a21aa0b230fbb69c7d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /root/.cache/huggingface/hub/models--Deeps03--qwen2-1.5b-log-classifier/snapshots/e182527b13a5113ab93fd40f379cb7a3f9c48a85/qwen2-1.5b-log-classifier-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:  168 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 934.35 MiB (5.08 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.9308 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.54 B\n",
            "print_info: general.name     = Qwen2 1.5B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151646\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151645 '<|im_end|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 170 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =   596.53 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   926.96 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            "...............\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   112.00 MiB\n",
            "llama_kv_cache_unified: size =  112.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2704\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   302.18 MiB\n",
            "llama_context: graph nodes  = 1070\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151645', 'general.basename': 'Qwen2', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2 1.5B Instruct', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '1.5B', 'general.license': 'apache-2.0', 'qwen2.context_length': '32768', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '8960', 'qwen2.attention.head_count': '12'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91d5e6dce5746508c992661842beb7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    9769.23 ms /    74 tokens (  132.02 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.26 ms /     1 runs   (  199.26 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    9972.19 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4196.22 ms /    16 tokens (  262.26 ms per token,     3.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     596.80 ms /     1 runs   (  596.80 ms per token,     1.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    4796.45 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2388.72 ms /    13 tokens (  183.75 ms per token,     5.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     535.95 ms /     3 runs   (  178.65 ms per token,     5.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    2936.07 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2286.84 ms /    12 tokens (  190.57 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.53 ms /     1 runs   (  191.53 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2481.89 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2364.85 ms /    13 tokens (  181.91 ms per token,     5.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     590.44 ms /     3 runs   (  196.81 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    2961.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    3199.26 ms /    17 tokens (  188.19 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     764.96 ms /     3 runs   (  254.99 ms per token,     3.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    3984.35 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2447.98 ms /    19 tokens (  128.84 ms per token,     7.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     539.67 ms /     3 runs   (  179.89 ms per token,     5.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    2996.56 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    5170.84 ms /    25 tokens (  206.83 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.64 ms /     1 runs   (  201.64 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    5383.95 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4020.24 ms /    15 tokens (  268.02 ms per token,     3.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     587.64 ms /     3 runs   (  195.88 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    4615.12 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1120.42 ms /    18 tokens (   62.25 ms per token,    16.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.25 ms /     3 runs   (  133.75 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    1529.74 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     846.77 ms /    18 tokens (   47.04 ms per token,    21.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.26 ms /     3 runs   (  135.09 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1257.99 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2563.54 ms /    12 tokens (  213.63 ms per token,     4.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     343.08 ms /     1 runs   (  343.08 ms per token,     2.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    2910.01 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    3144.09 ms /    14 tokens (  224.58 ms per token,     4.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     238.56 ms /     1 runs   (  238.56 ms per token,     4.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3389.59 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4093.05 ms /    20 tokens (  204.65 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.65 ms /     1 runs   (  208.65 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    4305.21 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2142.31 ms /    16 tokens (  133.89 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     533.70 ms /     3 runs   (  177.90 ms per token,     5.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    2682.68 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2192.04 ms /    17 tokens (  128.94 ms per token,     7.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     556.24 ms /     3 runs   (  185.41 ms per token,     5.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2769.28 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1770.06 ms /    13 tokens (  136.16 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     179.52 ms /     1 runs   (  179.52 ms per token,     5.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1953.26 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4044.76 ms /    16 tokens (  252.80 ms per token,     3.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     328.81 ms /     1 runs   (  328.81 ms per token,     3.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    4387.09 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1767.11 ms /    14 tokens (  126.22 ms per token,     7.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.55 ms /     3 runs   (  191.18 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2352.19 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2653.58 ms /    14 tokens (  189.54 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.92 ms /     1 runs   (  189.92 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2847.35 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    3003.77 ms /    21 tokens (  143.04 ms per token,     6.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     630.06 ms /     3 runs   (  210.02 ms per token,     4.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    3647.55 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    3135.97 ms /    14 tokens (  224.00 ms per token,     4.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     556.51 ms /     3 runs   (  185.50 ms per token,     5.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    3706.44 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4326.12 ms /    18 tokens (  240.34 ms per token,     4.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.65 ms /     1 runs   (  314.65 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    4644.18 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2232.57 ms /    14 tokens (  159.47 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     561.69 ms /     3 runs   (  187.23 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2801.18 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4969.00 ms /    17 tokens (  292.29 ms per token,     3.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     563.95 ms /     3 runs   (  187.98 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    5540.15 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1666.01 ms /    13 tokens (  128.15 ms per token,     7.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     263.37 ms /     1 runs   (  263.37 ms per token,     3.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    1932.94 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1249.22 ms /    12 tokens (  104.10 ms per token,     9.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.48 ms /     1 runs   (  187.48 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1440.15 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    3031.12 ms /    17 tokens (  178.30 ms per token,     5.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     596.65 ms /     3 runs   (  198.88 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3641.79 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1364.53 ms /    22 tokens (   62.02 ms per token,    16.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     173.32 ms /     1 runs   (  173.32 ms per token,     5.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    1541.33 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1927.85 ms /    14 tokens (  137.70 ms per token,     7.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     170.97 ms /     1 runs   (  170.97 ms per token,     5.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    2105.64 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     800.02 ms /    14 tokens (   57.14 ms per token,    17.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     138.74 ms /     1 runs   (  138.74 ms per token,     7.21 tokens per second)\n",
            "llama_perf_context_print:       total time =     942.03 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     682.60 ms /    12 tokens (   56.88 ms per token,    17.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     144.92 ms /     1 runs   (  144.92 ms per token,     6.90 tokens per second)\n",
            "llama_perf_context_print:       total time =     830.80 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     664.64 ms /    12 tokens (   55.39 ms per token,    18.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.99 ms /     1 runs   (  134.99 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =     802.77 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     879.27 ms /    18 tokens (   48.85 ms per token,    20.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.15 ms /     1 runs   (  136.15 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1018.62 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1069.63 ms /    22 tokens (   48.62 ms per token,    20.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.55 ms /     1 runs   (  141.55 ms per token,     7.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1214.36 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     846.09 ms /    18 tokens (   47.00 ms per token,    21.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     407.60 ms /     3 runs   (  135.87 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1259.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     709.49 ms /    12 tokens (   59.12 ms per token,    16.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     145.63 ms /     1 runs   (  145.63 ms per token,     6.87 tokens per second)\n",
            "llama_perf_context_print:       total time =     858.37 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     730.35 ms /    13 tokens (   56.18 ms per token,    17.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.73 ms /     1 runs   (  134.73 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     868.53 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     945.04 ms /    20 tokens (   47.25 ms per token,    21.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.82 ms /     3 runs   (  135.27 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1357.33 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     826.60 ms /    17 tokens (   48.62 ms per token,    20.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     173.17 ms /     1 runs   (  173.17 ms per token,     5.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    1003.06 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2578.97 ms /    23 tokens (  112.13 ms per token,     8.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.60 ms /     1 runs   (  135.60 ms per token,     7.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2717.78 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     921.56 ms /    19 tokens (   48.50 ms per token,    20.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.43 ms /     3 runs   (  134.48 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1330.92 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     776.24 ms /    14 tokens (   55.45 ms per token,    18.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.49 ms /     1 runs   (  133.49 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =     912.89 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1461.14 ms /    29 tokens (   50.38 ms per token,    19.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.63 ms /     3 runs   (  135.54 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1874.24 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     906.34 ms /    15 tokens (   60.42 ms per token,    16.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.52 ms /     1 runs   (  134.52 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1044.16 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     856.47 ms /    18 tokens (   47.58 ms per token,    21.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.10 ms /     3 runs   (  131.37 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1256.53 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     674.46 ms /    12 tokens (   56.20 ms per token,    17.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.35 ms /     1 runs   (  132.35 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =     810.02 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     681.35 ms /    12 tokens (   56.78 ms per token,    17.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.32 ms /     1 runs   (  133.32 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =     817.93 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     937.50 ms /    20 tokens (   46.88 ms per token,    21.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.90 ms /     1 runs   (  132.90 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1073.67 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2005.20 ms /    16 tokens (  125.32 ms per token,     7.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     486.79 ms /     3 runs   (  162.26 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2498.52 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     727.37 ms /    13 tokens (   55.95 ms per token,    17.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.72 ms /     1 runs   (  132.72 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =     863.31 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     730.26 ms /    13 tokens (   56.17 ms per token,    17.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.68 ms /     3 runs   (  132.56 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    1133.84 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1100.43 ms /    22 tokens (   50.02 ms per token,    19.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     139.11 ms /     1 runs   (  139.11 ms per token,     7.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1242.82 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     764.11 ms /    13 tokens (   58.78 ms per token,    17.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     143.70 ms /     1 runs   (  143.70 ms per token,     6.96 tokens per second)\n",
            "llama_perf_context_print:       total time =     911.17 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     845.06 ms /    15 tokens (   56.34 ms per token,    17.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.02 ms /     1 runs   (  134.02 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     982.18 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     935.86 ms /    20 tokens (   46.79 ms per token,    21.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.82 ms /     1 runs   (  136.82 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1076.19 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     698.14 ms /    12 tokens (   58.18 ms per token,    17.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.51 ms /     1 runs   (  132.51 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     833.71 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    5156.27 ms /    15 tokens (  343.75 ms per token,     2.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.39 ms /     1 runs   (  134.39 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    5293.83 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     893.18 ms /    15 tokens (   59.55 ms per token,    16.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.53 ms /     1 runs   (  136.53 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1032.93 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     807.10 ms /    17 tokens (   47.48 ms per token,    21.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.84 ms /     3 runs   (  135.61 ms per token,     7.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1219.80 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     783.54 ms /    14 tokens (   55.97 ms per token,    17.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.07 ms /     1 runs   (  134.07 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     920.72 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     856.51 ms /    15 tokens (   57.10 ms per token,    17.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.49 ms /     1 runs   (  132.49 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     992.10 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1423.13 ms /    28 tokens (   50.83 ms per token,    19.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.78 ms /     3 runs   (  131.26 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1822.91 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     896.23 ms /    19 tokens (   47.17 ms per token,    21.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.36 ms /     3 runs   (  133.12 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1301.51 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     703.61 ms /    12 tokens (   58.63 ms per token,    17.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.24 ms /     1 runs   (  133.24 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     840.00 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     822.06 ms /    14 tokens (   58.72 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.71 ms /     1 runs   (  133.71 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     958.99 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1067.59 ms /    17 tokens (   62.80 ms per token,    15.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     184.50 ms /     1 runs   (  184.50 ms per token,     5.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1255.64 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1914.35 ms /    13 tokens (  147.26 ms per token,     6.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.25 ms /     1 runs   (  135.25 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2054.20 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     806.89 ms /    14 tokens (   57.64 ms per token,    17.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.75 ms /     1 runs   (  136.75 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:       total time =     946.88 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     806.28 ms /    14 tokens (   57.59 ms per token,    17.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.14 ms /     3 runs   (  132.38 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1209.25 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1412.44 ms /    29 tokens (   48.70 ms per token,    20.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1261.36 ms /     3 runs   (  420.45 ms per token,     2.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2699.10 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    8425.09 ms /    16 tokens (  526.57 ms per token,     1.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1287.63 ms /     3 runs   (  429.21 ms per token,     2.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    9746.71 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     964.81 ms /    15 tokens (   64.32 ms per token,    15.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.08 ms /     1 runs   (  135.08 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1107.26 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 62 prefix-match hit, remaining 9 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     548.00 ms /     9 tokens (   60.89 ms per token,    16.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.66 ms /     1 runs   (  134.66 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =     685.89 ms /    10 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     893.55 ms /    15 tokens (   59.57 ms per token,    16.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.81 ms /     3 runs   (  135.27 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1305.36 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     874.74 ms /    15 tokens (   58.32 ms per token,    17.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.85 ms /     1 runs   (  132.85 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1010.76 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1148.07 ms /    23 tokens (   49.92 ms per token,    20.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     138.15 ms /     1 runs   (  138.15 ms per token,     7.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.79 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     741.48 ms /    13 tokens (   57.04 ms per token,    17.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.93 ms /     1 runs   (  133.93 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =     878.63 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 31 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1828.34 ms /    31 tokens (   58.98 ms per token,    16.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.22 ms /     1 runs   (  186.22 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2017.99 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1891.22 ms /    18 tokens (  105.07 ms per token,     9.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.81 ms /     3 runs   (  132.27 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    2293.97 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1020.46 ms /    21 tokens (   48.59 ms per token,    20.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.86 ms /     3 runs   (  136.29 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1435.49 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1130.70 ms /    23 tokens (   49.16 ms per token,    20.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.53 ms /     1 runs   (  136.53 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1270.50 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     774.23 ms /    13 tokens (   59.56 ms per token,    16.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.78 ms /     1 runs   (  133.78 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     911.10 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     672.49 ms /    12 tokens (   56.04 ms per token,    17.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     138.26 ms /     1 runs   (  138.26 ms per token,     7.23 tokens per second)\n",
            "llama_perf_context_print:       total time =     814.06 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1011.83 ms /    21 tokens (   48.18 ms per token,    20.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.67 ms /     3 runs   (  132.22 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1415.01 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     879.50 ms /    18 tokens (   48.86 ms per token,    20.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.75 ms /     3 runs   (  133.58 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1286.24 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     782.41 ms /    17 tokens (   46.02 ms per token,    21.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.41 ms /     3 runs   (  132.14 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1184.65 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     770.02 ms /    13 tokens (   59.23 ms per token,    16.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.48 ms /     1 runs   (  134.48 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =     907.71 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2058.00 ms /    17 tokens (  121.06 ms per token,     8.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     483.74 ms /     3 runs   (  161.25 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2548.09 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     699.62 ms /    16 tokens (   43.73 ms per token,    22.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.15 ms /     3 runs   (  134.72 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1110.08 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     924.68 ms /    19 tokens (   48.67 ms per token,    20.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.13 ms /     1 runs   (  133.13 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1061.00 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     937.47 ms /    20 tokens (   46.87 ms per token,    21.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.62 ms /     3 runs   (  132.21 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1340.11 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1206.69 ms /    23 tokens (   52.46 ms per token,    19.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.45 ms /     1 runs   (  135.45 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1345.45 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     750.77 ms /    13 tokens (   57.75 ms per token,    17.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.62 ms /     1 runs   (  133.62 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     888.03 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     745.44 ms /    13 tokens (   57.34 ms per token,    17.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.06 ms /     1 runs   (  133.06 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     881.80 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     863.19 ms /    18 tokens (   47.95 ms per token,    20.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.14 ms /     3 runs   (  132.38 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1266.16 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     912.83 ms /    19 tokens (   48.04 ms per token,    20.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.80 ms /     3 runs   (  135.60 ms per token,     7.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1325.46 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1133.19 ms /    15 tokens (   75.55 ms per token,    13.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.61 ms /     1 runs   (  188.61 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1325.32 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1818.54 ms /    17 tokens (  106.97 ms per token,     9.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.43 ms /     3 runs   (  133.48 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    2224.85 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     816.41 ms /    14 tokens (   58.32 ms per token,    17.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.42 ms /     3 runs   (  131.47 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1216.58 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     896.94 ms /    15 tokens (   59.80 ms per token,    16.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     413.74 ms /     3 runs   (  137.91 ms per token,     7.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1316.59 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     803.31 ms /    14 tokens (   57.38 ms per token,    17.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.12 ms /     3 runs   (  133.71 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    1210.19 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     841.79 ms /    18 tokens (   46.77 ms per token,    21.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.64 ms /     3 runs   (  134.21 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1250.34 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     704.20 ms /    16 tokens (   44.01 ms per token,    22.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     137.42 ms /     1 runs   (  137.42 ms per token,     7.28 tokens per second)\n",
            "llama_perf_context_print:       total time =     846.48 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     792.77 ms /    14 tokens (   56.63 ms per token,    17.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     143.88 ms /     1 runs   (  143.88 ms per token,     6.95 tokens per second)\n",
            "llama_perf_context_print:       total time =     939.86 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1069.67 ms /    22 tokens (   48.62 ms per token,    20.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.89 ms /     1 runs   (  134.89 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1207.88 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1289.68 ms /    16 tokens (   80.60 ms per token,    12.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.09 ms /     1 runs   (  134.09 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1426.93 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1824.81 ms /    13 tokens (  140.37 ms per token,     7.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     529.63 ms /     3 runs   (  176.54 ms per token,     5.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2362.25 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     755.11 ms /    16 tokens (   47.19 ms per token,    21.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.87 ms /     3 runs   (  131.96 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1156.70 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     791.55 ms /    17 tokens (   46.56 ms per token,    21.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.83 ms /     1 runs   (  134.83 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     929.66 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1143.71 ms /    24 tokens (   47.65 ms per token,    20.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.87 ms /     1 runs   (  142.87 ms per token,     7.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.82 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     666.41 ms /    12 tokens (   55.53 ms per token,    18.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.02 ms /     1 runs   (  135.02 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =     804.56 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     734.94 ms /    16 tokens (   45.93 ms per token,    21.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.41 ms /     1 runs   (  136.41 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:       total time =     874.55 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1456.36 ms /    29 tokens (   50.22 ms per token,    19.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.25 ms /     1 runs   (  134.25 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1594.07 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     810.73 ms /    17 tokens (   47.69 ms per token,    20.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.75 ms /     1 runs   (  133.75 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     947.77 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1008.22 ms /    21 tokens (   48.01 ms per token,    20.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.21 ms /     3 runs   (  132.40 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1411.45 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     753.59 ms /    13 tokens (   57.97 ms per token,    17.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.79 ms /     3 runs   (  158.93 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1236.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2131.95 ms /    15 tokens (  142.13 ms per token,     7.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     409.06 ms /     3 runs   (  136.35 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    2547.04 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     801.03 ms /    14 tokens (   57.22 ms per token,    17.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.69 ms /     1 runs   (  142.69 ms per token,     7.01 tokens per second)\n",
            "llama_perf_context_print:       total time =     946.94 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     787.62 ms /    17 tokens (   46.33 ms per token,    21.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.82 ms /     1 runs   (  136.82 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:       total time =     927.96 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     873.19 ms /    15 tokens (   58.21 ms per token,    17.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     442.33 ms /     3 runs   (  147.44 ms per token,     6.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    1321.86 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     753.72 ms /    13 tokens (   57.98 ms per token,    17.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     145.34 ms /     1 runs   (  145.34 ms per token,     6.88 tokens per second)\n",
            "llama_perf_context_print:       total time =     902.57 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     786.41 ms /    17 tokens (   46.26 ms per token,    21.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     407.62 ms /     3 runs   (  135.88 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1200.02 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     733.12 ms /    13 tokens (   56.39 ms per token,    17.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.32 ms /     1 runs   (  133.32 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =     870.12 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     834.46 ms /    18 tokens (   46.36 ms per token,    21.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     143.71 ms /     1 runs   (  143.71 ms per token,     6.96 tokens per second)\n",
            "llama_perf_context_print:       total time =     983.59 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1135.35 ms /    22 tokens (   51.61 ms per token,    19.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.72 ms /     3 runs   (  132.91 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1540.10 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1093.26 ms /    17 tokens (   64.31 ms per token,    15.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     553.13 ms /     3 runs   (  184.38 ms per token,     5.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1653.18 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1850.18 ms /    22 tokens (   84.10 ms per token,    11.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     145.00 ms /     1 runs   (  145.00 ms per token,     6.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    1998.54 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     815.29 ms /    14 tokens (   58.23 ms per token,    17.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     419.29 ms /     3 runs   (  139.76 ms per token,     7.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1240.68 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     879.69 ms /    15 tokens (   58.65 ms per token,    17.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.27 ms /     3 runs   (  133.09 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1285.02 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1207.88 ms /    24 tokens (   50.33 ms per token,    19.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.88 ms /     3 runs   (  135.29 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1619.65 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4028.83 ms /    18 tokens (  223.82 ms per token,     4.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.43 ms /     3 runs   (  135.14 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    4440.49 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     925.90 ms /    15 tokens (   61.73 ms per token,    16.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     515.25 ms /     3 runs   (  171.75 ms per token,     5.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    1447.70 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1873.73 ms /    17 tokens (  110.22 ms per token,     9.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.99 ms /     1 runs   (  133.99 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    2014.53 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     817.47 ms /    17 tokens (   48.09 ms per token,    20.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.95 ms /     3 runs   (  131.65 ms per token,     7.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    1218.32 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1416.78 ms /    28 tokens (   50.60 ms per token,    19.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.74 ms /     3 runs   (  133.58 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1823.50 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     814.38 ms /    17 tokens (   47.90 ms per token,    20.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.08 ms /     1 runs   (  135.08 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =     952.74 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     886.34 ms /    18 tokens (   49.24 ms per token,    20.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.00 ms /     3 runs   (  132.34 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.30 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     813.77 ms /    14 tokens (   58.13 ms per token,    17.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.88 ms /     1 runs   (  133.88 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =     950.81 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     934.20 ms /    19 tokens (   49.17 ms per token,    20.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.54 ms /     1 runs   (  132.54 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1069.88 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1158.22 ms /    24 tokens (   48.26 ms per token,    20.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.92 ms /     1 runs   (  135.92 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1297.69 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1116.02 ms /    22 tokens (   50.73 ms per token,    19.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.91 ms /     1 runs   (  185.91 ms per token,     5.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1308.56 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2424.16 ms /    23 tokens (  105.40 ms per token,     9.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.96 ms /     1 runs   (  136.96 ms per token,     7.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2566.04 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1004.85 ms /    20 tokens (   50.24 ms per token,    19.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.27 ms /     1 runs   (  135.27 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1143.33 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1130.91 ms /    23 tokens (   49.17 ms per token,    20.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.67 ms /     1 runs   (  133.67 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    1267.84 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     729.61 ms /    16 tokens (   45.60 ms per token,    21.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.69 ms /     1 runs   (  133.69 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     866.61 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     773.69 ms /    13 tokens (   59.51 ms per token,    16.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.50 ms /     1 runs   (  133.50 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =     910.39 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     919.46 ms /    19 tokens (   48.39 ms per token,    20.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.70 ms /     3 runs   (  132.23 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1322.04 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     817.81 ms /    14 tokens (   58.41 ms per token,    17.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.67 ms /     1 runs   (  136.67 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:       total time =     957.55 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1118.46 ms /    23 tokens (   48.63 ms per token,    20.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.40 ms /     3 runs   (  135.47 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1531.02 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1505.26 ms /    30 tokens (   50.18 ms per token,    19.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.46 ms /     1 runs   (  134.46 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1642.96 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2110.74 ms /    13 tokens (  162.36 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.83 ms /     1 runs   (  194.83 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2309.15 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     724.73 ms /    16 tokens (   45.30 ms per token,    22.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.72 ms /     3 runs   (  134.91 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1135.73 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     907.72 ms /    19 tokens (   47.77 ms per token,    20.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.68 ms /     1 runs   (  134.68 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1045.66 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     680.29 ms /    12 tokens (   56.69 ms per token,    17.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.89 ms /     1 runs   (  135.89 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =     819.48 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     943.71 ms /    20 tokens (   47.19 ms per token,    21.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.58 ms /     3 runs   (  135.19 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1355.93 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     912.00 ms /    19 tokens (   48.00 ms per token,    20.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.46 ms /     3 runs   (  132.82 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1316.38 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     810.24 ms /    14 tokens (   57.87 ms per token,    17.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.38 ms /     1 runs   (  134.38 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =     947.86 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     724.36 ms /    16 tokens (   45.27 ms per token,    22.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.85 ms /     1 runs   (  134.85 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     862.47 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     720.65 ms /    13 tokens (   55.43 ms per token,    18.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     146.43 ms /     1 runs   (  146.43 ms per token,     6.83 tokens per second)\n",
            "llama_perf_context_print:       total time =     870.15 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     833.39 ms /    18 tokens (   46.30 ms per token,    21.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     144.63 ms /     1 runs   (  144.63 ms per token,     6.91 tokens per second)\n",
            "llama_perf_context_print:       total time =     981.29 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1117.05 ms /    18 tokens (   62.06 ms per token,    16.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     535.94 ms /     3 runs   (  178.65 ms per token,     5.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    1660.08 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1717.72 ms /    17 tokens (  101.04 ms per token,     9.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.34 ms /     1 runs   (  135.34 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1856.28 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     710.00 ms /    12 tokens (   59.17 ms per token,    16.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.30 ms /     1 runs   (  134.30 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     847.63 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     869.04 ms /    18 tokens (   48.28 ms per token,    20.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.34 ms /     3 runs   (  132.78 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1273.82 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1054.85 ms /    21 tokens (   50.23 ms per token,    19.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.57 ms /     3 runs   (  133.86 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1462.44 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1206.49 ms /    24 tokens (   50.27 ms per token,    19.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.99 ms /     1 runs   (  142.99 ms per token,     6.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1352.78 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     949.05 ms /    15 tokens (   63.27 ms per token,    15.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.27 ms /     1 runs   (  135.27 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1087.42 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     803.58 ms /    17 tokens (   47.27 ms per token,    21.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     151.18 ms /     1 runs   (  151.18 ms per token,     6.61 tokens per second)\n",
            "llama_perf_context_print:       total time =     964.75 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     839.26 ms /    18 tokens (   46.63 ms per token,    21.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     409.43 ms /     3 runs   (  136.48 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1254.78 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     901.69 ms /    18 tokens (   50.09 ms per token,    19.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     437.34 ms /     3 runs   (  145.78 ms per token,     6.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    1345.37 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2363.91 ms /    20 tokens (  118.20 ms per token,     8.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.25 ms /     3 runs   (  136.08 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    2779.87 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     814.89 ms /    14 tokens (   58.21 ms per token,    17.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.74 ms /     3 runs   (  133.25 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1220.62 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     871.49 ms /    18 tokens (   48.42 ms per token,    20.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.94 ms /     1 runs   (  136.94 ms per token,     7.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1011.69 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 61 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     767.04 ms /    13 tokens (   59.00 ms per token,    16.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.11 ms /     1 runs   (  134.11 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     904.33 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     872.62 ms /    15 tokens (   58.17 ms per token,    17.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.10 ms /     3 runs   (  134.03 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1280.72 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     897.67 ms /    15 tokens (   59.84 ms per token,    16.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.00 ms /     1 runs   (  135.00 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1035.79 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     825.14 ms /    14 tokens (   58.94 ms per token,    16.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.16 ms /     1 runs   (  134.16 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     962.53 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     671.37 ms /    11 tokens (   61.03 ms per token,    16.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.04 ms /     1 runs   (  136.04 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =     810.56 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     758.65 ms /    13 tokens (   58.36 ms per token,    17.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.20 ms /     1 runs   (  135.20 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =     897.13 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     864.29 ms /    18 tokens (   48.02 ms per token,    20.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     464.66 ms /     3 runs   (  154.89 ms per token,     6.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1337.81 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2336.67 ms /    21 tokens (  111.27 ms per token,     8.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     432.69 ms /     3 runs   (  144.23 ms per token,     6.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    2775.72 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     844.97 ms /    15 tokens (   56.33 ms per token,    17.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.34 ms /     3 runs   (  134.45 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1254.18 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 59 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     467.62 ms /     8 tokens (   58.45 ms per token,    17.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.62 ms /     1 runs   (  134.62 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =     605.44 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     764.68 ms /    13 tokens (   58.82 ms per token,    17.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.25 ms /     1 runs   (  134.25 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     902.21 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     747.66 ms /    16 tokens (   46.73 ms per token,    21.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.69 ms /     1 runs   (  132.69 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =     883.46 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1132.92 ms /    24 tokens (   47.20 ms per token,    21.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     138.98 ms /     1 runs   (  138.98 ms per token,     7.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1275.11 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     770.54 ms /    13 tokens (   59.27 ms per token,    16.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.82 ms /     1 runs   (  135.82 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =     909.61 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     698.51 ms /    12 tokens (   58.21 ms per token,    17.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.44 ms /     1 runs   (  132.44 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     834.28 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1039.24 ms /    21 tokens (   49.49 ms per token,    20.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.45 ms /     1 runs   (  135.45 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1177.87 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     725.11 ms /    16 tokens (   45.32 ms per token,    22.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.79 ms /     3 runs   (  132.93 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1129.78 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2106.61 ms /    14 tokens (  150.47 ms per token,     6.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.03 ms /     1 runs   (  188.03 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2298.62 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     956.82 ms /    14 tokens (   68.34 ms per token,    14.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.81 ms /     1 runs   (  136.81 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1096.86 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     887.24 ms /    18 tokens (   49.29 ms per token,    20.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     411.44 ms /     3 runs   (  137.15 ms per token,     7.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1304.62 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     734.53 ms /    13 tokens (   56.50 ms per token,    17.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.90 ms /     1 runs   (  141.90 ms per token,     7.05 tokens per second)\n",
            "llama_perf_context_print:       total time =     879.60 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     856.01 ms /    15 tokens (   57.07 ms per token,    17.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     143.69 ms /     1 runs   (  143.69 ms per token,     6.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    1003.45 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     769.83 ms /    17 tokens (   45.28 ms per token,    22.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.36 ms /     3 runs   (  136.12 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1184.12 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     717.44 ms /    16 tokens (   44.84 ms per token,    22.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     407.86 ms /     3 runs   (  135.95 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1131.28 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     857.70 ms /    18 tokens (   47.65 ms per token,    20.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.62 ms /     3 runs   (  132.54 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    1261.30 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     859.33 ms /    18 tokens (   47.74 ms per token,    20.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.60 ms /     3 runs   (  132.53 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1262.72 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     737.52 ms /    16 tokens (   46.10 ms per token,    21.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     418.82 ms /     3 runs   (  139.61 ms per token,     7.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1162.57 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    4268.88 ms /    12 tokens (  355.74 ms per token,     2.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     916.50 ms /     3 runs   (  305.50 ms per token,     3.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    5195.86 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 58 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     613.11 ms /    10 tokens (   61.31 ms per token,    16.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.90 ms /     3 runs   (  133.30 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1018.78 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     728.17 ms /    16 tokens (   45.51 ms per token,    21.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.87 ms /     1 runs   (  132.87 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =     864.37 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     750.31 ms /    13 tokens (   57.72 ms per token,    17.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.73 ms /     3 runs   (  131.91 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1152.31 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     763.91 ms /    13 tokens (   58.76 ms per token,    17.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.02 ms /     1 runs   (  135.02 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =     902.28 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1010.42 ms /    21 tokens (   48.12 ms per token,    20.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.49 ms /     3 runs   (  133.83 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1417.84 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     894.49 ms /    15 tokens (   59.63 ms per token,    16.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     139.30 ms /     1 runs   (  139.30 ms per token,     7.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1037.26 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     727.83 ms /    16 tokens (   45.49 ms per token,    21.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.84 ms /     1 runs   (  134.84 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     866.02 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 33 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2922.08 ms /    33 tokens (   88.55 ms per token,    11.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.89 ms /     1 runs   (  134.89 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    3060.31 ms /    34 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     698.97 ms /    12 tokens (   58.25 ms per token,    17.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     137.97 ms /     1 runs   (  137.97 ms per token,     7.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     840.22 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     829.05 ms /    14 tokens (   59.22 ms per token,    16.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.57 ms /     3 runs   (  133.19 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1234.46 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     696.00 ms /    12 tokens (   58.00 ms per token,    17.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.03 ms /     1 runs   (  134.03 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     833.25 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1021.36 ms /    20 tokens (   51.07 ms per token,    19.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.29 ms /     3 runs   (  134.76 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1431.69 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     731.24 ms /    16 tokens (   45.70 ms per token,    21.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.53 ms /     1 runs   (  132.53 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     866.92 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     881.73 ms /    15 tokens (   58.78 ms per token,    17.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.20 ms /     3 runs   (  132.07 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1284.04 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     884.41 ms /    18 tokens (   49.13 ms per token,    20.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     414.20 ms /     3 runs   (  138.07 ms per token,     7.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1305.03 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     957.32 ms /    19 tokens (   50.39 ms per token,    19.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.51 ms /     3 runs   (  133.84 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1364.65 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1421.30 ms /    17 tokens (   83.61 ms per token,    11.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     177.41 ms /     1 runs   (  177.41 ms per token,     5.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1602.18 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1403.17 ms /    12 tokens (  116.93 ms per token,     8.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.47 ms /     1 runs   (  134.47 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1540.90 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     729.61 ms /    16 tokens (   45.60 ms per token,    21.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.66 ms /     1 runs   (  133.66 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     866.66 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1147.26 ms /    24 tokens (   47.80 ms per token,    20.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.55 ms /     3 runs   (  134.18 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1555.73 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     736.55 ms /    16 tokens (   46.03 ms per token,    21.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.86 ms /     3 runs   (  133.95 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1144.50 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     823.31 ms /    18 tokens (   45.74 ms per token,    21.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     411.37 ms /     3 runs   (  137.12 ms per token,     7.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1240.55 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     785.81 ms /    14 tokens (   56.13 ms per token,    17.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     140.63 ms /     1 runs   (  140.63 ms per token,     7.11 tokens per second)\n",
            "llama_perf_context_print:       total time =     929.98 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     765.49 ms /    17 tokens (   45.03 ms per token,    22.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     140.18 ms /     1 runs   (  140.18 ms per token,     7.13 tokens per second)\n",
            "llama_perf_context_print:       total time =     909.26 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1119.22 ms /    23 tokens (   48.66 ms per token,    20.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.00 ms /     1 runs   (  133.00 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1255.69 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     725.64 ms /    16 tokens (   45.35 ms per token,    22.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.01 ms /     1 runs   (  142.01 ms per token,     7.04 tokens per second)\n",
            "llama_perf_context_print:       total time =     870.84 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     694.71 ms /    16 tokens (   43.42 ms per token,    23.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.89 ms /     1 runs   (  132.89 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     830.77 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2082.63 ms /    16 tokens (  130.16 ms per token,     7.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     183.33 ms /     1 runs   (  183.33 ms per token,     5.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    2269.43 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1065.33 ms /    22 tokens (   48.42 ms per token,    20.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.07 ms /     1 runs   (  134.07 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1202.62 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     785.55 ms /    17 tokens (   46.21 ms per token,    21.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.73 ms /     1 runs   (  142.73 ms per token,     7.01 tokens per second)\n",
            "llama_perf_context_print:       total time =     931.54 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1442.43 ms /    29 tokens (   49.74 ms per token,    20.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.94 ms /     3 runs   (  132.31 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1845.39 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     866.14 ms /    15 tokens (   57.74 ms per token,    17.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.46 ms /     3 runs   (  134.15 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1274.42 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     926.68 ms /    19 tokens (   48.77 ms per token,    20.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.98 ms /     3 runs   (  131.99 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1329.95 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     699.44 ms /    12 tokens (   58.29 ms per token,    17.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.72 ms /     1 runs   (  131.72 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =     834.29 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     707.93 ms /    16 tokens (   44.25 ms per token,    22.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.28 ms /     1 runs   (  141.28 ms per token,     7.08 tokens per second)\n",
            "llama_perf_context_print:       total time =     852.40 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     894.03 ms /    19 tokens (   47.05 ms per token,    21.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     426.95 ms /     3 runs   (  142.32 ms per token,     7.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1327.27 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1356.93 ms /    13 tokens (  104.38 ms per token,     9.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     175.47 ms /     1 runs   (  175.47 ms per token,     5.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    1543.80 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1450.72 ms /    17 tokens (   85.34 ms per token,    11.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.52 ms /     3 runs   (  132.51 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1854.14 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     901.71 ms /    18 tokens (   50.10 ms per token,    19.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.05 ms /     3 runs   (  132.35 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1304.66 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     877.49 ms /    18 tokens (   48.75 ms per token,    20.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.50 ms /     3 runs   (  135.50 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.75 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     797.62 ms /    14 tokens (   56.97 ms per token,    17.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.47 ms /     3 runs   (  134.16 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1208.46 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     718.73 ms /    16 tokens (   44.92 ms per token,    22.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     152.78 ms /     1 runs   (  152.78 ms per token,     6.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     874.98 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     853.08 ms /    15 tokens (   56.87 ms per token,    17.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.57 ms /     3 runs   (  136.19 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1268.31 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     803.35 ms /    17 tokens (   47.26 ms per token,    21.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.10 ms /     3 runs   (  132.37 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1206.16 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     793.11 ms /    17 tokens (   46.65 ms per token,    21.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.29 ms /     1 runs   (  133.29 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =     929.50 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     673.94 ms /    11 tokens (   61.27 ms per token,    16.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.78 ms /     1 runs   (  133.78 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =     810.92 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2308.50 ms /    23 tokens (  100.37 ms per token,     9.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     180.77 ms /     1 runs   (  180.77 ms per token,     5.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    2492.59 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1053.98 ms /    15 tokens (   70.27 ms per token,    14.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.11 ms /     1 runs   (  134.11 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1192.02 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1154.02 ms /    24 tokens (   48.08 ms per token,    20.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.03 ms /     1 runs   (  133.03 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1290.18 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     813.04 ms /    14 tokens (   58.07 ms per token,    17.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.93 ms /     1 runs   (  133.93 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =     950.94 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     774.83 ms /    13 tokens (   59.60 ms per token,    16.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.12 ms /     3 runs   (  133.71 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    1182.11 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1149.98 ms /    24 tokens (   47.92 ms per token,    20.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     391.83 ms /     3 runs   (  130.61 ms per token,     7.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    1547.67 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     720.03 ms /    16 tokens (   45.00 ms per token,    22.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.13 ms /     1 runs   (  133.13 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     856.31 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1130.35 ms /    23 tokens (   49.15 ms per token,    20.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.33 ms /     1 runs   (  134.33 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1267.94 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     878.48 ms /    17 tokens (   51.68 ms per token,    19.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.13 ms /     3 runs   (  132.04 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1280.52 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     763.40 ms /    13 tokens (   58.72 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     184.04 ms /     1 runs   (  184.04 ms per token,     5.43 tokens per second)\n",
            "llama_perf_context_print:       total time =     951.38 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1873.93 ms /    12 tokens (  156.16 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     180.09 ms /     1 runs   (  180.09 ms per token,     5.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    2059.62 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     847.09 ms /    17 tokens (   49.83 ms per token,    20.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.53 ms /     3 runs   (  132.51 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1252.73 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     931.65 ms /    19 tokens (   49.03 ms per token,    20.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.65 ms /     1 runs   (  161.65 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1096.49 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     918.83 ms /    19 tokens (   48.36 ms per token,    20.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.58 ms /     3 runs   (  134.19 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1327.31 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1067.08 ms /    22 tokens (   48.50 ms per token,    20.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.38 ms /     1 runs   (  132.38 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1202.73 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     916.25 ms /    19 tokens (   48.22 ms per token,    20.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.87 ms /     1 runs   (  133.87 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1053.27 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     842.17 ms /    14 tokens (   60.15 ms per token,    16.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     145.79 ms /     1 runs   (  145.79 ms per token,     6.86 tokens per second)\n",
            "llama_perf_context_print:       total time =     991.19 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     877.96 ms /    18 tokens (   48.78 ms per token,    20.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.91 ms /     1 runs   (  131.91 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1015.38 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     726.18 ms /    16 tokens (   45.39 ms per token,    22.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.07 ms /     1 runs   (  134.07 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     863.40 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     724.95 ms /    16 tokens (   45.31 ms per token,    22.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.39 ms /     1 runs   (  142.39 ms per token,     7.02 tokens per second)\n",
            "llama_perf_context_print:       total time =     870.70 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2029.42 ms /    18 tokens (  112.75 ms per token,     8.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     533.18 ms /     3 runs   (  177.73 ms per token,     5.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    2569.10 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     724.65 ms /    16 tokens (   45.29 ms per token,    22.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     130.78 ms /     1 runs   (  130.78 ms per token,     7.65 tokens per second)\n",
            "llama_perf_context_print:       total time =     858.55 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1109.53 ms /    23 tokens (   48.24 ms per token,    20.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.52 ms /     1 runs   (  133.52 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1246.20 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     720.04 ms /    16 tokens (   45.00 ms per token,    22.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.21 ms /     1 runs   (  132.21 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =     855.42 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     733.32 ms /    16 tokens (   45.83 ms per token,    21.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     149.88 ms /     1 runs   (  149.88 ms per token,     6.67 tokens per second)\n",
            "llama_perf_context_print:       total time =     886.67 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1009.20 ms /    21 tokens (   48.06 ms per token,    20.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.11 ms /     1 runs   (  132.11 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1144.67 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     824.03 ms /    14 tokens (   58.86 ms per token,    16.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.01 ms /     1 runs   (  133.01 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     960.33 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     733.85 ms /    13 tokens (   56.45 ms per token,    17.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.78 ms /     1 runs   (  141.78 ms per token,     7.05 tokens per second)\n",
            "llama_perf_context_print:       total time =     878.76 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     739.53 ms /    13 tokens (   56.89 ms per token,    17.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.50 ms /     3 runs   (  133.50 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1146.03 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1191.40 ms /    23 tokens (   51.80 ms per token,    19.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.46 ms /     3 runs   (  132.82 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1595.95 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1596.46 ms /    15 tokens (  106.43 ms per token,     9.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     180.22 ms /     1 runs   (  180.22 ms per token,     5.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1779.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1673.58 ms /    23 tokens (   72.76 ms per token,    13.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.89 ms /     1 runs   (  134.89 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1813.04 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     678.95 ms /    12 tokens (   56.58 ms per token,    17.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.58 ms /     1 runs   (  132.58 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =     814.69 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 67 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     281.98 ms /     2 runs   (  140.99 ms per token,     7.09 tokens per second)\n",
            "llama_perf_context_print:       total time =     284.95 ms /     3 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     652.67 ms /    11 tokens (   59.33 ms per token,    16.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     154.93 ms /     1 runs   (  154.93 ms per token,     6.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     810.95 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1364.90 ms /    27 tokens (   50.55 ms per token,    19.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.94 ms /     3 runs   (  132.31 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1767.76 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     877.78 ms /    15 tokens (   58.52 ms per token,    17.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.63 ms /     3 runs   (  131.21 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1277.57 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     801.01 ms /    14 tokens (   57.21 ms per token,    17.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.98 ms /     3 runs   (  135.66 ms per token,     7.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1213.86 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1217.89 ms /    24 tokens (   50.75 ms per token,    19.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.09 ms /     1 runs   (  132.09 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1354.87 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     756.81 ms /    13 tokens (   58.22 ms per token,    17.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.36 ms /     3 runs   (  131.45 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1156.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1454.61 ms /    15 tokens (   96.97 ms per token,    10.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.38 ms /     1 runs   (  200.38 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1658.35 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1377.97 ms /    16 tokens (   86.12 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.06 ms /     1 runs   (  135.06 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1516.23 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     811.15 ms /    17 tokens (   47.71 ms per token,    20.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.02 ms /     3 runs   (  132.01 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1213.03 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     809.33 ms /    14 tokens (   57.81 ms per token,    17.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.47 ms /     1 runs   (  132.47 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     944.93 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     705.38 ms /    12 tokens (   58.78 ms per token,    17.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.92 ms /     3 runs   (  131.64 ms per token,     7.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    1106.22 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     942.17 ms /    20 tokens (   47.11 ms per token,    21.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.44 ms /     3 runs   (  133.81 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1349.45 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     778.41 ms /    17 tokens (   45.79 ms per token,    21.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.96 ms /     1 runs   (  132.96 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     914.68 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     698.53 ms /    12 tokens (   58.21 ms per token,    17.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.28 ms /     1 runs   (  133.28 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =     835.00 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     885.18 ms /    15 tokens (   59.01 ms per token,    16.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.27 ms /     1 runs   (  132.27 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1020.57 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     878.15 ms /    15 tokens (   58.54 ms per token,    17.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.81 ms /     1 runs   (  131.81 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1013.09 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1562.48 ms /    29 tokens (   53.88 ms per token,    18.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     526.41 ms /     3 runs   (  175.47 ms per token,     5.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    2096.68 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1728.08 ms /    13 tokens (  132.93 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.13 ms /     1 runs   (  132.13 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1867.53 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     801.89 ms /    17 tokens (   47.17 ms per token,    21.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.13 ms /     1 runs   (  133.13 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     938.47 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     778.62 ms /    13 tokens (   59.89 ms per token,    16.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.62 ms /     1 runs   (  134.62 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =     916.45 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     759.28 ms /    13 tokens (   58.41 ms per token,    17.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.26 ms /     1 runs   (  134.26 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     896.65 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1403.92 ms /    29 tokens (   48.41 ms per token,    20.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.08 ms /     3 runs   (  132.36 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1806.88 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1020.36 ms /    21 tokens (   48.59 ms per token,    20.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.24 ms /     3 runs   (  134.08 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1428.63 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     865.27 ms /    18 tokens (   48.07 ms per token,    20.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.47 ms /     3 runs   (  130.82 ms per token,     7.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1263.45 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     729.59 ms /    16 tokens (   45.60 ms per token,    21.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.29 ms /     3 runs   (  131.10 ms per token,     7.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    1128.87 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     867.87 ms /    17 tokens (   51.05 ms per token,    19.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     138.08 ms /     1 runs   (  138.08 ms per token,     7.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1009.37 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1887.91 ms /    16 tokens (  117.99 ms per token,     8.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     513.99 ms /     3 runs   (  171.33 ms per token,     5.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    2408.46 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     939.41 ms /    15 tokens (   62.63 ms per token,    15.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.21 ms /     3 runs   (  134.40 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1348.52 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     731.19 ms /    13 tokens (   56.25 ms per token,    17.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     140.88 ms /     1 runs   (  140.88 ms per token,     7.10 tokens per second)\n",
            "llama_perf_context_print:       total time =     875.16 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     833.67 ms /    18 tokens (   46.31 ms per token,    21.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.12 ms /     1 runs   (  132.12 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =     969.24 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1135.82 ms /    23 tokens (   49.38 ms per token,    20.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.53 ms /     1 runs   (  133.53 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1273.00 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     871.54 ms /    15 tokens (   58.10 ms per token,    17.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.87 ms /     1 runs   (  131.87 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1006.50 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     823.01 ms /    18 tokens (   45.72 ms per token,    21.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.25 ms /     3 runs   (  134.42 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1232.00 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     863.78 ms /    15 tokens (   57.59 ms per token,    17.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.79 ms /     1 runs   (  136.79 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1003.89 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     840.63 ms /    15 tokens (   56.04 ms per token,    17.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.34 ms /     1 runs   (  134.34 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =     978.08 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     948.29 ms /    20 tokens (   47.41 ms per token,    21.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.40 ms /     3 runs   (  157.47 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1427.08 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2082.93 ms /    14 tokens (  148.78 ms per token,     6.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.26 ms /     1 runs   (  133.26 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    2221.24 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     689.62 ms /    12 tokens (   57.47 ms per token,    17.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.38 ms /     1 runs   (  133.38 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =     826.19 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     663.10 ms /    11 tokens (   60.28 ms per token,    16.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.00 ms /     1 runs   (  131.00 ms per token,     7.63 tokens per second)\n",
            "llama_perf_context_print:       total time =     797.17 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1305.11 ms /    27 tokens (   48.34 ms per token,    20.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.99 ms /     3 runs   (  132.66 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    1708.88 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     770.50 ms /    13 tokens (   59.27 ms per token,    16.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.28 ms /     1 runs   (  136.28 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:       total time =     910.05 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     493.36 ms /     8 tokens (   61.67 ms per token,    16.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.10 ms /     1 runs   (  142.10 ms per token,     7.04 tokens per second)\n",
            "llama_perf_context_print:       total time =     638.63 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     863.48 ms /    18 tokens (   47.97 ms per token,    20.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.90 ms /     1 runs   (  133.90 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1002.41 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     775.60 ms /    16 tokens (   48.48 ms per token,    20.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.02 ms /     3 runs   (  132.34 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1178.33 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1489.13 ms /    30 tokens (   49.64 ms per token,    20.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.80 ms /     1 runs   (  141.80 ms per token,     7.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1634.12 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     894.70 ms /    18 tokens (   49.71 ms per token,    20.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     529.56 ms /     3 runs   (  176.52 ms per token,     5.67 tokens per second)\n",
            "llama_perf_context_print:       total time =    1432.16 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1975.13 ms /    19 tokens (  103.95 ms per token,     9.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.69 ms /     1 runs   (  133.69 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    2112.37 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     693.12 ms /    12 tokens (   57.76 ms per token,    17.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.39 ms /     1 runs   (  132.39 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     829.81 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     588.85 ms /    10 tokens (   58.89 ms per token,    16.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     419.07 ms /     3 runs   (  139.69 ms per token,     7.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1014.00 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     749.61 ms /    16 tokens (   46.85 ms per token,    21.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.70 ms /     3 runs   (  131.23 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1149.17 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     802.57 ms /    17 tokens (   47.21 ms per token,    21.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.93 ms /     3 runs   (  132.31 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1205.44 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     831.71 ms /    17 tokens (   48.92 ms per token,    20.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.17 ms /     3 runs   (  131.39 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1231.66 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     855.11 ms /    17 tokens (   50.30 ms per token,    19.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.74 ms /     3 runs   (  132.91 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1259.88 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     923.32 ms /    20 tokens (   46.17 ms per token,    21.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.43 ms /     1 runs   (  142.43 ms per token,     7.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1068.92 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     876.33 ms /    15 tokens (   58.42 ms per token,    17.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.10 ms /     3 runs   (  134.03 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1284.38 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1988.43 ms /    15 tokens (  132.56 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     177.45 ms /     1 runs   (  177.45 ms per token,     5.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2169.34 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1021.76 ms /    14 tokens (   72.98 ms per token,    13.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.32 ms /     1 runs   (  134.32 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1160.71 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     759.00 ms /    13 tokens (   58.38 ms per token,    17.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.78 ms /     1 runs   (  132.78 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =     894.87 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     846.95 ms /    14 tokens (   60.50 ms per token,    16.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.91 ms /     3 runs   (  130.97 ms per token,     7.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1245.67 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     740.09 ms /    16 tokens (   46.26 ms per token,    21.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.96 ms /     3 runs   (  132.32 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1142.92 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     731.77 ms /    16 tokens (   45.74 ms per token,    21.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.61 ms /     3 runs   (  129.87 ms per token,     7.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    1127.02 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     795.69 ms /    17 tokens (   46.81 ms per token,    21.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.06 ms /     3 runs   (  132.69 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    1199.73 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     868.82 ms /    15 tokens (   57.92 ms per token,    17.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     420.99 ms /     3 runs   (  140.33 ms per token,     7.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1296.12 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1406.75 ms /    32 tokens (   43.96 ms per token,    22.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.79 ms /     1 runs   (  134.79 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1544.86 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1099.13 ms /    19 tokens (   57.85 ms per token,    17.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     177.33 ms /     1 runs   (  177.33 ms per token,     5.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1280.10 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1998.98 ms /    18 tokens (  111.05 ms per token,     9.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     402.69 ms /     3 runs   (  134.23 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    2407.53 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     768.07 ms /    13 tokens (   59.08 ms per token,    16.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     143.97 ms /     1 runs   (  143.97 ms per token,     6.95 tokens per second)\n",
            "llama_perf_context_print:       total time =     915.17 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     763.15 ms /    13 tokens (   58.70 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.89 ms /     3 runs   (  136.30 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1177.97 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1439.87 ms /    32 tokens (   45.00 ms per token,    22.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.09 ms /     1 runs   (  133.09 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1576.25 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     903.02 ms /    18 tokens (   50.17 ms per token,    19.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.82 ms /     3 runs   (  134.61 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1313.14 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     673.85 ms /    11 tokens (   61.26 ms per token,    16.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.96 ms /     1 runs   (  131.96 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =     809.02 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     858.42 ms /    18 tokens (   47.69 ms per token,    20.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.53 ms /     3 runs   (  135.51 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1270.76 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     756.07 ms /    13 tokens (   58.16 ms per token,    17.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.60 ms /     1 runs   (  135.60 ms per token,     7.37 tokens per second)\n",
            "llama_perf_context_print:       total time =     894.91 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1199.51 ms /    24 tokens (   49.98 ms per token,    20.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.91 ms /     1 runs   (  136.91 ms per token,     7.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1339.93 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2567.95 ms /    22 tokens (  116.72 ms per token,     8.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.71 ms /     1 runs   (  135.71 ms per token,     7.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2706.94 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     714.43 ms /    12 tokens (   59.54 ms per token,    16.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.13 ms /     1 runs   (  136.13 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =     853.64 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     725.43 ms /    12 tokens (   60.45 ms per token,    16.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.10 ms /     1 runs   (  134.10 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     862.84 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     897.08 ms /    15 tokens (   59.81 ms per token,    16.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.77 ms /     1 runs   (  136.77 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1037.23 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1171.78 ms /    24 tokens (   48.82 ms per token,    20.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.91 ms /     1 runs   (  134.91 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1309.84 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     810.86 ms /    17 tokens (   47.70 ms per token,    20.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.57 ms /     1 runs   (  133.57 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =     947.61 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     706.37 ms /    12 tokens (   58.86 ms per token,    16.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.30 ms /     1 runs   (  134.30 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     843.86 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     815.87 ms /    17 tokens (   47.99 ms per token,    20.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.61 ms /     3 runs   (  132.20 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1218.25 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     940.88 ms /    20 tokens (   47.04 ms per token,    21.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.50 ms /     1 runs   (  133.50 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1077.45 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     755.67 ms /    13 tokens (   58.13 ms per token,    17.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.98 ms /     1 runs   (  132.98 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     891.88 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     795.47 ms /    16 tokens (   49.72 ms per token,    20.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     177.83 ms /     1 runs   (  177.83 ms per token,     5.62 tokens per second)\n",
            "llama_perf_context_print:       total time =     976.97 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 64 prefix-match hit, remaining 7 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1436.95 ms /     7 tokens (  205.28 ms per token,     4.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     184.02 ms /     1 runs   (  184.02 ms per token,     5.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1625.40 ms /     8 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     956.63 ms /    16 tokens (   59.79 ms per token,    16.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.20 ms /     3 runs   (  136.07 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1371.41 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     872.52 ms /    18 tokens (   48.47 ms per token,    20.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.34 ms /     3 runs   (  132.45 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1275.65 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     823.61 ms /    14 tokens (   58.83 ms per token,    17.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.64 ms /     1 runs   (  134.64 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =     961.43 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     712.34 ms /    12 tokens (   59.36 ms per token,    16.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     424.64 ms /     3 runs   (  141.55 ms per token,     7.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1143.19 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1183.06 ms /    23 tokens (   51.44 ms per token,    19.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.98 ms /     1 runs   (  133.98 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1320.32 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     822.52 ms /    17 tokens (   48.38 ms per token,    20.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     407.61 ms /     3 runs   (  135.87 ms per token,     7.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1236.81 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     830.02 ms /    17 tokens (   48.82 ms per token,    20.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     413.56 ms /     3 runs   (  137.85 ms per token,     7.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1249.74 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     844.54 ms /    14 tokens (   60.32 ms per token,    16.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.75 ms /     1 runs   (  134.75 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     982.48 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     898.23 ms /    17 tokens (   52.84 ms per token,    18.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     173.35 ms /     1 runs   (  173.35 ms per token,     5.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    1076.96 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2001.72 ms /    13 tokens (  153.98 ms per token,     6.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.71 ms /     1 runs   (  134.71 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    2139.60 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     758.23 ms /    16 tokens (   47.39 ms per token,    21.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     139.69 ms /     1 runs   (  139.69 ms per token,     7.16 tokens per second)\n",
            "llama_perf_context_print:       total time =     901.10 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     825.78 ms /    14 tokens (   58.98 ms per token,    16.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     409.19 ms /     3 runs   (  136.40 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1240.98 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     827.97 ms /    14 tokens (   59.14 ms per token,    16.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.65 ms /     1 runs   (  136.65 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:       total time =     968.04 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     717.53 ms /    16 tokens (   44.85 ms per token,    22.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     405.81 ms /     3 runs   (  135.27 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1129.63 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     865.36 ms /    17 tokens (   50.90 ms per token,    19.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.32 ms /     3 runs   (  134.44 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1274.64 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     879.57 ms /    15 tokens (   58.64 ms per token,    17.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.86 ms /     3 runs   (  131.29 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.36 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     946.07 ms /    20 tokens (   47.30 ms per token,    21.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.09 ms /     3 runs   (  136.03 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1360.39 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     943.90 ms /    19 tokens (   49.68 ms per token,    20.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.50 ms /     3 runs   (  132.83 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1348.34 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2198.51 ms /    20 tokens (  109.93 ms per token,     9.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.82 ms /     3 runs   (  161.61 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2689.82 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     650.72 ms /    11 tokens (   59.16 ms per token,    16.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.47 ms /     1 runs   (  136.47 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:       total time =     790.72 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     922.00 ms /    19 tokens (   48.53 ms per token,    20.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.84 ms /     1 runs   (  132.84 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1058.04 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     708.32 ms /    12 tokens (   59.03 ms per token,    16.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.48 ms /     1 runs   (  133.48 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =     844.97 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1100.82 ms /    22 tokens (   50.04 ms per token,    19.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.22 ms /     1 runs   (  134.22 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1238.50 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     748.49 ms /    16 tokens (   46.78 ms per token,    21.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.84 ms /     3 runs   (  133.28 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1154.20 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     756.77 ms /    16 tokens (   47.30 ms per token,    21.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.14 ms /     1 runs   (  134.14 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     894.12 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     834.19 ms /    14 tokens (   59.58 ms per token,    16.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.95 ms /     3 runs   (  132.98 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1239.01 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     933.63 ms /    19 tokens (   49.14 ms per token,    20.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.48 ms /     3 runs   (  134.49 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1342.90 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     896.68 ms /    15 tokens (   59.78 ms per token,    16.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     457.85 ms /     3 runs   (  152.62 ms per token,     6.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1360.86 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2017.00 ms /    16 tokens (  126.06 ms per token,     7.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.67 ms /     3 runs   (  133.56 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    2423.59 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1154.57 ms /    23 tokens (   50.20 ms per token,    19.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     144.83 ms /     1 runs   (  144.83 ms per token,     6.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    1302.57 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     691.77 ms /    12 tokens (   57.65 ms per token,    17.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.17 ms /     1 runs   (  134.17 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     829.14 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     765.05 ms /    13 tokens (   58.85 ms per token,    16.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.76 ms /     1 runs   (  133.76 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     901.87 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     887.15 ms /    15 tokens (   59.14 ms per token,    16.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.23 ms /     3 runs   (  134.74 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1297.16 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     846.49 ms /    18 tokens (   47.03 ms per token,    21.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     139.50 ms /     1 runs   (  139.50 ms per token,     7.17 tokens per second)\n",
            "llama_perf_context_print:       total time =     989.55 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1137.37 ms /    23 tokens (   49.45 ms per token,    20.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.98 ms /     1 runs   (  134.98 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1276.33 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     934.11 ms /    19 tokens (   49.16 ms per token,    20.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.51 ms /     3 runs   (  131.17 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1333.50 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     751.64 ms /    13 tokens (   57.82 ms per token,    17.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.12 ms /     3 runs   (  132.04 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1153.59 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1247.50 ms /    18 tokens (   69.31 ms per token,    14.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     526.83 ms /     3 runs   (  175.61 ms per token,     5.69 tokens per second)\n",
            "llama_perf_context_print:       total time =    1780.80 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 61 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1566.72 ms /    18 tokens (   87.04 ms per token,    11.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.04 ms /     1 runs   (  133.04 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1706.16 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1153.55 ms /    24 tokens (   48.06 ms per token,    20.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.18 ms /     1 runs   (  135.18 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1291.88 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     863.35 ms /    18 tokens (   47.96 ms per token,    20.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.60 ms /     3 runs   (  130.87 ms per token,     7.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1261.77 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     875.17 ms /    15 tokens (   58.34 ms per token,    17.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.85 ms /     3 runs   (  131.95 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1276.97 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     951.14 ms /    20 tokens (   47.56 ms per token,    21.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.58 ms /     1 runs   (  133.58 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1087.94 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     764.25 ms /    13 tokens (   58.79 ms per token,    17.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.76 ms /     1 runs   (  133.76 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     901.20 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     705.14 ms /    12 tokens (   58.76 ms per token,    17.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.63 ms /     1 runs   (  133.63 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     841.90 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1145.38 ms /    24 tokens (   47.72 ms per token,    20.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.77 ms /     1 runs   (  131.77 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1280.27 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     868.14 ms /    18 tokens (   48.23 ms per token,    20.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     400.10 ms /     3 runs   (  133.37 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1274.06 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1726.27 ms /    16 tokens (  107.89 ms per token,     9.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     179.41 ms /     1 runs   (  179.41 ms per token,     5.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1910.95 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1135.04 ms /    17 tokens (   66.77 ms per token,    14.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.15 ms /     3 runs   (  133.05 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1540.37 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1001.32 ms /    21 tokens (   47.68 ms per token,    20.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.82 ms /     1 runs   (  133.82 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1138.44 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     885.81 ms /    18 tokens (   49.21 ms per token,    20.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     140.18 ms /     1 runs   (  140.18 ms per token,     7.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1029.51 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     947.25 ms /    20 tokens (   47.36 ms per token,    21.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.24 ms /     1 runs   (  132.24 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1082.70 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     871.13 ms /    18 tokens (   48.40 ms per token,    20.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.51 ms /     3 runs   (  131.84 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1272.52 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1072.61 ms /    22 tokens (   48.76 ms per token,    20.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.87 ms /     1 runs   (  132.87 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1209.66 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     842.02 ms /    14 tokens (   60.14 ms per token,    16.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.32 ms /     3 runs   (  131.77 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1243.09 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     864.92 ms /    15 tokens (   57.66 ms per token,    17.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     404.77 ms /     3 runs   (  134.92 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1275.77 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     900.63 ms /    15 tokens (   60.04 ms per token,    16.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     531.67 ms /     3 runs   (  177.22 ms per token,     5.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1438.96 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1763.93 ms /    12 tokens (  146.99 ms per token,     6.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.17 ms /     1 runs   (  133.17 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1900.89 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     815.42 ms /    17 tokens (   47.97 ms per token,    20.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.20 ms /     1 runs   (  133.20 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     951.70 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     956.20 ms /    20 tokens (   47.81 ms per token,    20.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.44 ms /     1 runs   (  132.44 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1091.77 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1053.12 ms /    21 tokens (   50.15 ms per token,    19.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     409.94 ms /     3 runs   (  136.65 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1469.05 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1162.13 ms /    24 tokens (   48.42 ms per token,    20.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.20 ms /     1 runs   (  134.20 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1300.94 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     966.99 ms /    20 tokens (   48.35 ms per token,    20.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.17 ms /     1 runs   (  131.17 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1101.28 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     872.63 ms /    18 tokens (   48.48 ms per token,    20.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     407.90 ms /     3 runs   (  135.97 ms per token,     7.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1286.33 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1128.94 ms /    23 tokens (   49.08 ms per token,    20.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.34 ms /     1 runs   (  133.34 ms per token,     7.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1265.43 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     807.16 ms /    17 tokens (   47.48 ms per token,    21.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.23 ms /     1 runs   (  133.23 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     943.54 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1608.52 ms /    17 tokens (   94.62 ms per token,    10.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     555.80 ms /     3 runs   (  185.27 ms per token,     5.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2171.01 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1177.35 ms /    15 tokens (   78.49 ms per token,    12.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.36 ms /     1 runs   (  132.36 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    1312.96 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     756.18 ms /    13 tokens (   58.17 ms per token,    17.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.17 ms /     1 runs   (  133.17 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     892.54 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     941.02 ms /    20 tokens (   47.05 ms per token,    21.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.51 ms /     1 runs   (  133.51 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1077.75 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     880.14 ms /    15 tokens (   58.68 ms per token,    17.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.28 ms /     1 runs   (  134.28 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1017.61 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     871.55 ms /    18 tokens (   48.42 ms per token,    20.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.69 ms /     3 runs   (  129.90 ms per token,     7.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    1267.06 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     747.48 ms /    16 tokens (   46.72 ms per token,    21.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.15 ms /     3 runs   (  130.72 ms per token,     7.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    1145.40 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     708.30 ms /    12 tokens (   59.03 ms per token,    16.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.05 ms /     3 runs   (  131.35 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1108.23 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     797.61 ms /    17 tokens (   46.92 ms per token,    21.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.54 ms /     3 runs   (  133.85 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1205.39 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     864.42 ms /    15 tokens (   57.63 ms per token,    17.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     139.50 ms /     1 runs   (  139.50 ms per token,     7.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1007.33 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1433.58 ms /    17 tokens (   84.33 ms per token,    11.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     536.81 ms /     3 runs   (  178.94 ms per token,     5.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1976.96 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1101.65 ms /    16 tokens (   68.85 ms per token,    14.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     140.59 ms /     1 runs   (  140.59 ms per token,     7.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1246.15 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1188.14 ms /    23 tokens (   51.66 ms per token,    19.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     390.74 ms /     3 runs   (  130.25 ms per token,     7.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    1584.82 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     731.48 ms /    16 tokens (   45.72 ms per token,    21.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.45 ms /     1 runs   (  133.45 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:       total time =     868.15 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     920.20 ms /    19 tokens (   48.43 ms per token,    20.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.59 ms /     1 runs   (  132.59 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    1056.05 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     757.41 ms /    13 tokens (   58.26 ms per token,    17.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.63 ms /     1 runs   (  134.63 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =     895.13 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     917.16 ms /    19 tokens (   48.27 ms per token,    20.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.87 ms /     3 runs   (  131.62 ms per token,     7.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    1318.24 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     901.89 ms /    15 tokens (   60.13 ms per token,    16.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.41 ms /     1 runs   (  132.41 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1037.68 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1554.91 ms /    30 tokens (   51.83 ms per token,    19.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     135.39 ms /     1 runs   (  135.39 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1693.68 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     948.37 ms /    20 tokens (   47.42 ms per token,    21.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.23 ms /     1 runs   (  155.23 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1107.03 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2254.52 ms /    17 tokens (  132.62 ms per token,     7.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.54 ms /     1 runs   (  134.54 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    2392.24 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     821.88 ms /    14 tokens (   58.71 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.45 ms /     1 runs   (  131.45 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =     956.45 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1125.08 ms /    23 tokens (   48.92 ms per token,    20.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.22 ms /     1 runs   (  131.22 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1259.44 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     926.95 ms /    19 tokens (   48.79 ms per token,    20.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     403.32 ms /     3 runs   (  134.44 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1336.15 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     670.14 ms /    12 tokens (   55.85 ms per token,    17.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.48 ms /     1 runs   (  134.48 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:       total time =     807.81 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     726.40 ms /    16 tokens (   45.40 ms per token,    22.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     397.53 ms /     3 runs   (  132.51 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1129.64 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 70 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     545.81 ms /     4 runs   (  136.45 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:       total time =     551.27 ms /     5 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     736.68 ms /    16 tokens (   46.04 ms per token,    21.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.68 ms /     1 runs   (  134.68 ms per token,     7.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     874.50 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     741.00 ms /    16 tokens (   46.31 ms per token,    21.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     395.12 ms /     3 runs   (  131.71 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1141.88 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1086.83 ms /    22 tokens (   49.40 ms per token,    20.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.97 ms /     1 runs   (  141.97 ms per token,     7.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1232.04 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1347.02 ms /    13 tokens (  103.62 ms per token,     9.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.91 ms /     1 runs   (  185.91 ms per token,     5.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1536.35 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1568.74 ms /    18 tokens (   87.15 ms per token,    11.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.29 ms /     3 runs   (  131.43 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1968.88 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     901.02 ms /    15 tokens (   60.07 ms per token,    16.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.46 ms /     1 runs   (  132.46 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1036.69 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     754.21 ms /    13 tokens (   58.02 ms per token,    17.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.38 ms /     1 runs   (  132.38 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     889.90 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     756.29 ms /    13 tokens (   58.18 ms per token,    17.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.32 ms /     1 runs   (  132.32 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =     891.75 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1422.39 ms /    29 tokens (   49.05 ms per token,    20.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.76 ms /     1 runs   (  133.76 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    1559.53 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     842.53 ms /    18 tokens (   46.81 ms per token,    21.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.26 ms /     1 runs   (  141.26 ms per token,     7.08 tokens per second)\n",
            "llama_perf_context_print:       total time =     986.93 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     727.78 ms /    13 tokens (   55.98 ms per token,    17.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.36 ms /     1 runs   (  131.36 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =     862.27 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     946.80 ms /    19 tokens (   49.83 ms per token,    20.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     140.69 ms /     1 runs   (  140.69 ms per token,     7.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1090.80 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     676.84 ms /    12 tokens (   56.40 ms per token,    17.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.20 ms /     1 runs   (  132.20 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =     812.20 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     848.14 ms /    14 tokens (   60.58 ms per token,    16.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.95 ms /     3 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1333.56 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2126.00 ms /    15 tokens (  141.73 ms per token,     7.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     390.50 ms /     3 runs   (  130.17 ms per token,     7.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    2522.38 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     696.34 ms /    12 tokens (   58.03 ms per token,    17.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.97 ms /     1 runs   (  132.97 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =     832.52 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     711.75 ms /    11 tokens (   64.70 ms per token,    15.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     142.81 ms /     1 runs   (  142.81 ms per token,     7.00 tokens per second)\n",
            "llama_perf_context_print:       total time =     858.30 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     839.61 ms /    18 tokens (   46.64 ms per token,    21.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     143.23 ms /     1 runs   (  143.23 ms per token,     6.98 tokens per second)\n",
            "llama_perf_context_print:       total time =     986.12 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     774.34 ms /    17 tokens (   45.55 ms per token,    21.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     136.65 ms /     1 runs   (  136.65 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:       total time =     914.52 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     733.19 ms /    16 tokens (   45.82 ms per token,    21.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     398.69 ms /     3 runs   (  132.90 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    1137.60 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1029.05 ms /    21 tokens (   49.00 ms per token,    20.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     391.84 ms /     3 runs   (  130.61 ms per token,     7.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    1426.86 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     830.36 ms /    14 tokens (   59.31 ms per token,    16.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     394.84 ms /     3 runs   (  131.61 ms per token,     7.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    1230.91 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     935.20 ms /    19 tokens (   49.22 ms per token,    20.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     393.19 ms /     3 runs   (  131.06 ms per token,     7.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    1334.23 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    2186.66 ms /    22 tokens (   99.39 ms per token,    10.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     177.65 ms /     1 runs   (  177.65 ms per token,     5.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    2368.14 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     887.60 ms /    16 tokens (   55.48 ms per token,    18.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.35 ms /     1 runs   (  131.35 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    1022.90 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     752.71 ms /    13 tokens (   57.90 ms per token,    17.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     139.44 ms /     1 runs   (  139.44 ms per token,     7.17 tokens per second)\n",
            "llama_perf_context_print:       total time =     895.98 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     786.24 ms /    17 tokens (   46.25 ms per token,    21.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     399.70 ms /     3 runs   (  133.23 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1191.81 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     771.24 ms /    17 tokens (   45.37 ms per token,    22.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     408.79 ms /     3 runs   (  136.26 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1185.71 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     945.02 ms /    20 tokens (   47.25 ms per token,    21.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.20 ms /     1 runs   (  133.20 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    1082.12 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     805.38 ms /    17 tokens (   47.38 ms per token,    21.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     132.19 ms /     1 runs   (  132.19 ms per token,     7.56 tokens per second)\n",
            "llama_perf_context_print:       total time =     940.65 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     770.68 ms /    13 tokens (   59.28 ms per token,    16.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     406.66 ms /     3 runs   (  135.55 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1183.59 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     761.02 ms /    13 tokens (   58.54 ms per token,    17.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.23 ms /     1 runs   (  134.23 ms per token,     7.45 tokens per second)\n",
            "llama_perf_context_print:       total time =     898.52 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     945.54 ms /    20 tokens (   47.28 ms per token,    21.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     131.74 ms /     1 runs   (  131.74 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1080.40 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1690.73 ms /    24 tokens (   70.45 ms per token,    14.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     182.08 ms /     1 runs   (  182.08 ms per token,     5.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1879.23 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =    1413.37 ms /    12 tokens (  117.78 ms per token,     8.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     134.63 ms /     1 runs   (  134.63 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1552.85 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     692.47 ms /    12 tokens (   57.71 ms per token,    17.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     133.71 ms /     1 runs   (  133.71 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:       total time =     829.42 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     816.97 ms /    17 tokens (   48.06 ms per token,    20.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.04 ms /     3 runs   (  130.68 ms per token,     7.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    1214.78 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     880.85 ms /    15 tokens (   58.72 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     392.63 ms /     3 runs   (  130.88 ms per token,     7.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.30 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    9769.56 ms\n",
            "llama_perf_context_print: prompt eval time =     804.65 ms /    17 tokens (   47.33 ms per token,    21.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     401.55 ms /     3 runs   (  133.85 ms per token,     7.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1212.13 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAIbCAYAAABSRxJhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAhxtJREFUeJzs3XVcFenbBvDr0J2KgGKjiAXGsibYtcbaimt3g7qsCSau7drdtbrGGqtiYLdii4qFiqIgICB5nvcPX+bnEVSQQ4xc3/3MZz0zzzxzH05w89QohBACRERERJTraeR0AERERESUPkzciIiIiGSCiRsRERGRTDBxIyIiIpIJJm5EREREMsHEjYiIiEgmmLgRERERyQQTNyIiIiKZYOJGREREJBNM3Ei2Hjx4gIYNG8LU1BQKhQK7d+9Wa/1PnjyBQqHA2rVr1VqvnLm5ucHNzS2nwyCZ8Pf3h0KhgL+/f47FoFAo4OPjo7Lv0qVLqF69OgwNDaFQKBAQEAAfHx8oFIpsjW3gwIFo0KBBtl5TDjp27Ij27dvndBi5FhM3ypSgoCD069cPxYsXh56eHkxMTFCjRg3Mnz8fHz58yNJrd+vWDTdv3sTUqVOxYcMGVKlSJUuvl526d+8OhUIBExOTNH+ODx48gEKhgEKhwKxZszJc/8uXL+Hj44OAgAA1RJt9EhMT8ddff6Fq1aowNjaGkZERqlatigULFiApKSmnw/uio0ePomfPnihVqhQMDAxQvHhx9O7dGyEhIek6v3v37jAyMsriKDNm165daNKkCfLlywcdHR3Y2tqiffv2OHbsWE6H9lWJiYlo164dwsPDMXfuXGzYsAFFihTJ9jgeP36MlStXYsyYMamOhYWFYdSoUShdujT09PRgYWGBRo0aYf/+/d+s9/Tp09J3w9u3bzMUU0qivWPHDpX9CQkJ+OWXX6ChoYHVq1enu76UOHr37p3m8bFjx6YZq5eXF/755x9cv349Q/HnGYLoO+3bt0/o6+sLMzMzMXToULF8+XKxcOFC0bFjR6GtrS369OmTZdeOjY0VAMTYsWOz7BpKpVJ8+PBBJCUlZdk1vqRbt25CS0tLaGpqim3btqU67u3tLfT09AQAMXPmzAzXf+nSJQFArFmzJkPnxcfHi/j4+AxfTx2io6OFq6urACB++eUXsXDhQrF48WLRokULAUDUrVtXxMTE5Ehs31K5cmVRrFgx8fvvv4sVK1aI0aNHC2NjY1GgQAEREhLyzfO7desmDA0NsyHSb1MqlaJ79+4CgHB2dhZTp04Vq1atElOmTBGVK1cWAMSZM2eEEEIcP35cABDHjx/PsXg/fPggEhMTpcd3794VAMSKFStUyiUmJooPHz5kW1zDhg0TpUqVSrX/3r17omDBgkJHR0f069dPrFixQsycOVM4OTkJAMLLy+uLdSYnJwsnJydhaGgoAIg3b95kKKaU12v79u3SvoSEBNG8eXOhUCjEypUrM1QfAKGnpyfMzMzS/N4oVqyY9D32eaw//fST+O233zJ0vbyCiRt9l0ePHgkjIyPh4OAgXr58mer4gwcPxLx587Ls+k+fPv3upEUOUn5RN2zYULRq1SrVcXt7e9GmTZtsS9xyQ0LUt29fAUAsWLAg1bGFCxcKAGLgwIE5ENm3nThxQiQnJ6fal94/PnJT4jZz5kwBQAwfPlwolcpUx9evXy8uXLgghMgdidvnUn7unyYnWeFrn5mEhASRL18+MW7cuFT7y5UrJwwMDMT58+dVjiUlJYkOHToIAOLvv/9Os94lS5YIS0tLMWzYMLUkbgkJCaJVq1ZCoVCI5cuXZ6guIT4mbq1atRIaGhpi9+7dKsfOnDkjAEjfY5/HOmvWLGFoaCjev3+f4ev+6Ji40Xfp37+/yl/W35KYmCgmTZokihcvLnR0dESRIkXE6NGjRVxcnEq5IkWKiGbNmolTp06JqlWrCl1dXVGsWDGxbt06qYy3t7cAoLIVKVJECPHxF1zKvz+Vcs6nDh8+LGrUqCFMTU2FoaGhKFWqlBg9erR0/PHjx2kmN0ePHhU1a9YUBgYGwtTUVLRo0ULcuXMnzes9ePBAdOvWTZiamgoTExPRvXv3dCVBKb+o165dK3R1dcW7d++kYxcvXhQAxD///JMqcQsLCxMjRowQ5cqVE4aGhsLY2Fg0btxYBAQESGVSvpw/31Kep6urqyhbtqy4fPmyqFWrltDX1xfDhg2Tjrm6ukp1de3aVejq6qZ6/g0bNhRmZmbixYsX33yu6REcHCw0NTVF3bp1v1imTp06QktLSzx//lwIIcSvv/4qnJ2dVcr88ssvAoDYs2ePtO/8+fMCgDhw4IC07927d2LYsGGiUKFCQkdHR5QoUUJMnz5dJflKeX/MnDlTLFu2THpvV6lSRVy8eDFdz8vCwkK0bt36m+XSm7j9/fffolKlSkJPT09YWloKd3d36efxebkyZcoIXV1dUbZsWbFz584vfnY+FRsbKywsLISDg0O6WqLTStxOnjwp2rZtK+zs7ISOjo4oVKiQGD58uIiNjVU5NyQkRHTv3l1qfbK2thYtWrQQjx8/lspcunRJNGzYUFhaWgo9PT1RtGhR0aNHD5V6AAhvb28hxMef4+fv+5T3c1rfEUIIsWHDBulnam5uLjp06CCePXumUuZrn5m0HDt2TAAQ/v7+Kvu3bNkiAIhJkyaleV5ERIQwMzMTZcqUSXUsLCxMWFpaikWLFknPJTOJW2JiomjdurVQKBRi6dKlGaonBQAxaNAg4ebmJtq3b69ybODAgaJ8+fJfjPX69esCgNi5c+d3XftHxjFu9F327t2L4sWLo3r16ukq37t3b0yYMAGVKlXC3Llz4erqCl9fX3Ts2DFV2YcPH6Jt27Zo0KABZs+eDXNzc3Tv3h23b98GALRu3Rpz584FAHTq1AkbNmzAvHnzMhT/7du38csvvyA+Ph6TJk3C7Nmz0aJFC5w5c+ar5x05cgSNGjVCaGgofHx84OnpibNnz6JGjRp48uRJqvLt27fH+/fv4evri/bt22Pt2rWYOHFiuuNs3bo1FAoFdu7cKe3bvHkzHBwcUKlSpVTlHz16hN27d+OXX37BnDlzMGrUKNy8eROurq54+fIlAKBMmTKYNGkSAKBv377YsGEDNmzYgNq1a0v1hIWFoUmTJnBycsK8efNQp06dNOObP38+8ufPj27duiE5ORkAsGzZMhw+fBgLFiyAra1tup/r1/z3339ITk5G165dv1ima9euSEpKwsGDBwEAtWrVwvXr1xEVFQUAEELgzJkz0NDQwKlTp6TzTp06BQ0NDdSoUQMAEBsbC1dXV2zcuBFdu3bFX3/9hRo1amD06NHw9PRMdd3Nmzdj5syZ6NevH6ZMmYInT56gdevWSExM/Opzio6ORnR0NPLly5fhn0da1q5di/bt20NTUxO+vr7o06cPdu7ciZo1ayIiIkIqt3//fnTo0AHa2trw9fVF69at0atXL1y5cuWb1zh9+jTCw8PRuXNnaGpqflec27dvR2xsLAYMGIAFCxagUaNGWLBgQarXtk2bNti1axd69OiBxYsXY+jQoXj//j2ePXsGAAgNDUXDhg3x5MkT/PHHH1iwYAHc3d1x/vz5L167X79+0piyoUOHYsOGDRg7duwXy0+dOhVdu3aFvb095syZg+HDh+Po0aOoXbu2ys8USP9nBgDOnj0LhUIBZ2dnlf179+4FgC++z01NTdGyZUvcvXsXQUFBKsfGjx8Pa2tr9OvX74vXTa+kpCR06tQJu3btwqJFizJdZ+fOnbF3715ER0dL9W/fvh2dO3f+4jmOjo7Q19f/5ndynpTTmSPJT2RkpAAgWrZsma7yAQEBAoDo3bu3yv6RI0cKAOLYsWPSviJFiggA4uTJk9K+0NBQoaurK0aMGCHt+7S141PpbXGbO3fuN/8iTavFzcnJSVhZWYmwsDBp3/Xr14WGhobo2rVrquv17NlTpc5ff/1VWFpafvGanz6PlBaWtm3binr16gkhPo5hsba2FhMnTkzzZxAXF5eqS+7x48dCV1dX5a/4r3WVpowjS+uv7M9b3IQQ4tChQwKAmDJlitSFnlb3bmYMHz5cABDXrl37YpmrV68KAMLT01MI8b/nmNKSduPGDQFAtGvXTri4uEjntWjRQqVlbvLkycLQ0FDcv39fpf4//vhDaGpqSq0tKT9/S0tLER4eLpXbs2ePACD27t371ec0efJkAUAcPXr0m8//Wy1uCQkJwsrKSpQrV05lnNa+ffsEADFhwgRpX/ny5UWhQoVUuqD8/f1VWq6/ZP78+QKA2LVr1zdjFiLtFrfPW9aEEMLX11coFArx9OlTIcTHFs+0Pt+f2rVrlwAgLl269NUY8EmL26cxfd5V+vl3xJMnT4SmpqaYOnWqSrmbN28KLS0tlf1f+8ykpUuXLml+Dzg5OQlTU9OvnjtnzhwBQPz777/SvuvXrwtNTU1x6NAhlefyvS1uKd/DixYtytD5n8P/t7iFh4cLHR0dsWHDBiGEEPv37xcKhUI8efLkq7GWKlVKNGnSJFMx/IjY4kYZltKCYWxsnK7yBw4cAIBUrRUjRowAgFQzpRwdHVGrVi3pcf78+VG6dGk8evTou2P+nJmZGQBgz549UCqV6TonJCQEAQEB6N69OywsLKT9FSpUQIMGDaTn+an+/furPK5VqxbCwsKkn2F6dO7cGf7+/nj16hWOHTuGV69effEvVV1dXWhofPxYJycnIywsDEZGRihdujSuXr2a7mvq6uqiR48e6SrbsGFD9OvXD5MmTULr1q2hp6eHZcuWpfta6fH+/XsAX3/PpRxLKevs7AwjIyOcPHkSwMeWtUKFCqFr1664evUqYmNjIYTA6dOnVd5v27dvR61atWBubo63b99KW/369ZGcnCzVl6JDhw4wNzeXHqfU9bX368mTJzFx4kS0b98edevWzciPIk2XL19GaGgoBg4cCD09PWl/s2bN4ODgIH3GXr58iZs3b6Jr164qs1RdXV1Rvnz5b14no5/9tOjr60v/jomJwdu3b1G9enUIIXDt2jWpjI6ODvz9/fHu3bs060n5DO/bt++brZvfY+fOnVAqlWjfvr3K+8Da2hr29vY4fvy4SvmMfGbCwsJU3jMp3r9//82f7efvc+Bj62GTJk3QsGHDdF3/W16/fg0tLS0UK1ZMLfWZm5ujcePG2LJlC4CPrdTVq1f/5mzelM8gqWLiRhlmYmICQPWL42uePn0KDQ0NlCxZUmW/tbU1zMzM8PTpU5X9hQsXTlWHubn5F7/Av0eHDh1Qo0YN9O7dGwUKFEDHjh3x999/fzWJS4mzdOnSqY6VKVMGb9++RUxMjMr+z59Lypd1Rp5L06ZNYWxsjG3btmHTpk2oWrVqqp9lCqVSiblz58Le3h66urrIly8f8ufPjxs3biAyMjLd1yxYsCB0dHTSXX7WrFmwsLBAQEAA/vrrL1hZWX3znDdv3uDVq1fSltKNkpa0fll9LuVYyrU1NTVRrVo1qVv01KlTqFWrFmrWrInk5GScP38ed+7cQXh4uEri9uDBAxw8eBD58+dX2erXrw/gYxfdpzL6Gt+7dw+//vorypUrh5UrV37x+WTE196bDg4O0vGU/6f1/vnSe+pTGf3sp+XZs2fSHz9GRkbInz8/XF1dAUB6j+rq6uLPP//Ef//9hwIFCqB27dqYMWMGXr16JdXj6uqKNm3aYOLEiciXLx9atmyJNWvWID4+/rtj+9SDBw8ghIC9vX2q98Ldu3dTvQ8y+pkRQqTaZ2xs/M2f7efv823btuHs2bOYPXt2uq/9LTNmzEDhwoXRtm1btXVVdu7cGX5+fnj27Bl279791W7SFEKIbF9bTw6YuFGGmZiYwNbWFrdu3crQeen9AH5p7ExaX3TpvUbK+KsU+vr6OHnyJI4cOYLffvsNN27cQIcOHdCgQYNUZTMjM88lha6uLlq3bo1169Zh165dX/3CmzZtGjw9PVG7dm1s3LgRhw4dgp+fH8qWLZvulkVAtVUkPa5duyb9Irt582a6zqlatSpsbGyk7Wvr0Tk6OgIAbty48cUyKceKFy8u7atZsyYuXbqEuLg4KXEzMzNDuXLlcOrUKSmp+zRxUyqVaNCgAfz8/NLc2rRpo3LdjLzGwcHB0qLRBw4cyFTLVU5wcHAAkP7X+HPJyclo0KAB9u/fDy8vL+zevRt+fn7SItefvkeHDx+O+/fvw9fXF3p6ehg/fjzKlCkjtcqlrDd27tw5DB48GC9evEDPnj1RuXLlr/4RkF5KpRIKhQIHDx5M833weatyRj4zlpaWaSb2jo6OiIyMlMbxpeXz9/moUaPQrl076Ojo4MmTJ3jy5Ik0/i44OFga25oRNjY28PPzg6mpKZo1a6aW9dRatGgBXV1ddOvWDfHx8elaYPfdu3dqGwP6I2HiRt/ll19+QVBQEM6dO/fNskWKFIFSqcSDBw9U9r9+/RoRERFqXfzS3Nw81aBhAKla9QBAQ0MD9erVw5w5c3Dnzh1MnToVx44dS9UFkiIlzsDAwFTH7t27h3z58sHQ0DBzT+ALOnfujGvXruH9+/dpTuhIsWPHDtSpUwerVq1Cx44d0bBhQ9SvXz/Vz0Sdf8XGxMSgR48ecHR0RN++fTFjxgxcunTpm+dt2rRJ5Rfh1yYeNGnSBJqamtiwYcMXy6xfvx46Ojpo2bKltK9WrVpISEjAli1b8OLFCylBq127tpS4lSpVCgUKFJDOKVGiBKKjo1G/fv00t7RahNMjLCwMDRs2RHx8PA4dOgQbG5vvqictX3tvBgYGSsdT/v/w4cNU5dLa97maNWvC3NwcW7Zs+a4/cG7evIn79+9j9uzZ8PLyQsuWLVG/fv0vTmIpUaIERowYgcOHD+PWrVtISEhI1bL0888/Y+rUqbh8+TI2bdqE27dvY+vWrRmOLa1rCyFQrFixNN8HP//883fX7eDggHfv3qVqBW/evDmAj+/ltERFRWHPnj2oVKmSlLgFBwdj8+bNKFasmLTNnz8fAFCpUiU0bdr0u2IsXrw4Dh06BA0NDTRq1CjV93dG6evro1WrVvD390eDBg2+mZAlJSUhODgYZcqUydR1f0RM3Oi7/P777zA0NETv3r3x+vXrVMeDgoKkL4+UL47PZ37OmTMHwMdxOOpSokQJREZGqrTMhISEYNeuXSrlwsPDU53r5OQEAF/sarGxsYGTkxPWrVunkgjdunULhw8f/u4vyPSoU6cOJk+ejIULF8La2vqL5TQ1NVO19Gzfvh0vXrxQ2ZeSYKaV5GaUl5cXnj17hnXr1mHOnDkoWrSo9Ff119SoUUPlF+GnLWWfK1SoEHr16oUjR45gyZIlqY4vXboUx44dQ79+/WBpaSntd3Fxgba2Nv78809YWFigbNmyAD4mdOfPn8eJEydUWtuAjzOBz507h0OHDqW6TkRExHfdoSEmJgZNmzbFixcvcODAAdjb22e4jq+pUqUKrKyssHTpUpWf+3///Ye7d+9KnzFbW1uUK1cO69evV2mVOnHiRLpa0QwMDODl5YW7d+/Cy8srzVbFjRs34uLFi2men9I6+el5QgjpuyJFbGws4uLiVPaVKFECxsbG0vN79+5dqut/6zOcEa1bt4ampiYmTpyY6jpCCISFhX133dWqVYMQItVM3jZt2qBs2bKYPn06Ll++rHJMqVRiwIABePfuncpM2F27dqXaOnToAOBjApgyA/97lC9fHvv370d0dDQaNGiQ6nsko0aOHAlvb2+MHz/+m2Xv3LmDuLi4dK9ckJdo5XQAJE8lSpTA5s2b0aFDB5QpUwZdu3ZFuXLlkJCQgLNnz2L79u3o3r07AKBixYro1q0bli9fjoiICLi6uuLixYtYt24dWrVq9dVp8xnVsWNHeHl54ddff8XQoUMRGxuLJUuWoFSpUiqD8ydNmoSTJ0+iWbNmKFKkCEJDQ7F48WIUKlQINWvW/GL9M2fORJMmTVCtWjX06tULHz58wIIFC2BqaprqfojqpKGhgXHjxn2z3C+//IJJkyahR48eqF69Om7evIlNmzalSopKlCgBMzMzLF26FMbGxjA0NISLi0uGByMfO3YMixcvhre3t7Q8yZo1a+Dm5obx48djxowZGarva+bMmYN79+5h4MCBOHjwIBo3bgwAOHToEPbs2YO6deti5syZKucYGBigcuXKOH/+PJo3by61NNauXRsxMTGIiYlJlbiNGjUK//77L3755Rd0794dlStXRkxMDG7evIkdO3bgyZMnGe6+cXd3x8WLF9GzZ0/cvXsXd+/elY4ZGRmhVatW36wjMTERU6ZMSbXfwsICAwcOxJ9//okePXrA1dUVnTp1wuvXrzF//nwULVoUHh4eUvlp06ahZcuWqFGjBnr06IF3795h4cKFKFeuXLq6GEeNGoXbt29j9uzZOH78ONq2bQtra2u8evUKu3fvxsWLF3H27Nk0z3VwcECJEiUwcuRIvHjxAiYmJvjnn39SdRvev38f9erVQ/v27eHo6AgtLS3s2rULr1+/llqc161bh8WLF+PXX39FiRIl8P79e6xYsQImJiZq+SOqRIkSmDJlCkaPHo0nT56gVatWMDY2xuPHj7Fr1y707dsXI0eO/K66a9asCUtLSxw5ckRlcoq2tjb++ecf1K1bFzVr1kSPHj1QpUoVREREYPPmzbh69SrGjBmD1q1bS+ek9d5JuZVdyi3JMqNatWrYuXMnmjdvjgYNGuDUqVMqfxxlRMWKFVGxYsV0lfXz84OBgQHv5ZqWbJ/HSj+U+/fviz59+oiiRYsKHR0dYWxsLGrUqCEWLFigsrhuYmKimDhxoihWrJjQ1tYWdnZ2X12A93OfL0PxpeVAhPi4sG65cuWEjo6OKF26tNi4cWOqqf5Hjx4VLVu2FLa2tkJHR0fY2tqKTp06qSwB8aUFeI8cOSJq1Kgh9PX1hYmJiWjevPkXF+D9fIr7mjVrBACVRUTTkp4FV7+0HMiIESOEjY2N0NfXFzVq1BDnzp1LcxmPPXv2CEdHR6GlpZXmArxp+bSeqKgoUaRIEVGpUiWVWwoJIYSHh4fQ0NAQ586d++pzyKiEhAQxb948UblyZWFgYCAtotqtW7dUy6CkGDVqlAAg/vzzT5X9JUuWFABEUFBQqnPev38vRo8eLUqWLCl0dHREvnz5RPXq1cWsWbNEQkKCEOLr70F8tgRFyvIKaW3fWoJDiLQXjk3ZSpQoIZXbtm2bcHZ2Frq6usLCwuKLC/Bu3bpVODg4CF1dXVGuXDnx77//ijZt2ggHB4dvxpJix44domHDhsLCwkJoaWkJGxsb0aFDB5VFZdNaDuTOnTuifv36wsjISOTLl0/06dNHWmw15T349u1bMWjQIOHg4CAMDQ2FqampcHFxUbljwNWrV0WnTp1E4cKFha6urrCyshK//PKLuHz5skqcn78W6V0OJMU///wjatasKQwNDYWhoaFwcHAQgwYNEoGBgVKZr31mvmTo0KGiZMmSaR578+aNGDFihPT+S3mtV61ala661bEA7+e2bdsmNDQ0RNWqVUVUVFS66sP/LwfyPbG6uLiILl26pD/4PEQhRAZGSRMR5SJRUVFwdXVFUFAQTp48KXWVUcY5OTkhf/788PPzy+lQ8oRHjx7BwcEB//33H+rVq/fVsjdv3kStWrVgZ2eH06dPw9TUNJuizBkBAQGoVKkSrl69ys90GjjGjYhky8TEBP/99x/y5cuHpk2bpjkJhVQlJiamGqfn7++P69evw83NLWeCyoOKFy+OXr16Yfr06d8sW758eezZswcPHjxAq1atkJCQkA0R5pzp06ejbdu2TNq+gC1uRER5yJMnT1C/fn106dIFtra2uHfvHpYuXQpTU1PcunXru8cvUe714cOHb67jaGFhka516JKTk/HmzZuvljEyMlJZ4JnUi5MTiIjyEHNzc1SuXBkrV67EmzdvYGhoiGbNmmH69OlM2n5Q27Zt++ZdHY4fP56uFtfg4OBvTmLy9vbO0slaeR1b3IiIiH5gISEhuH379lfLVK5cOc3bcH0uLi4Op0+f/mqZ4sWLf3V5H8ocJm5EREREMsHJCUREREQywTFulGsolUq8fPkSxsbGvLEwEZHMCCHw/v172NraQkMj69qF4uLi1DazVkdHB3p6emqpK7swcaNc4+XLl7Czs8vpMIiIKBOCg4NRqFChLKk7Li4O+saWQFKsWuqztrbG48ePZZW8MXGjXMPY2BgA0HnJUejoZ83N2in3+LM5bx5N9CN5HxWFksXspO/yrJCQkAAkxULXsRug+e3lS74qOQGv7qxDQkICEzei75HSPaqjbwgdA64B9KMzMTHJ6RCIKAtky1AXLT0oMpm4CYU8h/kzcSMiIiJ5UQDIbIIo06HU8kw3iYiIiPIgJm5EREQkLwoN9Wzp5Ovri6pVq8LY2BhWVlZo1aoVAgMDVcrExcVh0KBBsLS0hJGREdq0aYPXr1+rlHn27BmaNWsGAwMDWFlZYdSoUanuHfwtTNyIiIhIXhQK9WzpdOLECQwaNAjnz5+Hn58fEhMT0bBhQ8TExEhlPDw8sHfvXmzfvh0nTpzAy5cv0bp1a+l4cnIymjVrhoSEBJw9exbr1q3D2rVrMWHChAw9dY5xIyIiInnJYIvZF+tIp4MHD6o8Xrt2LaysrHDlyhXUrl0bkZGRWLVqFTZv3oy6desCANasWYMyZcrg/Pnz+Pnnn3H48GHcuXMHR44cQYECBeDk5ITJkyfDy8sLPj4+0NFJ32QLtrgRERFRnhUVFaWyxcfHf/OcyMhIAICFhQUA4MqVK0hMTET9+vWlMg4ODihcuDDOnTsHADh37hzKly+PAgUKSGUaNWqEqKiob95L9lNM3IiIiEhe1NhVamdnB1NTU2nz9fX96qWVSiWGDx+OGjVqoFy5cgCAV69eQUdHB2ZmZiplCxQogFevXkllPk3aUo6nHEsvdpUSERGRzKihq/T/266Cg4NV1pXU1dX96lmDBg3CrVu3cPr06Uxe//uwxY2IiIjyLBMTE5Xta4nb4MGDsW/fPhw/flzltl7W1tZISEhARESESvnXr1/D2tpaKvP5LNOUxyll0oOJGxEREclLNs8qFUJg8ODB2LVrF44dO4ZixYqpHK9cuTK0tbVx9OhRaV9gYCCePXuGatWqAQCqVauGmzdvIjQ0VCrj5+cHExMTODo6pjsWdpUSERGRvGTzrNJBgwZh8+bN2LNnD4yNjaUxaaamptDX14epqSl69eoFT09PWFhYwMTEBEOGDEG1atXw888/AwAaNmwIR0dH/Pbbb5gxYwZevXqFcePGYdCgQd/snv0UEzciIiKir1iyZAkAwM3NTWX/mjVr0L17dwDA3LlzoaGhgTZt2iA+Ph6NGjXC4sWLpbKamprYt28fBgwYgGrVqsHQ0BDdunXDpEmTMhQLEzciIiKSlwx2dX6xjnQSQnyzjJ6eHhYtWoRFixZ9sUyRIkVw4MCBdF83LUzciIiISF6yuas0N5Fn1ERERER5EFvciIiISF6yuas0N2HiRkRERPKSh7tKmbgRERGRvCgUakjc5NniJs90k4iIiCgPYosbERERyYuG4uOW2TpkiIkbERERyUseHuMmz6iJiIiI8iC2uBEREZG8cDkQIiIiIplgVykRERER5XZscSMiIiJ5YVcpERERkUzk4a5SJm5EREQkL3m4xU2e6SYRERFRHsQWNyIiIpIXdpUSERERyQS7SomIiIgot2OLGxEREcmMGrpKZdp2xcSNiIiI5IVdpURERESU27HFjYiIiORFoVDDrFJ5trgxcSMiIiJ5ycPLgcgzaiIiIqI8iC1uREREJC95eHICEzciIiKSlzzcVcrEjYiIiOQlD7e4yTPdJCIiIsqD2OJGRERE8sKuUiIiIiKZYFcpEREREeV2bHEjIiIiWVEoFFDk0RY3Jm5EREQkK3k5cWNXKREREdE3nDx5Es2bN4etrS0UCgV2796tcjwlmfx8mzlzplSmaNGiqY5Pnz49Q3GwxY2IiIjkRfH/W2bryICYmBhUrFgRPXv2ROvWrVMdDwkJUXn833//oVevXmjTpo3K/kmTJqFPnz7SY2Nj4wzFwcSNiIiIZCUnukqbNGmCJk2afPG4tbW1yuM9e/agTp06KF68uMp+Y2PjVGUzgl2lRERElGdFRUWpbPHx8Zmu8/Xr19i/fz969eqV6tj06dNhaWkJZ2dnzJw5E0lJSRmqmy1uREREJCvqbHGzs7NT2e3t7Q0fH59MVb1u3ToYGxun6lIdOnQoKlWqBAsLC5w9exajR49GSEgI5syZk+66mbgRERGRrKgzcQsODoaJiYm0W1dXN3P1Ali9ejXc3d2hp6enst/T01P6d4UKFaCjo4N+/frB19c33ddl4paLuLm5wcnJCfPmzct0XT4+Pti9ezcCAgK+WKZ79+6IiIhINTOGMq+EpQHq2ueDnZkeTPW1sfL8M9wMea9SpkmZ/KhW1Bz62pp4HBaL7QEheBOTIB2f0NAeloY6Kufsvf0aR+6/zZbnQOq14u8TWLDxKELDolDOviD+HNUOlcsWzemwKAvwtc566kzcTExMVBK3zDp16hQCAwOxbdu2b5Z1cXFBUlISnjx5gtKlS6erfo5xy0V27tyJyZMnq6WukSNH4ujRo2qpKyN8fHzg5OSU7dfNbXS0NPAiMg47roekebyefT7ULm6JvwNCMNf/ERKSlehfowi0NFS/iPbfCcW4A4HSdjIoLDvCJzXbefgKxs3bBa/eTeC/wQvl7AuizZBFeBP+/tsnk6zwtaZVq1ahcuXKqFix4jfLBgQEQENDA1ZWVumun4lbLmJhYZHhacFfYmRkBEtLS7XURRl393U0DtwNxY2QtL+sXUta4HDgG9wKeY+XUfHYePkFTPW0UN5G9fWPT0rG+/gkaUtIFtkRPqnZ4s3H0LVVdbi3qAaH4jaYM7ojDPR0sPHfczkdGqkZX+tsolDTlgHR0dEICAiQerIeP36MgIAAPHv2TCoTFRWF7du3o3fv3qnOP3fuHObNm4fr16/j0aNH2LRpEzw8PNClSxeYm5unOw4mbrmIm5sbhg8fDuDjIn3Tpk1Dz549YWxsjMKFC2P58uUq5Z8/f45OnTrBwsIChoaGqFKlCi5cuAAgdctXcnIyPD09YWZmBktLS/z+++8QQjUJUCqV8PX1RbFixaCvr4+KFStix44d0nF/f38oFAocPXoUVapUgYGBAapXr47AwEAAwNq1azFx4kRcv35dasZeu3at+n9QMmdpoA1TPW3cfxMj7YtLUuLpuw8oZmGgUrZ+qXyY1qw0RtUpjrr2ltDI7LpFlO0SEpMQcC8Ybj/9rxtEQ0MDrj+VxqWbj3MwMlI3vtbZ50uL3WZ0y4jLly/D2dkZzs7OAD6OV3N2dsaECROkMlu3boUQAp06dUp1vq6uLrZu3QpXV1eULVsWU6dOhYeHR6rf7d/CMW652OzZszF58mSMGTMGO3bswIABA+Dq6orSpUsjOjoarq6uKFiwIP79919YW1vj6tWrUCqVX6xr7dq1WL16NcqUKYPZs2dj165dqFu3rlTG19cXGzduxNKlS2Fvb4+TJ0+iS5cuyJ8/P1xdXaVyY8eOxezZs5E/f370798fPXv2xJkzZ9ChQwfcunULBw8exJEjRwAApqamWftDkiFjvY8fu/dxqlPA38clSccA4OSjcDyP+IDYhGQUszDAL2ULwERPC7tvvs7WeClzwiKikZysRH4L1dbU/BYmePCEr+WPhK/1j83NzS1Vg8fn+vbti759+6Z5rFKlSjh//nym42Dilos1bdoUAwcOBAB4eXlh7ty5OH78OEqXLo3NmzfjzZs3uHTpEiwsLAAAJUuW/GJd8+bNw+jRo6WpyUuXLsWhQ4ek4/Hx8Zg2bRqOHDmCatWqAQCKFy+O06dPY9myZSqJ29SpU6XHf/zxB5o1a4a4uDjo6+vDyMgIWlpa6VpcMD4+XmW9nKioqPT+aPIE/4f/G8/2MioeSUKgg5Mt9t4ORbKSXaZElHcpFFDD5AT1xJLdmLjlYhUqVJD+rVAoYG1tjdDQUAAfBzQ6OztLSdvXREZGIiQkBC4uLtI+LS0tVKlSRfrr4eHDh4iNjUWDBg1Uzk1ISJCahdOKy8bGBgAQGhqKwoULZ+j5+fr6YuLEiRk650eQ0tJmrKeFqPj/tboZ62nhRUTcF897Gv4BmhoKWBpoIzQ64YvlKHexNDOCpqZGqsHpb8KjYGWpvplslPP4WmcfBdQwq1SmmRvHuOVi2traKo8VCoXUFaqvr6/Wa0VHRwMA9u/fLw2+DAgIwJ07d1TGuX0eV8oH50tdtF8zevRoREZGSltwcHAmnoF8hMUmIjIuEaXyG0r7dLU0UMRcH4/DY794XkFTPSiFwPv4jK2yTTlLR1sLTg52OHEpUNqnVCpx8tJ9VC1fLAcjI3Xja03ZgS1uMlWhQgWsXLkS4eHh32x1MzU1hY2NDS5cuIDatWsDAJKSknDlyhVUqlQJAODo6AhdXV08e/ZMpVs0o3R0dJCcnJyusrq6umpZ6DA30tHUQH6j/63BZmmgg4KmeohNSMa7D4k48TAcDUvnx5voBITFJqBpGStExiVJa70VtdBHEXN9PHgTg/gkJYpaGODXCta4HByJD4kZT5IpZw3sXBcDJ26Ac5nCqFS2KJZsOY6YD/Fwb/5zTodGasbXOnvkxL1KcwsmbjLVqVMnTJs2Da1atYKvry9sbGxw7do12NraSmPUPjVs2DBMnz4d9vb2cHBwwJw5cxARESEdNzY2xsiRI+Hh4QGlUomaNWsiMjISZ86cgYmJCbp165auuIoWLSpNkS5UqBCMjY1/2OTsawqb62FIrf/9hf1rhY9j/i48fYfNV1/i6IO30NFSoIOzDfS1NfEoLBZLzz5F0v+PXUtKFqhUyBSNHaygpalAeEwC/B+G4fhDruMmR60bVsbbiGhMW7YfoWHvUb5UQez4axC7z35AfK2zyXcs55FmHTLExE2mdHR0cPjwYYwYMQJNmzZFUlISHB0dsWjRojTLjxgxAiEhIejWrRs0NDTQs2dP/Prrr4iMjJTKTJ48Gfnz54evry8ePXoEMzMzVKpUCWPGjEl3XG3atMHOnTtRp04dREREYM2aNejevXtmn67sPHwbi2G7bn+1zH933+C/u2/SPPY8Mg5zT3D5gB9J3/au6Nv++1uzST74WlNWUohvzW0lyiZRUVEwNTVF97XnoWNglNPhUBab/2vZnA6BiNQoKioKBSxNERkZqdZbSH1+DVNTU5h3WgUNHYNvn/AVyoRYvNvSK0vjzQpscSMiIiJZUccYt8zPSs0ZTNyIiIhIVvJy4sblQIiIiIhkgi1uREREJC+cVUpEREQkD+wqJSIiIqJcjy1uREREJCt5ucWNiRsRERHJSl5O3NhVSkRERCQTbHEjIiIiWcnLLW5M3IiIiEhe8vByIOwqJSIiIpIJtrgRERGRrLCrlIiIiEgmmLgRERERyUReTtw4xo2IiIhIJtjiRkRERPKSh2eVMnEjIiIiWWFXKRERERHlemxxIyIiIlnJyy1uTNyIiIhIVhRQQ+Im00Fu7ColIiIikgm2uBEREZGssKuUiIiISC7y8HIg7ColIiIikgm2uBEREZGssKuUiIiISCbycuLGrlIiIiKSFYVCPVtGnDx5Es2bN4etrS0UCgV2796tcrx79+5SQpmyNW7cWKVMeHg43N3dYWJiAjMzM/Tq1QvR0dEZioOJGxEREdE3xMTEoGLFili0aNEXyzRu3BghISHStmXLFpXj7u7uuH37Nvz8/LBv3z6cPHkSffv2zVAc7ColIiIiWfnYYpbZrtKMlW/SpAmaNGny1TK6urqwtrZO89jdu3dx8OBBXLp0CVWqVAEALFiwAE2bNsWsWbNga2ubrjjY4kZERETyoo5u0v9P3KKiolS2+Pj47w7L398fVlZWKF26NAYMGICwsDDp2Llz52BmZiYlbQBQv359aGho4MKFC+m+BhM3IiIiyrPs7Oxgamoqbb6+vt9VT+PGjbF+/XocPXoUf/75J06cOIEmTZogOTkZAPDq1StYWVmpnKOlpQULCwu8evUq3ddhVykRERHJijpnlQYHB8PExETar6ur+131dezYUfp3+fLlUaFCBZQoUQL+/v6oV69epmL9FFvciIiISFbUOavUxMREZfvexO1zxYsXR758+fDw4UMAgLW1NUJDQ1XKJCUlITw8/Ivj4tLCxI2IiIhIzZ4/f46wsDDY2NgAAKpVq4aIiAhcuXJFKnPs2DEolUq4uLiku152lRIREZGsaGgooKGRua5SkcHzo6OjpdYzAHj8+DECAgJgYWEBCwsLTJw4EW3atIG1tTWCgoLw+++/o2TJkmjUqBEAoEyZMmjcuDH69OmDpUuXIjExEYMHD0bHjh3TPaMUYIsbERERyUxOLMB7+fJlODs7w9nZGQDg6ekJZ2dnTJgwAZqamrhx4wZatGiBUqVKoVevXqhcuTJOnTql0vW6adMmODg4oF69emjatClq1qyJ5cuXZygOtrgRERERfYObmxuEEF88fujQoW/WYWFhgc2bN2cqDiZuREREJCt5+V6lTNyIiIhIVr6nqzOtOuSIiRsRERHJSl5ucePkBCIiIiKZYIsbERERyUpebnFj4kZERESykpfHuLGrlIiIiEgm2OJGREREsqKAGrpKIc8mNyZuREREJCvsKiUiIiKiXI8tbkRERCQrnFVKREREJBPsKiUiIiKiXI8tbkRERCQr7ColIiIikom83FXKxI2IiIhkJS+3uHGMGxEREZFMsMWNcp0/m5eBiYlJTodBWcy86uCcDoGy0btLC3M6BPqRqKGrVKY3TmDiRkRERPLCrlIiIiIiyvXY4kZERESywlmlRERERDLBrlIiIiIiyvXY4kZERESywq5SIiIiIplgVykRERER5XpscSMiIiJZycstbkzciIiISFY4xo2IiIhIJvJyixvHuBERERHJBFvciIiISFbYVUpEREQkE+wqJSIiIqJcjy1uREREJCsKqKGrVC2RZD+2uBEREZGsaCgUatky4uTJk2jevDlsbW2hUCiwe/du6VhiYiK8vLxQvnx5GBoawtbWFl27dsXLly9V6ihatKjUzZuyTZ8+PWPPPUOliYiIiPKgmJgYVKxYEYsWLUp1LDY2FlevXsX48eNx9epV7Ny5E4GBgWjRokWqspMmTUJISIi0DRkyJENxsKuUiIiIZCUnZpU2adIETZo0SfOYqakp/Pz8VPYtXLgQP/30E549e4bChQtL+42NjWFtbZ3heFOwxY2IiIhk5fPuxu/dACAqKkpli4+PV0uMkZGRUCgUMDMzU9k/ffp0WFpawtnZGTNnzkRSUlKG6mWLGxEREeVZdnZ2Ko+9vb3h4+OTqTrj4uLg5eWFTp06wcTERNo/dOhQVKpUCRYWFjh79ixGjx6NkJAQzJkzJ911M3EjIiIiWdFQfNwyWwcABAcHqyRXurq6mao3MTER7du3hxACS5YsUTnm6ekp/btChQrQ0dFBv3794Ovrm+7rMnEjIiIieVGoYQHd/z/dxMREJXHLjJSk7enTpzh27Ng363VxcUFSUhKePHmC0qVLp+saTNyIiIhIVnLjLa9SkrYHDx7g+PHjsLS0/OY5AQEB0NDQgJWVVbqvw8SNiIiI6Buio6Px8OFD6fHjx48REBAACwsL2NjYoG3btrh69Sr27duH5ORkvHr1CgBgYWEBHR0dnDt3DhcuXECdOnVgbGyMc+fOwcPDA126dIG5uXm642DiRkRERLKi+P//MltHRly+fBl16tSRHqeMV+vWrRt8fHzw77//AgCcnJxUzjt+/Djc3Nygq6uLrVu3wsfHB/Hx8ShWrBg8PDxUxr2lBxM3IiIikhV1Tk5ILzc3Nwghvnj8a8cAoFKlSjh//nzGLpoGruNGREREJBNscSMiIiJZ+XQB3czUIUfpStxS+m3TI637chERERGpS26cVZpd0pW4tWrVKl2VKRQKJCcnZyYeIiIiIvqCdCVuSqUyq+MgIiIiShcNhQIamWwyy+z5OSVTY9zi4uKgp6enrliIiIiIvikvd5VmeFZpcnIyJk+ejIIFC8LIyAiPHj0CAIwfPx6rVq1Se4BERERE9FGGE7epU6di7dq1mDFjBnR0dKT95cqVw8qVK9UaHBEREdHnUmaVZnaTowwnbuvXr8fy5cvh7u4OTU1NaX/FihVx7949tQZHRERE9LmUrtLMbnKU4TFuL168QMmSJVPtVyqVSExMVEtQRERERF+SlycnZLjFzdHREadOnUq1f8eOHXB2dlZLUERERESUWoZb3CZMmIBu3brhxYsXUCqV2LlzJwIDA7F+/Xrs27cvK2IkIiIikij+f8tsHXKU4Ra3li1bYu/evThy5AgMDQ0xYcIE3L17F3v37kWDBg2yIkYiIiIiSV6enPBd67jVqlULfn5+6o6FiIiIiL7iuxfgvXz5Mu7evQvg47i3ypUrqy0oIiIioi/RUHzcMluHHGU4cXv+/Dk6deqEM2fOwMzMDAAQERGB6tWrY+vWrShUqJC6YyQiIiKSqKOrU65dpRke49a7d28kJibi7t27CA8PR3h4OO7evQulUonevXtnRYxEREREhO9ocTtx4gTOnj2L0qVLS/tKly6NBQsWoFatWmoNjoiIiCgtMm0wy7QMJ252dnZpLrSbnJwMW1tbtQRFRERE9CXsKs2AmTNnYsiQIbh8+bK07/Llyxg2bBhmzZql1uCIiIiI6H/S1eJmbm6ukpnGxMTAxcUFWlofT09KSoKWlhZ69uyJVq1aZUmgRERERABnlX7TvHnzsjgMIiIiovTJy12l6UrcunXrltVxEBEREaVLXr7l1XcvwAsAcXFxSEhIUNlnYmKSqYCIiIiIKG0ZTtxiYmLg5eWFv//+G2FhYamOJycnqyUwIiIiorRoKBTQyGRXZ2bPzykZnlX6+++/49ixY1iyZAl0dXWxcuVKTJw4Eba2tli/fn1WxEhEREQkUSjUs8lRhlvc9u7di/Xr18PNzQ09evRArVq1ULJkSRQpUgSbNm2Cu7t7VsRJRERElOdluMUtPDwcxYsXB/BxPFt4eDgAoGbNmjh58qR6oyMiIiL6TMqs0sxucpThFrfixYvj8ePHKFy4MBwcHPD333/jp59+wt69e6WbzhNR+qz4+wQWbDyK0LAolLMviD9HtUPlskVzOixKJ4/uDfFLnYqwL1IAcfGJuHjjEXwW7sHDp6FSGV0dLUwZ3hqtG1SGjo4Wjp2/i5F/bsOb8PdSmXeXFqaqu9eYNdjpdyVbngepFz/XWU8dXZ0yzdsy3uLWo0cPXL9+HQDwxx9/YNGiRdDT04OHhwdGjRql9gDlpmjRorJf987HxwdOTk45HcYPb+fhKxg3bxe8ejeB/wYvlLMviDZDFqn8QqfcrXqlkli5/SQa9pyF1oMXQltLEzsXDIaBno5UZppHGzSuVQ7dR6/CL/3mwTqfKTbM6J2qroETN6B049HStv/E9ex8KqQm/FxTVstw4ubh4YGhQ4cCAOrXr4979+5h8+bNuHbtGoYNG6b2AHOrtWvXptnCeOnSJfTt2zf7A/pOCoUCu3fvVtk3cuRIHD16NGcCykMWbz6Grq2qw71FNTgUt8Gc0R1hoKeDjf+ey+nQKJ3aDV2MLfsu4N6jV7j14AUGTtwIOxsLOJWxAwCYGOqhS8tqGDt3J05dvo/r94IxeNJGuFQsgSrliqrUFfn+A0LD3ktbfEJSDjwjyix+rrNHyqzSzG5ylOHE7XNFihRB69atUaFCBXXE81WfrxmXG+XPnx8GBgY5HUamGBkZwdLSMqfD+KElJCYh4F4w3H4qLe3T0NCA60+lcenm4xyMjDLDxEgPAPAuKhYAULFMYehoa8H/YqBU5sHT1wgOCUfV8sVUzp35e3s89JuOI2tHwr35z9kXNKkNP9fZJy/PKk1X4vbXX3+le8sINzc3DB48GIMHD4apqSny5cuH8ePHQwgB4GO34+TJk9G1a1eYmJhILVmnT59GrVq1oK+vDzs7OwwdOhQxMTEAgDFjxsDFxSXVtSpWrIhJkyZJj1euXIkyZcpAT08PDg4OWLx4sXTsyZMnUCgU2LlzJ+rUqQMDAwNUrFgR5859/IvJ398fPXr0QGRkpDTA0cfHR4o5pau0c+fO6NChg0ociYmJyJcvn7R0ilKphK+vL4oVKwZ9fX1UrFgRO3bsSNfPLzk5Gb169ZLOLV26NObPn5+q3OrVq1G2bFno6urCxsYGgwcPlmIFgF9//RUKhUJ6/HlXqVKpxKRJk1CoUCHo6urCyckJBw8eTPfPi1ILi4hGcrIS+S2MVfbntzBBaFhUDkVFmaFQKODr2RbnA4JwNygEAFDA0gTxCYmIiv6gUjY0PAoFLP+3WPnUpfvQc/Rq/DpoIfYeC8Asrw7o28E1W+OnzOPnmrJDuiYnzJ07N12VKRQKqRs1vdatW4devXrh4sWLuHz5Mvr27YvChQujT58+AIBZs2ZhwoQJ8Pb2BgAEBQWhcePGmDJlClavXo03b95Iyd+aNWvg7u4OX19fBAUFoUSJEgCA27dv48aNG/jnn38AAJs2bcKECROwcOFCODs749q1a+jTpw8MDQ1Vbu81duxYzJo1C/b29hg7diw6deqEhw8fonr16pg3bx4mTJiAwMCPf0kbGRmlem7u7u5o164doqOjpeOHDh1CbGwsfv31VwCAr68vNm7ciKVLl8Le3h4nT55Ely5dkD9/fri6fv2LW6lUolChQti+fTssLS1x9uxZ9O3bFzY2Nmjfvj0AYMmSJfD09MT06dPRpEkTREZG4syZMwA+dutaWVlhzZo1aNy4MTQ1NdO8zvz58zF79mwsW7YMzs7OWL16NVq0aIHbt2/D3t7+mz8vLa2032bx8fGIj4+XHkdF8YuN5GvW7+1RpoQNmvRJ3/elyrmr/veH0M37z2Ggr4uhv9XH8m0n1Bki0Q8jL9+rNF0tbo8fP07X9ujRowwHYGdnh7lz56J06dJwd3fHkCFDVBLFunXrYsSIEShRogRKlCgBX19fuLu7Y/jw4bC3t0f16tXx119/Yf369YiLi0PZsmVRsWJFbN68Wapj06ZNcHFxQcmSJQEA3t7emD17Nlq3bo1ixYqhdevW8PDwwLJly1RiGzlyJJo1a4ZSpUph4sSJePr0KR4+fAgdHR2YmppCoVDA2toa1tbWaSZujRo1gqGhIXbt2iXt27x5M1q0aAFjY2PEx8dj2rRpWL16NRo1aoTixYuje/fu6NKlS6pY0qKtrY2JEyeiSpUqKFasGNzd3dGjRw/8/fffUpkpU6ZgxIgRGDZsGEqVKoWqVati+PDhAD526wKAmZkZrK2tpcefmzVrFry8vNCxY0eULl0af/75J5ycnFJNwvjSz+tLfH19YWpqKm12dnbffM4/CkszI2hqaqQasPwmPApWlrxtnNzMGNUOjWqVQ/MBf+FlaIS0/3VYFHR1tGFipK9S3srCBK+/0gJz5dYTFCxgDh3tTN2VkLIZP9fZR0NNW0acPHkSzZs3h62tbZrjw4UQmDBhAmxsbKCvr4/69evjwYMHKmXCw8Ph7u4OExMTmJmZoVevXoiOjs7wc89RP//8s0rWW61aNTx48EC6dVaVKlVUyl+/fh1r166FkZGRtDVq1AhKpRKPH38cQ+Du7i4lbkIIbNmyRVoYOCYmBkFBQejVq5dKHVOmTEFQUJDKtT4dt2djYwMACA0NRXppaWmhffv22LRpk3TtPXv2SLE8fPgQsbGxaNCggUos69evTxXLlyxatAiVK1dG/vz5YWRkhOXLl+PZs2dSrC9fvkS9evXSHfPnoqKi8PLlS9SoUUNlf40aNXD37l2VfRn9eY0ePRqRkZHSFhwc/N1xyo2OthacHOxw4tL/xj4plUqcvHQ/1dgnyt1mjGqHZm4V0WLAX3j2UvU2gNfvPkNCYhJcq/5vzFPJIlaws7H46pin8qUK4V1kDBISOUFBTvi5zj45sY5bTEwMKlasiEWLFqV5fMaMGfjrr7+wdOlSXLhwAYaGhmjUqBHi4uKkMu7u7rh9+zb8/Pywb98+nDx5MsMTGnP9n3OGhoYqj6Ojo9GvX780u2QLFy4MAOjUqRO8vLxw9epVfPjwAcHBwdJYs5TMdsWKFanGwn3eVaitrS39O+UFViqVGYrf3d0drq6uCA0NhZ+fH/T19dG4cWOVWPbv34+CBQuqnKerq/vNurdu3YqRI0di9uzZqFatGoyNjTFz5kxcuHABAKCvr/+NGtQroz8vXV3ddD3PH9XAznUxcOIGOJcpjEpli2LJluOI+RDPgekyMsurPdo2qoLOI5cjOjYOVpYfxzZFRcchLj4RUTFx2LjnHKZ6tMa7qBi8j4nDjFHtcPHGI1y+9QQA0LhWOeS3MMblW08QF5+IOi4O8OjREAs3cma3HPFz/eNq0qQJmjRpkuYxIQTmzZuHcePGoWXLlgCA9evXo0CBAti9ezc6duyIu3fv4uDBg7h06ZLUKLVgwQI0bdoUs2bNgq2tbbriyPHELSXJSHH+/HnY29t/cbxVpUqVcOfOHanbMy2FChWCq6srNm3ahA8fPqBBgwawsrICABQoUAC2trZ49OhRpm7PpaOjI7UKfk316tVhZ2eHbdu24b///kO7du2kBMfR0RG6urp49uzZN8ezpeXMmTOoXr06Bg4cKO37tKXO2NgYRYsWxdGjR1GnTp0069DW1v7q8zAxMYGtrS3OnDmjEuOZM2fw008/ZThm+p/WDSvjbUQ0pi3bj9Cw9yhfqiB2/DWIXSoy0qttbQDA/mXDVfYPnLgBW/Z9/G4bM/cfKIXA+j97qyzAmyIxKRm929XGVI82UCgUePz8DcbN3Yl1u89m2/Mg9eHnOnsoFICGmhbg/Xx89fc0Kjx+/BivXr1C/fr1pX2mpqZwcXHBuXPn0LFjR5w7dw5mZmYqPYn169eHhoYGLly4II19/5YcT9yePXsGT09P9OvXD1evXsWCBQswe/bsL5b38vLCzz//jMGDB6N3794wNDTEnTt34Ofnh4UL/7f6uLu7O7y9vZGQkJBqcsXEiRMxdOhQmJqaonHjxoiPj8fly5fx7t07eHp6pivuokWLIjo6GkePHkXFihVhYGDwxWVAOnfujKVLl+L+/fs4fvy4tN/Y2BgjR46Eh4cHlEolatasKU0eMDExUZkokRZ7e3usX78ehw4dQrFixbBhwwZcunQJxYr9r0nex8cH/fv3h5WVFZo0aYL379/jzJkzGDJkiPQ8jh49iho1akBXVxfm5uaprjNq1Ch4e3ujRIkScHJywpo1axAQECB1AdP369veFX3bc/agXJlXHfzNMvEJSRg142+MmvF3msePnruLo+fupnmM5Imf66ynoYbELeX8z8dXe3t7SytFpNerV68AfGwc+lSBAgWkY69evZIakVJoaWnBwsJCKpMeOZ64de3aFR8+fMBPP/0ETU1NDBs27Kv9vRUqVMCJEycwduxY1KpVC0IIlChRItWyG23btsXgwYOhqamJVq1aqRzr3bs3DAwMMHPmTIwaNQqGhoYoX768NGg/PapXr47+/fujQ4cOCAsL++oL7e7ujqlTp6JIkSKpxopNnjwZ+fPnh6+vLx49egQzMzNUqlQJY8aM+WYM/fr1w7Vr19ChQwcoFAp06tQJAwcOxH///SeV6datG+Li4jB37lyMHDkS+fLlQ9u2baXjs2fPhqenJ1asWIGCBQviyZMnqa4zdOhQREZGYsSIEQgNDYWjoyP+/fdflRmlREREchQcHAwTk/+1iOb2ITwKkbJoWgacOnUKy5YtQ1BQEHbs2IGCBQtiw4YNKFasGGrWrJnuetzc3NKcnUh5U1RUFExNTfE6LFLlQ0Q/pvS0VtGPI637sdKPJSoqCgUsTREZmXXf4Sm/JwZtvQxdg9SrOWREfGw0FnWs8l3xKhQK7Nq1S2oYevToEUqUKIFr166prIPq6uoKJycnzJ8/H6tXr8aIESPw7t076XhSUhL09PSwffv2dHeVZnhW6T///INGjRpBX18f165dk9bhioyMxLRp0zJaHREREVGGpHSVZnZTl2LFisHa2lrldpFRUVG4cOECqlWrBuDjqhkRERG4cuWKVObYsWNQKpVp3jjgi889o8FNmTIFS5cuxYoVK1RmEdaoUQNXr17NaHX0Ff3791dZJuTTrX///jkdHhERUZ4RHR2NgIAABAQEAPg4ISEgIADPnj2DQqHA8OHDMWXKFPz777+4efMmunbtCltbW6lVrkyZMmjcuDH69OmDixcv4syZMxg8eDA6duyY7hmlwHeMcQsMDETt2rVT7Tc1NUVERESG6vL398/o5fOUSZMmYeTIkWkeY1ciERHlVeq412hGz798+bLKCg0pkxm7deuGtWvX4vfff0dMTAz69u2LiIgI1KxZEwcPHoSenp50zqZNmzB48GDUq1cPGhoaaNOmTYZvF5rhxM3a2hoPHz6U7muZ4vTp0yhevHhGq6OvsLKySjUDhYiIKK/TUCigkcnMLaPnu7m54WvTAhQKBSZNmqRyX/TPWVhYqNzZ6XtkuKu0T58+GDZsGC5cuACFQoGXL19i06ZNGDlyJAYMGJCpYIiIiIjoyzLc4vbHH39AqVSiXr16iI2NRe3ataGrq4uRI0dKa4MRERERZZXvuddoWnXIUYYTN4VCgbFjx2LUqFF4+PAhoqOj4ejomOZN1omIiIjULSfGuOUW370Ar46ODhwdHdUZCxEREdE3aUANY9wgz8wtw4lbnTp1pBuIp+XYsWOZCoiIiIiI0pbhxO3TFYEBIDExEQEBAbh169Y3761JRERElFnsKs2Az2/YnsLHxwfR0dGZDoiIiIjoa9R5k3m5Udukii5dumD16tXqqo6IiIiIPvPdkxM+d+7cOZXVgYmIiIiygkKR8QV006pDjjKcuLVu3VrlsRACISEhuHz5MsaPH6+2wIiIiIjSwjFuGWBqaqryWENDA6VLl8akSZPQsGFDtQVGRERERKoylLglJyejR48eKF++PMzNzbMqJiIiIqIv4uSEdNLU1ETDhg0RERGRReEQERERfZ1CTf/JUYZnlZYrVw6PHj3KiliIiIiI6CsynLhNmTIFI0eOxL59+xASEoKoqCiVjYiIiCgrpXSVZnaTo3SPcZs0aRJGjBiBpk2bAgBatGihcusrIQQUCgWSk5PVHyURERHR/8vLY9zSnbhNnDgR/fv3x/Hjx7MyHiIiIqKvUigUX71venrrkKN0J25CCACAq6trlgVDRERERF+WoeVA5JqdEhER0Y+DXaXpVKpUqW8mb+Hh4ZkKiIiIiOhreOeEdJo4cWKqOycQERERUfbIUOLWsWNHWFlZZVUsRERERN+koVBk+ibzmT0/p6Q7ceP4NiIiIsoN8vIYt3QvwJsyq5SIiIiIcka6W9yUSmVWxkFERESUPmqYnCDTW5VmbIwbERERUU7TgAIamcy8Mnt+TsnwvUqJiIiIKGewxY2IiIhkheu4EREREclEXp5VysSNiIiIZCUvr+PGMW5EREREMsEWNyIiIpIVjnEjIiIikgkNqKGrlMuBEBEREVFWYuJGREREspLSVZrZLb2KFi0KhUKRahs0aBAAwM3NLdWx/v37Z8lzZ1cpERERyYoGMt/ylJHzL126hOTkZOnxrVu30KBBA7Rr107a16dPH0yaNEl6bGBgkMkI08bEjYiIiOgr8ufPr/J4+vTpKFGiBFxdXaV9BgYGsLa2zvJY2FVKREREspJWt+X3bN8jISEBGzduRM+ePVXq2LRpE/Lly4dy5cph9OjRiI2NVdfTVcEWNyIiIpIVxf9vma0DAKKiolT26+rqQldX94vn7d69GxEREejevbu0r3PnzihSpAhsbW1x48YNeHl5ITAwEDt37sxklKkxcSMiIqI8y87OTuWxt7c3fHx8vlh+1apVaNKkCWxtbaV9ffv2lf5dvnx52NjYoF69eggKCkKJEiXUGi8TNyIiIpIVdd7yKjg4GCYmJtL+r7W2PX36FEeOHPlmS5qLiwsA4OHDh0zciIiIiNS1fK6JiYlK4vY1a9asgZWVFZo1a/bVcgEBAQAAGxubzIaXChM3IiIikpWcuOWVUqnEmjVr0K1bN2hp/S99CgoKwubNm9G0aVNYWlrixo0b8PDwQO3atVGhQoXMBZkGJm5ERERE33DkyBE8e/YMPXv2VNmvo6ODI0eOYN68eYiJiYGdnR3atGmDcePGZUkcTNyIiIhIVjKznMendWREw4YNIYRItd/Ozg4nTpzIVCwZwcSNiIiIZCW775yQm8g1biIiIqI8hy1uREREJCs50VWaWzBxIyIiIllR550T5IZdpUREREQywRY3IsoR7y4tzOkQKBuZN/LN6RAoi4mkuGy7FrtKiYiIiGSCs0qJiIiIKNdjixsRERHJCrtKiYiIiGQiL88qZeJGREREspITN5nPLTjGjYiIiEgm2OJGREREsqIBBTQy2dmZ2fNzChM3IiIikhV2lRIRERFRrscWNyIiIpIVxf//l9k65IiJGxEREckKu0qJiIiIKNdjixsRERHJikINs0rZVUpERESUDfJyVykTNyIiIpKVvJy4cYwbERERkUywxY2IiIhkhcuBEBEREcmEhuLjltk65IhdpUREREQywRY3IiIikhV2lRIRERHJBGeVEhEREVGuxxY3IiIikhUFMt/VKdMGNyZuREREJC+cVUpEREREuR5b3IiIiEhWOKuUiIiISCby8qxSJm5EREQkKwpkfnKBTPM2jnEjIiIi+hofHx8oFAqVzcHBQToeFxeHQYMGwdLSEkZGRmjTpg1ev36dJbEwcSMiIiJZ0YACGopMbhlscytbtixCQkKk7fTp09IxDw8P7N27F9u3b8eJEyfw8uVLtG7dWt1PGwC7SomIiEhmcqKrVEtLC9bW1qn2R0ZGYtWqVdi8eTPq1q0LAFizZg3KlCmD8+fP4+eff85kpKrY4kZERET0DQ8ePICtrS2KFy8Od3d3PHv2DABw5coVJCYmon79+lJZBwcHFC5cGOfOnVN7HGxxIyIiInlRY5NbVFSUym5dXV3o6uqq7HNxccHatWtRunRphISEYOLEiahVqxZu3bqFV69eQUdHB2ZmZirnFChQAK9evcpkkKkxcSMiIiJZUec6bnZ2dir7vb294ePjo7KvSZMm0r8rVKgAFxcXFClSBH///Tf09fUzFUdGMXEjIiKiPCs4OBgmJibS489b29JiZmaGUqVK4eHDh2jQoAESEhIQERGh0ur2+vXrNMfEZRbHuBEREZG8KP63CO/3bikNdiYmJipbehK36OhoBAUFwcbGBpUrV4a2tjaOHj0qHQ8MDMSzZ89QrVo1tT91trgRERGRrGT3rNKRI0eiefPmKFKkCF6+fAlvb29oamqiU6dOMDU1Ra9eveDp6QkLCwuYmJhgyJAhqFatmtpnlAJM3IiIiIi+6vnz5+jUqRPCwsKQP39+1KxZE+fPn0f+/PkBAHPnzoWGhgbatGmD+Ph4NGrUCIsXL86SWJi4ERERkbxkc5Pb1q1bv3pcT08PixYtwqJFizIZ1LcxcSMiIiJZUeesUrlh4kZERESyIk0wyGQdcsRZpUREREQywRY3IiIikpWcuFdpbsHEjYiIiOQlD2du7ColIiIikgm2uBEREZGscFYpERERkUxwVikRERER5XpscSMiIiJZycNzE5i4ERERkczk4cyNXaVEREREMsEWNyIiIpIVziolIiIikom8PKuUiRsRERHJSh4e4sYxbkRERERywRY3ohy04u8TWLDxKELDolDOviD+HNUOlcsWzemwKAvwtZa/6uXsMKSdCyraW8PG0hjuPjtw4NwD6Xh+MwP49KqDOpWLwdRQD2dvBcNr0WE8evlOKqOrrYkpfeuhtZsjdLQ1cezKI4xccAhvImJz4inJVx5ucmOLG2WJokWLYt68eTkdRq628/AVjJu3C169m8B/gxfK2RdEmyGL8Cb8fU6HRmrG1/rHYKCnjVuPQjFq4eE0j2/0bouiNmZw9/kHroNW4/nrSOye3gkGutpSmWn966PxzyXRfcou/DJyE6wtjLFhQpvsego/DIWa/pMjJm5EOWTx5mPo2qo63FtUg0NxG8wZ3REGejrY+O+5nA6N1Iyv9Y/hyOVHmLruJPafvZ/qWImCFvjJsSBGLDiEa/dD8PB5ODwXHISerhba1HEEAJgY6KJLo4oYu+woTl1/iusPX2HwnH1wKVsIVRxss/vpkEwxccujEhIScjqEPC0hMQkB94Lh9lNpaZ+GhgZcfyqNSzcf52BkpG58rfMGXW1NAEBcQpK0TwggITEZP5ctBACoaG8NHW1N+F97IpV5EByO4NeRqFqmYLbGK3cps0ozu8kREzeZcHNzw9ChQ/H777/DwsIC1tbW8PHxkY4/e/YMLVu2hJGREUxMTNC+fXu8fv1aOu7j4wMnJyesXLkSxYoVg56eHgBAoVBg2bJl+OWXX2BgYIAyZcrg3LlzePjwIdzc3GBoaIjq1asjKChIqisoKAgtW7ZEgQIFYGRkhKpVq+LIkSPZ9rP4EYRFRCM5WYn8FsYq+/NbmCA0LCqHoqKswNc6b7gfHIbg15GY0NMNpkZ60NbSwLD2P6NgfhMUsDACABSwMER8QhKiYuJVzg2NiEEBC8OcCFu2FGra5IiJm4ysW7cOhoaGuHDhAmbMmIFJkybBz88PSqUSLVu2RHh4OE6cOAE/Pz88evQIHTp0UDn/4cOH+Oeff7Bz504EBARI+ydPnoyuXbsiICAADg4O6Ny5M/r164fRo0fj8uXLEEJg8ODBUvno6Gg0bdoUR48exbVr19C4cWM0b94cz549y9DziY+PR1RUlMpGRCRHSclK/DZpJ0oWtMCTfzzw8t9RqFmxCPwuBkEIkdPh0Q+Es0plpEKFCvD29gYA2NvbY+HChTh69CgA4ObNm3j8+DHs7OwAAOvXr0fZsmVx6dIlVK1aFcDH7tH169cjf/78KvX26NED7du3BwB4eXmhWrVqGD9+PBo1agQAGDZsGHr06CGVr1ixIipWrCg9njx5Mnbt2oV///1XJcH7Fl9fX0ycODGjP4YfgqWZETQ1NVINTn8THgUrS5McioqyAl/rvOP6w1eoPXA1TAx0oa2tgbDID/Cb3w0B90MAAK/DY6CrowUTQ12VVjcrM0O8Do/JqbDlibNKSQ4qVKig8tjGxgahoaG4e/cu7OzspKQNABwdHWFmZoa7d+9K+4oUKZIqafu83gIFCgAAypcvr7IvLi5OahGLjo7GyJEjUaZMGZiZmcHIyAh3797NcIvb6NGjERkZKW3BwcEZOl/OdLS14ORghxOXAqV9SqUSJy/dR9XyxXIwMlI3vtZ5T1RsPMIiP6C4rTmc7a2lJUOuP3iFhMRkuDoXlcqWLGQBuwKmuHT3RQ5FK095eVYpW9xkRFtbW+WxQqGAUqlM9/mGhmmPofi0XsX/j9ZMa1/KtUaOHAk/Pz/MmjULJUuWhL6+Ptq2bZvhCQ+6urrQ1dXN0Dk/koGd62LgxA1wLlMYlcoWxZItxxHzIR7uzX/O6dBIzfha/xgM9bRRzNZcelzE2gzlilsh4n0cnr+JQstaDngbGYvnoVFwLJYf0/vXx/5z93H86sdJKFGx8dh46Dqm9q2Hd+8/4H1MAmYMaoCLd57j8r2XOfW0SGaYuP0AypQpg+DgYAQHB0utbnfu3EFERAQcHR3Vfr0zZ86ge/fu+PXXXwF8bIF78uSJ2q/zo2vdsDLeRkRj2rL9CA17j/KlCmLHX4PYffYD4mv9Y3AqZYN9M92lx9P61wcAbD58A4Nm70cBCyNM7VcP+c0M8To8GluP3MLMzadV6hiz9AiUSoH141t/XID38mOMXHgoW5/Hj4D3KiVZq1+/PsqXLw93d3fMmzcPSUlJGDhwIFxdXVGlShW1X8/e3h47d+5E8+bNoVAoMH78+Ay1/NH/9G3vir7tXXM6DMoGfK3l78yNZzBv5PvF48v3XMbyPZe/Wkd8YjJGLTqMUYvSXsSX0icPD3HjGLcfgUKhwJ49e2Bubo7atWujfv36KF68OLZt25Yl15szZw7Mzc1RvXp1NG/eHI0aNUKlSpWy5FpERESp5OH1QBSC85Qpl4iKioKpqSleh0XCxIRdSEQ/kq+1VNGPQSTFIf7kJERGZt13eMrviSsPQmBknLlrRL+PQmV7myyNNyuwq5SIiIhkRR2zQjmrlIiIiCg7qOOWVfLM2zjGjYiIiEgu2OJGREREspKXZ5UycSMiIiJ5ycOZG7tKiYiIiL7C19cXVatWhbGxMaysrNCqVSsEBgaqlHFzc4NCoVDZ+vfvr/ZYmLgRERGRrGT3vUpPnDiBQYMG4fz58/Dz80NiYiIaNmyImJgYlXJ9+vRBSEiItM2YMUPdT51dpURERCQv2X3Lq4MHD6o8Xrt2LaysrHDlyhXUrl1b2m9gYABra+vMBfYNbHEjIiIiyoDIyEgAgIWFhcr+TZs2IV++fChXrhxGjx6N2NhYtV+bLW5EREQkK+qcmxAVFaWyX1dXF7q6ul88T6lUYvjw4ahRowbKlSsn7e/cuTOKFCkCW1tb3LhxA15eXggMDMTOnTszGakqJm5EREQkL2rM3Ozs7FR2e3t7w8fH54unDRo0CLdu3cLp06dV9vft21f6d/ny5WFjY4N69eohKCgIJUqUyGSw/8PEjYiIiGRFnbe8Cg4OVrlX6dda2wYPHox9+/bh5MmTKFSo0Ffrd3FxAQA8fPiQiRsRERGROpiYmHzzJvNCCAwZMgS7du2Cv78/ihUr9s16AwICAAA2NjbqCFPCxI2IiIhkRQE1zCrNQNlBgwZh8+bN2LNnD4yNjfHq1SsAgKmpKfT19REUFITNmzejadOmsLS0xI0bN+Dh4YHatWujQoUKmQv0M0zciIiISFay+8YJS5YsAfBxkd1PrVmzBt27d4eOjg6OHDmCefPmISYmBnZ2dmjTpg3GjRuXyShTY+JGRERE9BVCiK8et7Ozw4kTJ7IlFiZuREREJCvZvQBvbsLEjYiIiGQm795lnndOICIiIpIJtrgRERGRrLCrlIiIiEgm8m5HKbtKiYiIiGSDLW5EREQkK+wqJSIiIpIJdd6rVG6YuBEREZG85OFBbhzjRkRERCQTbHEjIiIiWcnDDW5M3IiIiEhe8vLkBHaVEhEREckEW9yIiIhIVjirlIiIiEgu8vAgN3aVEhEREckEW9yIiIhIVvJwgxsTNyIiIpIXziolIiIiolyPLW5EREQkM5mfVSrXzlImbkRERCQr7ColIiIiolyPiRsRERGRTLCrlIiIiGQlL3eVMnEjIiIiWcnLt7xiVykRERGRTLDFjYiIiGSFXaVEREREMpGXb3nFrlIiIiIimWCLGxEREclLHm5yY+JGREREssJZpURERESU67HFjYiIiGSFs0qJiIiIZCIPD3FjVykRERHJjEJNWwYtWrQIRYsWhZ6eHlxcXHDx4sVMP5WMYuJGRERE9A3btm2Dp6cnvL29cfXqVVSsWBGNGjVCaGhotsbBxI2IiIhkRaGm/zJizpw56NOnD3r06AFHR0csXboUBgYGWL16dRY9y7QxcSMiIiJZSZmckNktvRISEnDlyhXUr19f2qehoYH69evj3LlzWfAMv4yTEyjXEEIAAN5HReVwJESkbiIpLqdDoCwmkuI//v//v8uzUpQafk+k1PF5Xbq6utDV1VXZ9/btWyQnJ6NAgQIq+wsUKIB79+5lOpaMYOJGucb79+8BACWL2eVwJERE9L3ev38PU1PTLKlbR0cH1tbWsFfT7wkjIyPY2anW5e3tDR8fH7XUnxWYuFGuYWtri+DgYBgbG0Mh1wV2MigqKgp2dnYIDg6GiYlJTodDWYivdd6SF19vIQTev38PW1vbLLuGnp4eHj9+jISEBLXUJ4RI9fvm89Y2AMiXLx80NTXx+vVrlf2vX7+GtbW1WmJJLyZulGtoaGigUKFCOR1GjjAxMckzX+55HV/rvCWvvd5Z1dL2KT09Pejp6WX5dT6lo6ODypUr4+jRo2jVqhUAQKlU4ujRoxg8eHC2xsLEjYiIiOgbPD090a1bN1SpUgU//fQT5s2bh5iYGPTo0SNb42DiRkRERPQNHTp0wJs3bzBhwgS8evUKTk5OOHjwYKoJC1mNiRtRDtLV1YW3t3eaYyrox8LXOm/h6/1jGjx4cLZ3jX5OIbJj3i4RERERZRoX4CUiIiKSCSZuRERERDLBxI2IiIhIJpi4EREREckEEzciIiIimWDiRvSD4ARxIvk5cOAArl+/ntNhkIwwcSOSEaVSmWpfWFgYAOSZ+7sS/QiEEHj48CHatWuHefPm4c6dOzkdEskEEzciGdHQ0MDjx4/h6+sLANixYwd69uyJ0NDQHI6M5CqtPwbS2kfqpVAoULJkSWzZsgUnTpzAnDlzcPv27ZwOi2SAd04gkpGkpCRs374dixcvxo0bN7Bt2zasWbMGVlZWOR0ayZBSqYSGxse/3588eYKkpCSULFlS2kdZRwgBhUKBFi1aQENDAwMHDgQAeHh4oGzZsjkcHeVmTNyIZERLSwuDBg3CtWvXsG3bNrRq1QrdunUDACQnJ0NTUzOHIyQ5SUnQxowZgy1btuDDhw+oWrUqlixZgkKFCuVwdD82hUIhJW+//PILhBAYNGgQACZv9HX8s4pIRoQQ0NbWRr58+dCyZUs8ePAAEyZMAABoamoiKSkphyMkOfi0K3Tr1q3YvHkzpk+fjr/++gtBQUFo0aIFx1xloZSJRJ+OS23evDkWLFiAw4cPY+7cuew2pS/ivUqJZCDlL/NPhYaGYsmSJdi6dSvatWuHSZMmSceCg4NhZ2eX3WGSzOzevRshISHQ1NRE3759AQARERGoVasWtLS0sHnzZpQpUyaHo/yxpHyWL168iLt37+Ldu3do1aoVChUqBC0tLezZswdDhgxBw4YN4enpCUdHx5wOmXIZdpUS5XIpX/T+/v44efIkAKBPnz6wsbFB7969AQB///03hBCYPHkyvL29cevWLaxfvx6GhoY5GTrlYm/fvkWXLl0QGxsLHx8fAB/fa2ZmZjh16hRq166N3377DatXr0aFChVyNtgfRMpneefOnejduzeqVKmCO3fuYM+ePejcuTO6deuGli1bAgA8PT0RHR0NHx8fODg45HDklKsIIsr1du/eLQwMDET16tVF8eLFhYWFhTh//rwQQogXL16IadOmiQIFCogyZcoIc3NzcfHixRyOmHIbpVKZat+dO3eEo6OjqFatmggJCVEpFxERIaysrET37t2zNc4f3YkTJ0SBAgXEypUrhRBCBAYGCi0tLVG5cmWxYMECER8fL4QQYtu2baJcuXLi5cuXORku5ULsKiXK5T58+IBJkyahVKlS6NGjB16+fAlPT08cOnQI+/btQ40aNfDu3TsEBgbi8uXLaNKkCUqUKJHTYVMu8uns0ZRxkFpaHztcbt++jYYNG6J8+fLYuHEj8uXLJ7UMxcTEQE9Pj5Ne1CQ5ORnz5s1DcHAw5s2bh0ePHqFBgwaoUaMGIiMjERAQgNGjR6NHjx7Q1dVFdHQ0jIyMcjpsymWYuBHlYhcvXkSLFi3g4OCAKVOmoGbNmgCAqKgo9OvXD//99x8OHDiA6tWr53CklFt9mrTNnj0bly5dwv3799GpUye4urrip59+wu3bt9GgQQNUrFgRGzZsUEneAM5YVqf79+8jOTkZhQsXRuPGjVGqVCmsWrUKISEhKFu2LAoUKIBhw4ahf//+aY5tJeKsUqJczNLSEhUrVsTJkyellhKlUgkTExMsX74czZs3R82aNXHx4sUcjpRym5S/yVOSttGjR2PatGkoU6YMypcvjx07dmDEiBE4evQoypYtCz8/P9y5cwdNmjRBZGSkSsLApO37pNUuUqxYMZQpUwY3b97Eu3fvMGzYMADA69evUbVqVfz8889o2rQpAN4NhdLGyQlEuViJEiWwfPlydO/eHb/99hvOnDmDwoULQwgBY2NjLFq0CLq6ujA1Nc3pUCmX+fSX/q1bt7B7927s2LEDderUAQD4+/tj+fLlmD59OooUKYKyZcti7969mDBhAoyNjXMq7B9GSmuZn58f9uzZA0NDQ7Rr1w5VqlQBAMTExODDhw94+PAhypQpg927d8PGxgYLFixg9yh9FbtKiXKJlC/6q1ev4v79+4iJiUHlypXh5OSEkJAQdOzYEY8fP8aZM2dgZ2cndYGxO4U+1alTJzRo0AA9e/aU9t24cQOurq7Ys2cPateuLe0/dOgQ+vXrh3Xr1sHV1VWlnk+7WOn7HD58GK1bt0bNmjURFhaG27dvY9u2bWjevDnevHmD9u3b4/nz59DS0kJoaCiOHDkCZ2fnnA6bcjm2uBHlEgqFAv/88w/69esHFxcXPH/+HDo6OmjZsiXGjRuHtWvXomfPnnBzc8PRo0dRtGhR6Twi4OPafjVq1MBvv/2msl9LSwsFChTA06dPAfzvj4RGjRpBT08PJ0+eTJW4MWnLvMDAQMyYMQMDBw7Ey5cvMXPmTPz666/YunUr2rZti82bN+O///5DXFwcGjZsiJIlS+Z0yCQD/GQS5aBPV7C/fv06Bg8ejClTpmD//v1Yvnw5bt68ifj4eAAfx8asX78eRkZGaN68Oe+SQKlYWVlh8ODB0NbWxuLFizF+/HgAgKOjI1xcXDBixAicOXNGSvbfvXsHfX19LtasJikdWIGBgQgICMC5c+ekYQy2trbw8fHB0KFD0bFjR/zzzz+wsbFBz549MXDgQCZtlG5scSPKAUeOHEG1atVgaGgozdgLDAxEyZIl0b9/fzx+/BgdO3ZE9+7dMXnyZAAffxmULl0a+/btg1KplJZzIAJSd23ev38f+/btg76+PsaMGYN169ahefPmaNmyJX777TdYWVnh+PHjSE5ORpcuXXIw8h+HQqHArl278Ntvv6F48eK4ffs2SpYsKb02pqam8Pb2hqamJtq1a4d///0Xv/zyS06HTTLDb36ibHb69GkMHjwYDRs2xPTp02FgYADg45e+ra0tXrx4gdq1a6Np06ZYvHgxAODEiRM4duwYhgwZwtYRSuXWrVsoUqQIjI2NMXbsWDRt2hReXl4wMTHB+vXroVQqMW7cOOzduxdjxozBzZs3cfHiRdjb22P//v3Q0tLikh+ZkNL1HBwcjKlTp2LOnDkoXbo0Dh48iGnTpqF48eLo3r07AMDU1BRjx46Fjo4O11uk78LJCUTZ7MOHD5g+fTr8/PxQtWpV+Pr6wsDAAGfOnIGrqyt0dHTQt29fzJs3Tzpn0KBBePnyJdatWwcTE5OcC55yFaVSiUePHqFUqVLw9fXFs2fPsG7dOly4cAFly5bF8+fPsXTpUuzYsQNdunTBuHHjAHx8DyoUCujp6QH4uCgvW3Az5/Dhwzhz5gyCg4OxbNkyaGtrAwC8vb0xdepUrFixAj169JDKc1IRfS9+UomyUVJSEvT19TFx4kRoaWnB398f48ePx+TJk1GjRg3MmTMHnp6eKFOmDF6+fInExEQsXrwYW7duxcmTJ5m0kQoNDQ2ULFkSGzZsQM+ePaGlpYVDhw6hbNmyEEKgUKFC6N+/PwBg8+bN0NTUxOjRo6Gvry/VIYRg0vadUpKv9+/fIzQ0FJMnT0ahQoXw8uVLFClSBAAwceJEKBQKDBo0CHFxcRgwYAAATiqi78dPK1E2SumKunTpEmJjY/H8+XNcunQJGhoamDhxIoYOHYo3b95gyJAhmDZtGszNzREXF4cjR46gbNmyORw95SafjmmzsrJCcnIyEhMTcebMGTg6OsLCwgIAUKhQIfTr1w8aGhr4888/UahQIZVZp0wgvp9CocDmzZvRrVs3JCQkIDY2Fv3798fGjRsxePBgaWKCj48PYmNjMWHCBHTu3JnrLlKmsKuUKAul1R2yf/9+tGzZEpMmTYKFhQX279+PR48eoVGjRpg6dSr09fVx+fJlvH79GsbGxihVqhSsra1z6BlQbvRp0nbr1i2UK1cOALBu3Tr06NED3t7eGDp0KMzNzaVzIiIisG3bNvTu3Ztj2TIp5XP99u1b/PHHHyhbtiw8PDwAALNmzcLvv/+OGTNmoG/fviqt5G/fvkW+fPlyKmz6UWTPveyJ8qanT59K/1YqlSI2Nla0aNFCDB06VNqfkJAgxo4dK0qVKiVGjRolYmJiciJUkgmlUin9e9y4ccLZ2VmsXLlS2r98+XKhUCjE5MmTxdu3b4UQQnTs2FGcPXtWOi8pKSl7g/4BXbp0SdSqVUvUqlVLBAYGioSEBOnYzJkzhUKhEHPmzBERERE5GCX9iNhVSpRFVq1ahbVr1+Lw4cPQ1dWFhoYG9PX1kZycjIiICKmctrY2Jk2ahEuXLmH16tWIjIzEvHnzVMYhEaVIacH19vbGkiVLsGPHDjg4OEj7+/TpAwAYOHAgrl+/jmfPniEsLEy61RLAe4+qw927dxEbG4sHDx7AwMAA2traiI+Ph66uLkaOHAkNDQ2MGDEC2traGDRoELukSW24AC9RFilVqhTWr18PfX19REdHAwASExNRtGhRPH78GC9fvlS5EXidOnVgYWGBN2/eIDIyMidDp1zu2bNnOHjwIJYuXQo3NzepKz1lUeY+ffpgw4YNsLCwQNWqVXH37l1oa2tz0WY16tSpE37//XdYWVmhU6dOCAsLg66uLhISEgAAnp6emD9/PurWrcukjdSKY9yIstjVq1fRs2dPLFq0CDVq1EBwcDCcnZ1Rp04dzJkzR1qXzdPTExYWFhgwYAAsLS1zOGrKze7du4effvoJ27dvR6NGjVSOxcbGQk9PDxoaGoiLi+OSH2ogPlmnTQiBDx8+oHTp0hBCYMeOHZg9ezby5cuHDRs2wNzcXGp5I8oKbHEjymLR0dGwsrKCp6endIP4I0eOwN/fH+3bt0fLli3RoUMHLFmyBB06dGDSRt+kqakJa2trvHr1Smq1Tfm/v78/Zs6cCSGElLQBYNL2nVKStp07d6J+/fqoU6cOXFxcMHDgQAQHB6Ndu3bw8PBAeHg4unfvLrW8EWUVJm5EapbyC/TevXt4+vQpateujQkTJqBgwYIYMmQIzp07BycnJwQEBKBOnTowMjKCnp4eLl26BHt7+xyOnnKTT+9l+yl7e3s4OTlhzJgxuHTpEoCPY98+fPiApUuX4v79+9kZ5g9NoVDgxIkT6NKlCzw8PLBq1SqsWbMG27dvx/Dhw/HixQu0a9cOQ4YMwcOHDzFw4MAvvm5E6sCuUiI1SvnrfNeuXfDw8MCwYcPQpUsX5M+fH/7+/pg/fz6ePn2KBQsWoEaNGlL3VWJiorTSOhGguuTHli1bcP36dZibm8PJyUnqHm3QoAGuX7+Odu3awdDQEBcuXEBYWBgCAgKgpaXF1fnVZOzYsQgICMD+/fulfQEBAahXrx66du2KuXPnIikpCbt370aVKlVQtGjRnAuWfnhM3IjU7MiRI2jVqhVmzZqFVq1aqazB5u/vj7/++gsvXrzA7NmzUbNmTQC8/Q19mZeXFzZs2IDq1asjKioK4eHh6NOnD/r16wfgY1Jx//59REVFoXTp0pgzZw60tLQ4pk1NhBDo1asXXrx4gUOHDkGpVCIpKQk6OjrYuHEjRowYgYsXL0p3SiDKakzciNQk5aPUo0cPaGtrY8WKFdKxT3+JnjlzBuPHj4dSqcTBgwdVxiERfWrp0qWYMWMGtmzZAhcXF6xYsQKDBw+GjY0NhgwZghEjRgD4OFtZoVBI7zEmbd8v5Y+o8PBw6OnpwcDAALt27UKnTp2wb98+1K9fX2oN3b17N8aMGYPTp09Ld6ogymoc40akJgqFAkII3Lt3T2plS05OBvC/geEvXrxAjRo1MGXKFGzcuJFJG31RYmIi7t+/j759+8LFxQX//vsvfv/9d4wbNw5169bFzJkzsWzZMgAf1wJMeY8J3ns0UxQKBXbv3o0WLVrAyckJ3t7e0NfXR//+/TFkyBD4+flJXdgXLlyAgYEBW8spW7HFjUjN2rZti5cvX+LkyZPQ0tJCcnIyNDU18fTpU+m+hra2tjkdJsnAu3fv8O7dOwBAkyZN0L9/f3h4eODgwYNo06YNAGDFihXo3LlzTob5Q7l69Srq1q2LESNGICwsDKdPn4a9vT1++uknBAcHY+HChahUqRK0tbVx69YtHDt2DM7OzjkdNuUhbHEj+k4pf/OEh4fj7du30v6uXbsiOjoanp6eUtIGAMuWLcOGDRu4aj2lixAC5ubmKF68OC5fvgxDQ0N069YNAKCrq4smTZpg0aJF6NChQw5H+uMICgrCgQMHMGrUKIwfPx7z5s2Dt7c33r59i3PnzsHNzQ1+fn5wc3ND8+bNcfHiRSZtlO3Ynk70nVJmj86YMQMhISFo27YtevbsiWbNmiEwMBBbtmxB1apV4eLigpcvX+LEiRPw9/dHgQIFcjp0koFPu9/09PQQEhICPz8/NGnSBLNnz0aJEiXQrVs3KBQKlT8Q6PtERUWhY8eOePbsGXr27Cntb968OQBg7ty5WLduHcaPH4/p06fnVJhE7ColyohPZ39evnwZTZs2Rf/+/aGnp4fly5ejYsWKGD9+PCpXroxTp05h/fr1CA0NReHChTFo0CCUKVMmh58ByVFQUBB8fHxw4MABmJiYwMTEBJcvX4a2tjZnJKvRtWvX0LFjR+TPnx/Lli1D2bJlpWMHDhzA2LFjUbZsWSxfvhz6+vr8uVOOYOJGlA7btm1DxYoV4eDgAODjL9Jdu3YhLi4O48aNA/Axkevfvz9sbW3xxx9/oHr16jkZMv1gHj16hGfPnuHVq1do164dNDU1OXs0C9y4cQPdunXDTz/9hKFDh6okb4cPH0bp0qW59AflKCZuRN/w/PlzdOrUCZs3b4adnR3evXuH8uXLIzw8HL1798Zff/0llb148SIGDBiAYsWKoVevXmjSpEkORk65lTpayZi0ZZ1r166hd+/eqFSpEjw8PODo6JjTIRFJODmB6BsKFSqEw4cPw87ODjdv3gQA7NixA/nz58e1a9cQEBAglf3pp5+wbNkyXL16FZs2bcKHDx9yKGrKrZRKpZS0ffjwAbGxsSrHv/S39Of7mbRlHWdnZ6xcuRI3btzA5MmTce/evZwOiUjCFjeidIqKikLNmjVRrlw5LFy4EPfv30f79u1Rr149eHp6onz58lLZq1evwtzcHMWKFcvBiCm3+fQ2VtOnT8fFixdx48YNtGvXDg0aNEDdunXTPO/TFrpdu3YBAH799dfsCToPu3TpEkaNGoUtW7bAxsYmp8MhAsDEjShDLl++jAEDBqBChQqYNWsW7ty5g06dOqFevXoYMWIEypUrl9MhkgyMGTMGy5cvx/z58xEfH48lS5YgISEBhw8fTjXr+NOkbcmSJfjjjz+we/du1KlTJydCz3Pi4uK4UDblKuwqJcqAKlWqYPny5bh69SpGjhwJR0dHbNmyBSdPnoSPjw/u3LmT0yFSLnfnzh0cOHAAu3btgru7O4oWLYrbt29j+PDhKFCgAJRKpVT2027VZcuWYcyYMVi1ahWTtmzEpI1yGyZuRBnk7OyM1atXS8lb2bJlsWrVKgQGBsLMzCynw6Nc5tNEDPg4qSA6OhouLi7YuXMnWrZsiTlz5qBHjx748OED/v77b7x69QoApG7V5cuX4/fff8fKlSvRtm3bbH8ORJR7MHEj+g4pyduNGzfQr18/ODs74+LFi7yVFaWSknzdu3cPycnJUCqVMDIywooVK9CrVy/8+eef6N+/P4CPsxn37NkjJW7AxxvNe3p6Ys2aNdJtrogo72LiRvSdnJ2dsXjxYrx69QqxsbHQ19fP6ZAol9q2bRuaN28OTU1NODk5oWTJkhgyZAh+//13DBw4EMDHGaZTp05FTEwMKlSoAAB48OABNm/ejHXr1qF169Y5+RSIKJfg5ASiTOLgZfqWd+/eoXTp0vDw8MDo0aPx5s0buLu74+bNmxg2bBgSEhJw8uRJvHr1CteuXYO2trZ0bnBwMOzs7HIweiLKTdjiRpRJTNroU5+PaUtISICpqSn69u2LCxcuICwsDPny5cPff/+N1q1b48CBAzhz5gwcHR0REBAAbW1tJCUlSfUwaSOiT7HFjYhIDd69ewdzc3Pp8aNHj1C8eHHp8dmzZ9GgQQOsW7dOZYJBTEwMDAwMpNmjvCMCEX0NW9yIiDLp119/lRbGBYDNmzejWbNm8PDwwMuXLxEfH4/q1aujf//+mDlzJp4/fy6V/TRpE0IwaSOir2LiRkSUSQ0bNkSXLl0AfEy+Uu5xuWvXLvz6668YOHAgnj9/jiZNmkBHR0e6hdKn67QByPT9S4nox8euUiKi7/T5zeLnzZuHsLAweHp6wtzcHHFxcVi6dCn279+PGzduYMCAAZg9ezaqVq2KY8eO5WDkRCRXbHEjIvpOCoVC5ebvYWFhWLFiBVasWIEXL15AT08Pw4YNg5+fH8aNG4egoCDExMTgzZs3X7yZPBHR13AwBRHRd/D394ebmxsUCgUmT56MwoULY/LkydDS0sLChQuhVCrRvXt3WFtbAwCGDBmCd+/eYdiwYXBycoJCoVC56TwRUXowcSMiyqAXL16gZ8+eKFKkCJycnLBkyRJcuHABAODt7Q2lUonFixcDAHr27AkrKysAgLm5OapUqQKAs0eJ6PtwjBsRUQYlJSXh/PnzaNasGZKSknDu3DlUqFABHz58kO6g4e3tjbVr12LQoEH47bffYGNjk8NRE9GPgG30REQZpKWlBW1tbRgZGSF//vwYNWoUhBDQ19dHXFwcAGDixIno2bMnxo4di6NHj+ZwxET0o2CLGxFROnw+Hk2pVOL169e4d+8eBgwYgEKFCuHIkSOpym7atAkdO3aEpqZmjsRNRD8WJm5ERN/waSLm5+eHDx8+oFixYihfvjwSExPh5+cHT09PFC5cGIcPHwYA9OnTB/Xr10eHDh0AAMnJyUzeiCjTmLgREaWTl5cXlixZAisrKzx9+hRz5szB4MGDoVQqcfjwYXh4eCAhIQFFihTBo0ePEBQUxAkIRKRW/EYhIvqCTxfYvX79Og4fPowjR46gQIEC2L17N4YNG4b379/jjz/+QOPGjVG4cGGsWbMGmpqa8PPzg5aWFlvaiEit2OJGRPQNM2bMwOvXr5GUlIT58+dL+5csWYJBgwZh6tSpGDFiBHR0dFTOY9JGROrGFjciom8ICQnB/Pnz4ebmhri4OOjp6QEABgwYAAAYOnQooqOjMW7cOGk5EABM2ohI7bgcCBHRJ5RKZap9c+fOxcSJE3HixAls3bpV5diAAQPg6+uLEydOSAkdEVFWYVcpEdH/+3T26I0bNxAVFQUzMzOULVsWCoUCo0aNwvz587F69Wp06dJF5dyU8XCf33ieiEid2FVKRISPiVdK0jZ69GgcOHAAoaGhcHR0hIGBAfbs2YOZM2dCW1sbvXr1goaGBjp37iydz6SNiLIDu0qJiAAp4Zo9ezZWrlyJxYsX4+nTp3B2dsb+/ftx8uRJAMC0adMwYsQIdOnSRVqz7fM6iIiyClvciIj+X2xsLC5evIg///wTNWrUwP79+7F8+XIsX74cbm5uiI2NhYGBAaZNm4bChQujbt26OR0yEeUxHONGRHnW57exAoDatWvDw8MDOjo66NixI2bOnIn+/fsjKSkJK1asgI2NDVq1aiWVT0pK4iK7RJRt2FVKRHlWStK2c+dOXL9+HUlJSShcuDDmz5+P3377DTNmzED//v0BAK9evcLevXvx9u1blTqYtBFRdmLiRkR5lhACT58+Ra9evXDlyhVoaWlh5MiRuH79Ouzt7dG2bVskJSXh7du36Nu3L6KiotCjR4+cDpuI8jB2lRJRnjd27Fhs27YNx44dQ+HChXHkyBG0atUKjo6O+PDhA8zMzBATE4MLFy5AW1ubd0QgohzDxI2I8ozPl+tITEyEtrY2rl+/jgEDBmDAgAH47bffAABBQUE4fPgwwsLCULJkSbRr1w6ampoc00ZEOYqJGxHlObt374aTkxOKFi0q7evYsSOCgoJw6dKlL57HljYiymkc40ZEecqVK1cwZcoUlC1bFrNmzYK/vz8AYNasWYiJicGCBQu+eC6TNiLKaWxxI6IfWlpLfsTHx2PhwoXYs2cPnj9/jkaNGqFLly5YsWIF9PX1sXjxYi6mS0S5EhM3IvphfZq0+fn5ISwsDElJSdJ9Rh8/fow7d+7A09MT9vb2uHbtGkJCQnDmzBlUq1YtJ0MnIkoTEzci+uF5eXlh165dMDExgVKpRHh4OA4fPoxSpUoBACIiInDgwAHs2bMHAQEBuH37NicgEFGuxMSNiH5oy5Ytw/jx43Hw4EFUqlQJGzZsQLdu3XDgwAE0btw41UzTlMecPUpEuREnJxDRD0WpVKo8DgwMhIeHBypVqoR//vkHgwcPxtKlS9G4cWNER0dLSVtycjKAjzeKF0IwaSOiXImJGxH9MIQQ0pi2I0eOIDk5GU+ePEFkZCSOHDmCHj16YPr06ejbty+EEFi8eDHmzp0LQHXGKCcmEFFuxcSNiH4In3Z5TpgwAcOHD8ezZ8/QrFkznDhxAs2bN8eMGTMwYMAAAEBkZCROnjyJ9+/f52TYREQZwsSNiH4IKUnbzZs3ce3aNSxevBjFihVDvXr1oKenB3t7exQsWBAJCQl48OAB3N3d8fr1a4wZMyaHIyciSj9OTiCiH8bixYuxbds2JCcnY+fOnbCysgIA3LlzB/369cPbt28RGhqKEiVKQFtbG/7+/rz3KBHJCkffEpFsfb64roODA548eYLQ0FBcvnwZTZs2BQA4Ojpix44dePHiBW7evAl7e3u4uLjw3qNEJDtscSMiWfo0aXv48CF0dXVhZ2eHR48eoUGDBnB0dIS3tzeqVKnyxTrY0kZEcsMxbkQkO5/OHv3jjz/QvHlzODs7o3bt2rhx4waOHDmCO3fuYMaMGbhy5YrKeZ9i0kZEcsPEjYhkRalUShMRtm7dinXr1mH69OmYPXs2XFxc0KZNG5w6dQp+fn64evUqZs+ejfPnzwPgMh9EJH8c2EFEspLS0ubv74+jR4/i999/R8uWLQEA79+/h52dHfr164ejR49i+/btqFmzJuzt7fHzzz/nZNhERGrBMW5EJDuvXr1CzZo1ERoaCi8vL4wdO1Y69u7dO3Tv3h12dnZYuHAhAgICUL58eXaLEtEPgV2lRCQ71tbW0nIfO3fuxLVr16Rj5ubmyJ8/Px4+fAgAcHJygqampnRLKyIiOWPiRkSyVKFCBezcuRPJycmYN28eAgICAHzsLr179y4KFy6sUp4tbkT0I2BXKRHJ2rVr19ClSxeEh4ejSpUq0NHRwePHj3H+/Hno6Oio3AqLiEju2OJGRLLm7OyMbdu2QV9fH5GRkWjQoAGuXr0KHR0dJCYmMmkjoh8KEzcikr1y5cph586dSEhIwNWrV6Xxbdra2jkcGRGRerGrlIh+GNeuXUP//v1RvHhxeHt7w8HBIadDIiJSK7a4EdEPw9nZGQsXLkRISAhMTU1zOhwiIrVjixsR/XDi4uKgp6eX02EQEakdEzciIiIimWBXKREREZFMMHEjIiIikgkmbkREREQywcSNiOgT3bt3R6tWraTHbm5uGD58eLbH4e/vD4VCgYiIiC+WUSgU2L17d7rr9PHxgZOTU6bievLkCRQKhXSLMSLKXkzciCjX6969OxQKBRQKBXR0dFCyZElMmjQJSUlJWX7tnTt3YvLkyekqm55ki4goM7RyOgAiovRo3Lgx1qxZg/j4eBw4cACDBg2CtrY2Ro8enapsQkICdHR01HJdCwsLtdRDRKQObHEjIlnQ1dWFtbU1ihQpggEDBqB+/fr4999/Afyve3Pq1KmwtbVF6dKlAQDBwcFo3749zMzMYGFhgZYtW+LJkydSncnJyfD09ISZmRksLS3x+++/4/MVkj7vKo2Pj4eXlxfs7Oygq6uLkiVLYtWqVXjy5Anq1KkDADA3N4dCoUD37t0BAEqlEr6+vihWrBj09fVRsWJF7NixQ+U6Bw4cQKlSpaCvr486deqoxJleXl5eKFWqFAwMDFC8eHGMHz8eiYmJqcotW7YMdnZ2MDAwQPv27REZGalyfOXKlShTpgz09PTg4OCAxYsXZzgWIsoaTNyISJb09fWRkJAgPT569CgCAwPh5+eHffv2ITExEY0aNYKxsTFOnTqFM2fOwMjICI0bN5bOmz17NtauXYvVq1fj9OnTCA8Px65du7563a5du2LLli3466+/cPfuXSxbtgxGRkaws7PDP//8AwAIDAxESEgI5s+fDwDw9fXF+vXrsXTpUty+fRseHh7o0qULTpw4AeBjgtm6dWs0b94cAQEB6N27N/74448M/0yMjY2xdu1a3LlzB/Pnz8eKFSswd+5clTIPHz7E33//jb179+LgwYO4du0aBg4cKB3ftGkTJkyYgKlTp+Lu3buYNm0axo8fj3Xr1mU4HiLKAoKIKJfr1q2baNmypRBCCKVSKfz8/ISurq4YOXKkdLxAgQIiPj5eOmfDhg2idOnSQqlUSvvi4+OFvr6+OHTokBBCCBsbGzFjxgzpeGJioihUqJB0LSGEcHV1FcOGDRNCCBEYGCgACD8/vzTjPH78uAAg3r17J+2Li4sTBgYG4uzZsyple/XqJTp16iSEEGL06NHC0dFR5biXl1equj4HQOzateuLx2fOnCkqV64sPfb29haampri+fPn0r7//vtPaGhoiJCQECGEECVKlBCbN29WqWfy5MmiWrVqQgghHj9+LACIa9euffG6RJR1OMaNiGRh3759MDIyQmJiIpRKJTp37gwfHx/pePny5VXGtV2/fh0PHz6EsbGxSj1xcXEICgpCZGQkQkJC4OLiIh3T0tJClSpVUnWXpggICICmpiZcXV3THffDhw8RGxuLBg0aqOxPSEiAs7MzAODu3bsqcQBAtWrV0n2NFNu2bcNff/2FoKAgREdHIykpCSYmJiplChcujIIFC6pcR6lUIjAwEMbGxggKCkKvXr3Qp08fqUxSUhLv/UqUSzBxIyJZqFOnDpYsWQIdHR3Y2tpCS0v168vQ0FDlcXR0NCpXroxNmzalqit//vzfFYO+vn6Gz4mOjgYA7N+/XyVhAj6O21OXc+fOwd3dHRMnTkSjRo1gamqKrVu3Yvbs2RmOdcWKFakSSU1NTbXFSkTfj4kbEcmCoaEhSpYsme7ylSpVwrZt22BlZZWq1SmFjY0NLly4gNq1awP42LJ05coVVKpUKc3y5cuXh1KpxIkTJ1C/fv1Ux1Na/JKTk6V9jo6O0NXVxbNnz77YUlemTBlpokWK8+fPf/tJfuLs2bMoUqQIxo4dK+17+vRpqnLPnj3Dy5cvYWtrK11HQ0MDpUuXRoECBWBra4tHjx7B3d09Q9cnouzByQlE9ENyd3dHvnz50LJlS5w6dQqPHz+Gv78/hg4diufPnwMAhg0bhunTp2P37t24d+8eBg4c+NU12IoWLYpu3bqhZ8+e2L17t1Tn33//DQAoUqQIFAoF9u3bhzdv3iA6OhrGxsYYOfL/2rl/19OjOI7jr7t9U2ZKKUUx+LGabJJB+SSbDChJSq6yfAYpNgMDg8Iii7L4ByijkglZ/A/Kdu9wS93ujzLc7v3cno/5dDpne3Z6fz6fVa/XNZ/PdbvddDgcNBwOXwP/5XJZ1+tVzWZT5/NZi8VCs9nsrfv6fD7d73ctl0vdbjcNBoOffmjx8fGhfD6v4/Go3W6nWq2mbDYrp9MpSWq32+r1ehoMBrpcLjqdTppOp+r3+2+dB8CfQbgB+C/ZbDZtt1u53W4ZhqFAIKBCoaDn8/l6gWs0Gsrlcsrn84pGo7Lb7Uqn07/ddzQaKZPJqFKpyO/3q1Qq6fF4SJJcLpfa7bZarZYcDoeq1aokqdPpyDRN9Xo9BQIBJRIJbTYbeTweSd/mzlarldbrtcLhsMbjsbrd7lv3TaVSqtfrqlarikQi2u/3Mk3zh3Ver1eGYSiZTCoejysUCn33u49isajJZKLpdKpgMKhYLKbZbPY6K4C/69OXX03hAgAA4J/CixsAAIBFEG4AAAAWQbgBAABYBOEGAABgEYQbAACARRBuAAAAFkG4AQAAWAThBgAAYBGEGwAAgEUQbgAAABZBuAEAAFgE4QYAAGARXwFi3KP8RhCvxAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import load_dataset\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Download GGUF model from Hugging Face\n",
        "# ----------------------------\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"Deeps03/qwen2-1.5b-log-classifier\",  # your repo\n",
        "    filename=\"qwen2-1.5b-log-classifier-Q5_K_M.gguf\"  # pick Q4, Q5, or Q8\n",
        ")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Load dataset (small subset)\n",
        "# ----------------------------\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/generated_logs.jsonl\")\n",
        "test_data = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)[\"test\"]\n",
        "\n",
        "small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Inference loop\n",
        "# ----------------------------\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for ex in small_test:\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert log analysis assistant. Your task is to classify log messages into one of:\n",
        "'incident', 'preventive_action', or 'normal'.\n",
        "Provide only the classification word.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Classify the following log message:\n",
        "Log Entry: {ex['message']}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=5,\n",
        "        stop=[\"<|im_end|>\", \"\\n\"],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    # Extract model response\n",
        "    pred = output[\"choices\"][0][\"text\"].strip().lower()\n",
        "\n",
        "    # fallback if model returns junk\n",
        "    if pred not in [\"incident\", \"preventive_action\", \"normal\"]:\n",
        "        pred = \"normal\"\n",
        "\n",
        "    y_pred.append(pred)\n",
        "    y_true.append(ex[\"label\"].lower())\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Confusion Matrix\n",
        "# ----------------------------\n",
        "labels = [\"incident\", \"preventive_action\", \"normal\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix - Qwen2 Log Classifier (Q5_K_M)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KVFh4lCcbarg",
        "outputId": "d0497b3b-6adf-4de5-9dff-2213376e4d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /root/.cache/huggingface/hub/models--Deeps03--qwen2-1.5b-log-classifier/snapshots/e182527b13a5113ab93fd40f379cb7a3f9c48a85/qwen2-1.5b-log-classifier-Q5_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                          general.file_type u32              = 17\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q5_K:  168 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_K - Medium\n",
            "print_info: file size   = 1.04 GiB (5.80 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.9308 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.54 B\n",
            "print_info: general.name     = Qwen2 1.5B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151646\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151645 '<|im_end|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 338 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  1066.91 MiB\n",
            "....................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   112.00 MiB\n",
            "llama_kv_cache_unified: size =  112.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2704\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   302.18 MiB\n",
            "llama_context: graph nodes  = 1070\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '17', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151645', 'general.basename': 'Qwen2', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2 1.5B Instruct', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '1.5B', 'general.license': 'apache-2.0', 'qwen2.context_length': '32768', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '8960', 'qwen2.attention.head_count': '12'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    7148.89 ms /    74 tokens (   96.61 ms per token,    10.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.00 ms /     1 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    7311.81 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1353.44 ms /    16 tokens (   84.59 ms per token,    11.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.52 ms /     1 runs   (  157.52 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1514.27 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2052.64 ms /    13 tokens (  157.90 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     641.45 ms /     3 runs   (  213.82 ms per token,     4.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    2700.56 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     993.34 ms /    12 tokens (   82.78 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     166.10 ms /     1 runs   (  166.10 ms per token,     6.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1162.68 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1091.57 ms /    13 tokens (   83.97 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.47 ms /     3 runs   (  158.49 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1572.97 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1464.36 ms /    17 tokens (   86.14 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     466.38 ms /     3 runs   (  155.46 ms per token,     6.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1936.50 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1586.28 ms /    19 tokens (   83.49 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.77 ms /     3 runs   (  158.26 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2066.95 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2041.43 ms /    25 tokens (   81.66 ms per token,    12.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.71 ms /     1 runs   (  158.71 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2203.35 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1367.43 ms /    15 tokens (   91.16 ms per token,    10.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     647.70 ms /     3 runs   (  215.90 ms per token,     4.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    2021.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2263.24 ms /    18 tokens (  125.74 ms per token,     7.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     477.57 ms /     3 runs   (  159.19 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2746.56 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1479.51 ms /    18 tokens (   82.20 ms per token,    12.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.38 ms /     3 runs   (  158.79 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1961.73 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1029.26 ms /    12 tokens (   85.77 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.13 ms /     1 runs   (  157.13 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1189.52 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1169.04 ms /    14 tokens (   83.50 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.46 ms /     1 runs   (  157.46 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1329.49 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1669.76 ms /    20 tokens (   83.49 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.68 ms /     1 runs   (  155.68 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1828.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1305.84 ms /    16 tokens (   81.61 ms per token,    12.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     467.05 ms /     3 runs   (  155.68 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1779.00 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2586.79 ms /    17 tokens (  152.16 ms per token,     6.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     536.23 ms /     3 runs   (  178.74 ms per token,     5.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    3129.00 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1092.42 ms /    13 tokens (   84.03 ms per token,    11.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     177.82 ms /     1 runs   (  177.82 ms per token,     5.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    1273.48 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1323.30 ms /    16 tokens (   82.71 ms per token,    12.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.16 ms /     1 runs   (  161.16 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1487.65 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1155.58 ms /    14 tokens (   82.54 ms per token,    12.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.52 ms /     3 runs   (  158.51 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1636.98 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1158.69 ms /    14 tokens (   82.76 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.01 ms /     1 runs   (  159.01 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1320.76 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1748.11 ms /    21 tokens (   83.24 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.30 ms /     3 runs   (  157.10 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2225.57 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1193.04 ms /    14 tokens (   85.22 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.61 ms /     3 runs   (  157.54 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1671.50 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2846.63 ms /    18 tokens (  158.15 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     174.10 ms /     1 runs   (  174.10 ms per token,     5.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    3023.94 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1163.35 ms /    14 tokens (   83.10 ms per token,    12.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.34 ms /     3 runs   (  156.11 ms per token,     6.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1637.53 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1398.46 ms /    17 tokens (   82.26 ms per token,    12.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.63 ms /     3 runs   (  157.54 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1877.02 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1071.70 ms /    13 tokens (   82.44 ms per token,    12.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.98 ms /     1 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1232.89 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     999.40 ms /    12 tokens (   83.28 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.22 ms /     1 runs   (  155.22 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1157.73 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1446.53 ms /    17 tokens (   85.09 ms per token,    11.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.57 ms /     3 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1925.98 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2305.16 ms /    22 tokens (  104.78 ms per token,     9.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     223.91 ms /     1 runs   (  223.91 ms per token,     4.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    2538.33 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1884.10 ms /    14 tokens (  134.58 ms per token,     7.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.83 ms /     1 runs   (  158.83 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2046.39 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1179.43 ms /    14 tokens (   84.24 ms per token,    11.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.26 ms /     1 runs   (  157.26 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1339.82 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1760.75 ms /    12 tokens (  146.73 ms per token,     6.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.05 ms /     1 runs   (  167.05 ms per token,     5.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1930.94 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1285.65 ms /    12 tokens (  107.14 ms per token,     9.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.20 ms /     1 runs   (  157.20 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1445.99 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1495.82 ms /    18 tokens (   83.10 ms per token,    12.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     163.79 ms /     1 runs   (  163.79 ms per token,     6.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1662.77 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1807.66 ms /    22 tokens (   82.17 ms per token,    12.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.24 ms /     1 runs   (  157.24 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1968.01 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1874.99 ms /    18 tokens (  104.17 ms per token,     9.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     650.01 ms /     3 runs   (  216.67 ms per token,     4.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    2531.67 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1479.93 ms /    12 tokens (  123.33 ms per token,     8.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.85 ms /     1 runs   (  158.85 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1642.02 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1148.32 ms /    13 tokens (   88.33 ms per token,    11.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.33 ms /     1 runs   (  156.33 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1307.72 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1642.64 ms /    20 tokens (   82.13 ms per token,    12.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.89 ms /     3 runs   (  161.63 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2133.58 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1395.85 ms /    17 tokens (   82.11 ms per token,    12.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.42 ms /     1 runs   (  155.42 ms per token,     6.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1554.36 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1899.73 ms /    23 tokens (   82.60 ms per token,    12.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.29 ms /     1 runs   (  161.29 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2064.21 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1563.64 ms /    19 tokens (   82.30 ms per token,    12.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     462.39 ms /     3 runs   (  154.13 ms per token,     6.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    2031.91 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2264.51 ms /    14 tokens (  161.75 ms per token,     6.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     218.36 ms /     1 runs   (  218.36 ms per token,     4.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    2486.32 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2427.11 ms /    29 tokens (   83.69 ms per token,    11.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.23 ms /     3 runs   (  156.74 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2906.83 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1246.37 ms /    15 tokens (   83.09 ms per token,    12.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.90 ms /     1 runs   (  156.90 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1406.38 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1497.08 ms /    18 tokens (   83.17 ms per token,    12.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.85 ms /     3 runs   (  157.95 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1976.99 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     997.89 ms /    12 tokens (   83.16 ms per token,    12.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.10 ms /     1 runs   (  158.10 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1159.18 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1015.93 ms /    12 tokens (   84.66 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.12 ms /     1 runs   (  157.12 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1176.35 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1789.83 ms /    20 tokens (   89.49 ms per token,    11.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     211.84 ms /     1 runs   (  211.84 ms per token,     4.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    2005.15 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2437.72 ms /    16 tokens (  152.36 ms per token,     6.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.93 ms /     3 runs   (  158.31 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2918.49 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1078.36 ms /    13 tokens (   82.95 ms per token,    12.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.57 ms /     1 runs   (  167.57 ms per token,     5.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    1249.35 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1075.85 ms /    13 tokens (   82.76 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.63 ms /     3 runs   (  156.54 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1551.41 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1821.90 ms /    22 tokens (   82.81 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.06 ms /     1 runs   (  158.06 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1983.14 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1106.06 ms /    13 tokens (   85.08 ms per token,    11.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     176.06 ms /     1 runs   (  176.06 ms per token,     5.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    1285.28 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1249.86 ms /    15 tokens (   83.32 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.80 ms /     1 runs   (  156.80 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1409.81 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1864.22 ms /    20 tokens (   93.21 ms per token,    10.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     216.49 ms /     1 runs   (  216.49 ms per token,     4.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    2084.38 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1989.50 ms /    12 tokens (  165.79 ms per token,     6.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.43 ms /     1 runs   (  158.43 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    2151.08 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1250.52 ms /    15 tokens (   83.37 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.99 ms /     1 runs   (  158.99 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1412.76 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1248.03 ms /    15 tokens (   83.20 ms per token,    12.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     168.53 ms /     1 runs   (  168.53 ms per token,     5.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    1419.73 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1426.34 ms /    17 tokens (   83.90 ms per token,    11.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.83 ms /     3 runs   (  156.61 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1902.15 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1189.72 ms /    14 tokens (   84.98 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.37 ms /     1 runs   (  159.37 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1352.44 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1259.04 ms /    15 tokens (   83.94 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.45 ms /     1 runs   (  161.45 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1423.80 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2470.01 ms /    28 tokens (   88.21 ms per token,    11.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     650.54 ms /     3 runs   (  216.85 ms per token,     4.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    3127.26 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2372.75 ms /    19 tokens (  124.88 ms per token,     8.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     481.19 ms /     3 runs   (  160.40 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2860.15 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1030.27 ms /    12 tokens (   85.86 ms per token,    11.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.90 ms /     1 runs   (  156.90 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1190.34 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1166.25 ms /    14 tokens (   83.30 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.85 ms /     1 runs   (  165.85 ms per token,     6.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1335.29 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1426.80 ms /    17 tokens (   83.93 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.27 ms /     1 runs   (  159.27 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1589.24 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1096.37 ms /    13 tokens (   84.34 ms per token,    11.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.09 ms /     1 runs   (  155.09 ms per token,     6.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1254.49 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1167.07 ms /    14 tokens (   83.36 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.61 ms /     1 runs   (  157.61 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1327.81 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1162.71 ms /    14 tokens (   83.05 ms per token,    12.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.26 ms /     3 runs   (  157.09 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1639.72 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3543.82 ms /    29 tokens (  122.20 ms per token,     8.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.41 ms /     3 runs   (  157.14 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    4021.14 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1363.99 ms /    16 tokens (   85.25 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     462.48 ms /     3 runs   (  154.16 ms per token,     6.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1832.25 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1251.05 ms /    15 tokens (   83.40 ms per token,    11.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.83 ms /     1 runs   (  158.83 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1413.01 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 62 prefix-match hit, remaining 9 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     769.48 ms /     9 tokens (   85.50 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.21 ms /     1 runs   (  156.21 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =     928.69 ms /    10 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1240.16 ms /    15 tokens (   82.68 ms per token,    12.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     464.34 ms /     3 runs   (  154.78 ms per token,     6.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1710.37 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1252.25 ms /    15 tokens (   83.48 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.54 ms /     1 runs   (  158.54 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1414.00 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2021.37 ms /    23 tokens (   87.89 ms per token,    11.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     217.06 ms /     1 runs   (  217.06 ms per token,     4.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    2241.77 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2092.02 ms /    13 tokens (  160.92 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.69 ms /     1 runs   (  157.69 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2252.91 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 31 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2542.71 ms /    31 tokens (   82.02 ms per token,    12.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     168.68 ms /     1 runs   (  168.68 ms per token,     5.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    2714.56 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1546.98 ms /    18 tokens (   85.94 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.79 ms /     3 runs   (  158.93 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2029.62 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1713.43 ms /    21 tokens (   81.59 ms per token,    12.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.69 ms /     3 runs   (  157.90 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    2193.49 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1918.91 ms /    23 tokens (   83.43 ms per token,    11.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.34 ms /     1 runs   (  161.34 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2083.51 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1358.09 ms /    13 tokens (  104.47 ms per token,     9.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     223.95 ms /     1 runs   (  223.95 ms per token,     4.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1585.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1851.92 ms /    12 tokens (  154.33 ms per token,     6.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.14 ms /     1 runs   (  159.14 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2014.20 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1763.83 ms /    21 tokens (   83.99 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.47 ms /     3 runs   (  156.49 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2239.12 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1507.76 ms /    18 tokens (   83.76 ms per token,    11.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     497.22 ms /     3 runs   (  165.74 ms per token,     6.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2010.99 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1412.72 ms /    17 tokens (   83.10 ms per token,    12.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.54 ms /     3 runs   (  156.18 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1887.23 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1089.85 ms /    13 tokens (   83.83 ms per token,    11.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.34 ms /     1 runs   (  155.34 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1248.42 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1433.27 ms /    17 tokens (   84.31 ms per token,    11.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     464.07 ms /     3 runs   (  154.69 ms per token,     6.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1903.13 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2512.47 ms /    16 tokens (  157.03 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     526.98 ms /     3 runs   (  175.66 ms per token,     5.69 tokens per second)\n",
            "llama_perf_context_print:       total time =    3045.75 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1586.93 ms /    19 tokens (   83.52 ms per token,    11.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     164.09 ms /     1 runs   (  164.09 ms per token,     6.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1760.64 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1650.72 ms /    20 tokens (   82.54 ms per token,    12.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.50 ms /     3 runs   (  159.83 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2136.13 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1904.80 ms /    23 tokens (   82.82 ms per token,    12.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.66 ms /     1 runs   (  156.66 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2064.84 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1085.46 ms /    13 tokens (   83.50 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.23 ms /     1 runs   (  157.23 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1245.86 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1077.81 ms /    13 tokens (   82.91 ms per token,    12.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.41 ms /     1 runs   (  160.41 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1241.50 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1701.00 ms /    18 tokens (   94.50 ms per token,    10.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     633.88 ms /     3 runs   (  211.29 ms per token,     4.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    2341.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2184.87 ms /    19 tokens (  114.99 ms per token,     8.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     481.28 ms /     3 runs   (  160.43 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2672.21 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1233.26 ms /    15 tokens (   82.22 ms per token,    12.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.75 ms /     1 runs   (  156.75 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1393.14 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1421.80 ms /    17 tokens (   83.64 ms per token,    11.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     466.10 ms /     3 runs   (  155.37 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1893.52 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1171.31 ms /    14 tokens (   83.67 ms per token,    11.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     483.21 ms /     3 runs   (  161.07 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1661.19 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1259.57 ms /    15 tokens (   83.97 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     463.94 ms /     3 runs   (  154.65 ms per token,     6.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1729.52 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1166.18 ms /    14 tokens (   83.30 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     511.94 ms /     3 runs   (  170.65 ms per token,     5.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    1684.51 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2695.69 ms /    18 tokens (  149.76 ms per token,     6.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     520.64 ms /     3 runs   (  173.55 ms per token,     5.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    3222.16 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1312.73 ms /    16 tokens (   82.05 ms per token,    12.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.88 ms /     1 runs   (  156.88 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1472.85 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1165.56 ms /    14 tokens (   83.25 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.73 ms /     1 runs   (  156.73 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1325.52 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1795.53 ms /    22 tokens (   81.61 ms per token,    12.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.37 ms /     1 runs   (  165.37 ms per token,     6.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1964.11 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1302.08 ms /    16 tokens (   81.38 ms per token,    12.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.36 ms /     1 runs   (  157.36 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1462.62 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1072.78 ms /    13 tokens (   82.52 ms per token,    12.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.16 ms /     3 runs   (  157.05 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1549.88 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1320.67 ms /    16 tokens (   82.54 ms per token,    12.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.62 ms /     3 runs   (  157.54 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1799.35 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2742.33 ms /    17 tokens (  161.31 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.05 ms /     1 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2904.88 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1979.04 ms /    24 tokens (   82.46 ms per token,    12.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.81 ms /     1 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2140.02 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1011.59 ms /    12 tokens (   84.30 ms per token,    11.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.88 ms /     1 runs   (  156.88 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1171.70 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1320.50 ms /    16 tokens (   82.53 ms per token,    12.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     169.03 ms /     1 runs   (  169.03 ms per token,     5.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    1492.70 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2479.17 ms /    29 tokens (   85.49 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.65 ms /     1 runs   (  160.65 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2642.97 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1452.92 ms /    17 tokens (   85.47 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.33 ms /     1 runs   (  159.33 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1615.45 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3056.51 ms /    21 tokens (  145.55 ms per token,     6.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.06 ms /     3 runs   (  157.35 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    3534.44 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1089.73 ms /    13 tokens (   83.83 ms per token,    11.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     517.92 ms /     3 runs   (  172.64 ms per token,     5.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    1613.64 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1252.54 ms /    15 tokens (   83.50 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.14 ms /     3 runs   (  156.71 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1728.64 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1176.23 ms /    14 tokens (   84.02 ms per token,    11.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.19 ms /     1 runs   (  165.19 ms per token,     6.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1344.55 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1424.91 ms /    17 tokens (   83.82 ms per token,    11.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.69 ms /     1 runs   (  158.69 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1586.60 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1258.01 ms /    15 tokens (   83.87 ms per token,    11.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.34 ms /     3 runs   (  159.78 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1743.12 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1088.05 ms /    13 tokens (   83.70 ms per token,    11.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     172.06 ms /     1 runs   (  172.06 ms per token,     5.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    1263.62 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2654.56 ms /    17 tokens (  156.15 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     467.61 ms /     3 runs   (  155.87 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    3128.08 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1081.70 ms /    13 tokens (   83.21 ms per token,    12.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.18 ms /     1 runs   (  157.18 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1241.98 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1551.70 ms /    18 tokens (   86.21 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.12 ms /     1 runs   (  158.12 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1713.04 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1835.66 ms /    22 tokens (   83.44 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.41 ms /     3 runs   (  156.80 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2312.02 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1436.30 ms /    17 tokens (   84.49 ms per token,    11.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.59 ms /     3 runs   (  158.86 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1918.69 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1824.36 ms /    22 tokens (   82.93 ms per token,    12.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.31 ms /     1 runs   (  158.31 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1985.84 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2455.33 ms /    14 tokens (  175.38 ms per token,     5.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.63 ms /     3 runs   (  158.88 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2937.84 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1231.01 ms /    15 tokens (   82.07 ms per token,    12.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     466.27 ms /     3 runs   (  155.42 ms per token,     6.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1703.27 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1962.74 ms /    24 tokens (   81.78 ms per token,    12.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.07 ms /     3 runs   (  161.36 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2452.61 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1502.25 ms /    18 tokens (   83.46 ms per token,    11.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.10 ms /     3 runs   (  157.37 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1980.18 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1275.96 ms /    15 tokens (   85.06 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     464.09 ms /     3 runs   (  154.70 ms per token,     6.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    1746.39 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1429.76 ms /    17 tokens (   84.10 ms per token,    11.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.34 ms /     1 runs   (  207.34 ms per token,     4.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    1640.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2636.76 ms /    17 tokens (  155.10 ms per token,     6.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.39 ms /     3 runs   (  159.46 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    3121.24 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2297.70 ms /    28 tokens (   82.06 ms per token,    12.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.24 ms /     3 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2776.83 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1425.29 ms /    17 tokens (   83.84 ms per token,    11.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.37 ms /     1 runs   (  157.37 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1586.98 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1523.68 ms /    18 tokens (   84.65 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.73 ms /     3 runs   (  159.58 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2008.19 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1180.73 ms /    14 tokens (   84.34 ms per token,    11.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     163.14 ms /     1 runs   (  163.14 ms per token,     6.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1348.26 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2552.73 ms /    19 tokens (  134.35 ms per token,     7.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     214.47 ms /     1 runs   (  214.47 ms per token,     4.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2775.56 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2244.82 ms /    24 tokens (   93.53 ms per token,    10.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.75 ms /     1 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2408.67 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1858.18 ms /    22 tokens (   84.46 ms per token,    11.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.82 ms /     1 runs   (  156.82 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2018.19 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1960.19 ms /    23 tokens (   85.23 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.00 ms /     1 runs   (  158.00 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    2121.43 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1694.26 ms /    20 tokens (   84.71 ms per token,    11.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.98 ms /     1 runs   (  156.98 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1854.36 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1915.55 ms /    23 tokens (   83.28 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.16 ms /     1 runs   (  159.16 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2077.83 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2626.23 ms /    16 tokens (  164.14 ms per token,     6.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     168.46 ms /     1 runs   (  168.46 ms per token,     5.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    2797.85 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1135.84 ms /    13 tokens (   87.37 ms per token,    11.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.25 ms /     1 runs   (  158.25 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1298.18 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1597.33 ms /    19 tokens (   84.07 ms per token,    11.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.51 ms /     3 runs   (  157.50 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    2075.59 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1151.14 ms /    14 tokens (   82.22 ms per token,    12.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.88 ms /     1 runs   (  155.88 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1310.43 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1990.61 ms /    23 tokens (   86.55 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.49 ms /     3 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2470.91 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2557.35 ms /    30 tokens (   85.24 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     217.63 ms /     1 runs   (  217.63 ms per token,     4.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    2778.75 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2201.06 ms /    13 tokens (  169.31 ms per token,     5.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.20 ms /     1 runs   (  157.20 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    2362.24 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1351.66 ms /    16 tokens (   84.48 ms per token,    11.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     465.27 ms /     3 runs   (  155.09 ms per token,     6.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    1822.76 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1582.01 ms /    19 tokens (   83.26 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.18 ms /     1 runs   (  157.18 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1742.34 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1012.02 ms /    12 tokens (   84.34 ms per token,    11.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.63 ms /     1 runs   (  156.63 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1171.77 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1674.73 ms /    20 tokens (   83.74 ms per token,    11.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     466.04 ms /     3 runs   (  155.35 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    2146.51 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1587.07 ms /    19 tokens (   83.53 ms per token,    11.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     463.84 ms /     3 runs   (  154.61 ms per token,     6.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    2056.49 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2143.06 ms /    14 tokens (  153.08 ms per token,     6.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     222.47 ms /     1 runs   (  222.47 ms per token,     4.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    2369.13 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1532.58 ms /    16 tokens (   95.79 ms per token,    10.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.69 ms /     1 runs   (  156.69 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1692.47 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1084.12 ms /    13 tokens (   83.39 ms per token,    11.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.18 ms /     1 runs   (  159.18 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1246.50 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1522.69 ms /    18 tokens (   84.59 ms per token,    11.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.05 ms /     1 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1683.95 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1513.90 ms /    18 tokens (   84.11 ms per token,    11.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     489.65 ms /     3 runs   (  163.22 ms per token,     6.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2009.77 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1465.31 ms /    17 tokens (   86.19 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.21 ms /     1 runs   (  157.21 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1625.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     999.77 ms /    12 tokens (   83.31 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.26 ms /     1 runs   (  155.26 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1158.15 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2229.33 ms /    18 tokens (  123.85 ms per token,     8.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     646.17 ms /     3 runs   (  215.39 ms per token,     4.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2889.12 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1887.15 ms /    21 tokens (   89.86 ms per token,    11.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     462.39 ms /     3 runs   (  154.13 ms per token,     6.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    2355.69 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1986.33 ms /    24 tokens (   82.76 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.33 ms /     1 runs   (  159.33 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2148.83 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1304.35 ms /    15 tokens (   86.96 ms per token,    11.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     162.43 ms /     1 runs   (  162.43 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1470.19 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1423.13 ms /    17 tokens (   83.71 ms per token,    11.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.47 ms /     1 runs   (  157.47 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1583.78 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1523.49 ms /    18 tokens (   84.64 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     466.95 ms /     3 runs   (  155.65 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1996.45 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2566.63 ms /    18 tokens (  142.59 ms per token,     7.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     597.37 ms /     3 runs   (  199.12 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3170.29 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1677.45 ms /    20 tokens (   83.87 ms per token,    11.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.17 ms /     3 runs   (  157.06 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2154.52 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1223.84 ms /    14 tokens (   87.42 ms per token,    11.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.44 ms /     3 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1703.21 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1490.37 ms /    18 tokens (   82.80 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     164.38 ms /     1 runs   (  164.38 ms per token,     6.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1657.97 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 61 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1094.50 ms /    13 tokens (   84.19 ms per token,    11.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.17 ms /     1 runs   (  160.17 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1257.94 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1252.78 ms /    15 tokens (   83.52 ms per token,    11.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.57 ms /     3 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1732.31 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1259.08 ms /    15 tokens (   83.94 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.11 ms /     1 runs   (  203.11 ms per token,     4.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    1465.81 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2386.20 ms /    14 tokens (  170.44 ms per token,     5.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.40 ms /     1 runs   (  156.40 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2545.97 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     930.62 ms /    11 tokens (   84.60 ms per token,    11.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.44 ms /     1 runs   (  158.44 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1092.52 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1109.13 ms /    13 tokens (   85.32 ms per token,    11.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     166.65 ms /     1 runs   (  166.65 ms per token,     6.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.02 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1529.03 ms /    18 tokens (   84.95 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.15 ms /     3 runs   (  161.38 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2019.20 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1775.57 ms /    21 tokens (   84.55 ms per token,    11.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     480.12 ms /     3 runs   (  160.04 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2261.85 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1283.71 ms /    15 tokens (   85.58 ms per token,    11.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.43 ms /     3 runs   (  159.81 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1769.30 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 59 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     716.35 ms /     8 tokens (   89.54 ms per token,    11.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.80 ms /     1 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =     877.73 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2319.05 ms /    13 tokens (  178.39 ms per token,     5.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.63 ms /     1 runs   (  205.63 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    2528.07 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1350.00 ms /    16 tokens (   84.38 ms per token,    11.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.12 ms /     1 runs   (  160.12 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1513.77 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2040.17 ms /    24 tokens (   85.01 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.52 ms /     1 runs   (  159.52 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2202.91 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1108.68 ms /    13 tokens (   85.28 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.13 ms /     1 runs   (  167.13 ms per token,     5.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.00 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1047.42 ms /    12 tokens (   87.29 ms per token,    11.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.91 ms /     1 runs   (  158.91 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1209.49 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1806.11 ms /    21 tokens (   86.00 ms per token,    11.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     172.68 ms /     1 runs   (  172.68 ms per token,     5.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    1982.06 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1354.68 ms /    16 tokens (   84.67 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     515.39 ms /     3 runs   (  171.80 ms per token,     5.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    1876.07 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2432.51 ms /    14 tokens (  173.75 ms per token,     5.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.73 ms /     1 runs   (  157.73 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2593.44 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1196.88 ms /    14 tokens (   85.49 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.14 ms /     1 runs   (  160.14 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1360.11 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1577.31 ms /    18 tokens (   87.63 ms per token,    11.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.68 ms /     3 runs   (  156.56 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2052.69 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1181.75 ms /    13 tokens (   90.90 ms per token,    11.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.44 ms /     1 runs   (  161.44 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1346.41 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1303.00 ms /    15 tokens (   86.87 ms per token,    11.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.57 ms /     1 runs   (  157.57 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1463.70 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1448.15 ms /    17 tokens (   85.19 ms per token,    11.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     488.05 ms /     3 runs   (  162.68 ms per token,     6.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1942.02 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1441.75 ms /    16 tokens (   90.11 ms per token,    11.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     637.82 ms /     3 runs   (  212.61 ms per token,     4.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    2085.94 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2328.86 ms /    18 tokens (  129.38 ms per token,     7.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     464.41 ms /     3 runs   (  154.80 ms per token,     6.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    2801.01 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1544.03 ms /    18 tokens (   85.78 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.14 ms /     3 runs   (  157.05 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2021.20 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1404.20 ms /    16 tokens (   87.76 ms per token,    11.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.44 ms /     3 runs   (  156.81 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1880.94 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1029.26 ms /    12 tokens (   85.77 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     480.71 ms /     3 runs   (  160.24 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1515.74 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 58 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     873.09 ms /    10 tokens (   87.31 ms per token,    11.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     480.08 ms /     3 runs   (  160.03 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1359.07 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1329.20 ms /    16 tokens (   83.07 ms per token,    12.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.81 ms /     1 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1490.99 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2066.11 ms /    13 tokens (  158.93 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     644.31 ms /     3 runs   (  214.77 ms per token,     4.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2717.34 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1103.09 ms /    13 tokens (   84.85 ms per token,    11.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.81 ms /     1 runs   (  158.81 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1265.15 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1847.22 ms /    21 tokens (   87.96 ms per token,    11.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.38 ms /     3 runs   (  158.46 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    2328.62 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1290.96 ms /    15 tokens (   86.06 ms per token,    11.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.97 ms /     1 runs   (  158.97 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1453.13 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1365.16 ms /    16 tokens (   85.32 ms per token,    11.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.25 ms /     1 runs   (  160.25 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1528.84 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 33 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2791.38 ms /    33 tokens (   84.59 ms per token,    11.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.00 ms /     1 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2953.66 ms /    34 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2048.43 ms /    12 tokens (  170.70 ms per token,     5.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     220.54 ms /     1 runs   (  220.54 ms per token,     4.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    2272.46 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1394.28 ms /    14 tokens (   99.59 ms per token,    10.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.25 ms /     3 runs   (  157.75 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1873.44 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1041.75 ms /    12 tokens (   86.81 ms per token,    11.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.15 ms /     1 runs   (  157.15 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1202.02 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1734.71 ms /    20 tokens (   86.74 ms per token,    11.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.90 ms /     3 runs   (  156.97 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2211.59 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1370.98 ms /    16 tokens (   85.69 ms per token,    11.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.29 ms /     1 runs   (  159.29 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1533.57 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1262.97 ms /    15 tokens (   84.20 ms per token,    11.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.38 ms /     3 runs   (  159.46 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1747.43 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1520.82 ms /    18 tokens (   84.49 ms per token,    11.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     499.23 ms /     3 runs   (  166.41 ms per token,     6.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    2026.29 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2853.09 ms /    19 tokens (  150.16 ms per token,     6.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     481.64 ms /     3 runs   (  160.55 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    3340.70 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1477.23 ms /    17 tokens (   86.90 ms per token,    11.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.15 ms /     1 runs   (  159.15 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1639.65 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1043.66 ms /    12 tokens (   86.97 ms per token,    11.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.13 ms /     1 runs   (  160.13 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1206.98 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1377.96 ms /    16 tokens (   86.12 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.70 ms /     1 runs   (  158.70 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1539.88 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2067.75 ms /    24 tokens (   86.16 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.64 ms /     3 runs   (  157.21 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    2545.26 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1376.38 ms /    16 tokens (   86.02 ms per token,    11.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     499.09 ms /     3 runs   (  166.36 ms per token,     6.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    1881.84 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2906.03 ms /    18 tokens (  161.45 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.30 ms /     3 runs   (  158.43 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    3387.16 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1194.19 ms /    14 tokens (   85.30 ms per token,    11.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     169.69 ms /     1 runs   (  169.69 ms per token,     5.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    1370.38 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1540.04 ms /    17 tokens (   90.59 ms per token,    11.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.70 ms /     1 runs   (  158.70 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1701.87 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1948.83 ms /    23 tokens (   84.73 ms per token,    11.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.16 ms /     1 runs   (  160.16 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    2112.24 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1358.97 ms /    16 tokens (   84.94 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.11 ms /     1 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1520.18 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1334.86 ms /    16 tokens (   83.43 ms per token,    11.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.11 ms /     1 runs   (  161.11 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1499.14 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1882.64 ms /    16 tokens (  117.66 ms per token,     8.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     230.18 ms /     1 runs   (  230.18 ms per token,     4.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2116.17 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2404.87 ms /    22 tokens (  109.31 ms per token,     9.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.75 ms /     1 runs   (  165.75 ms per token,     6.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2573.91 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1434.42 ms /    17 tokens (   84.38 ms per token,    11.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.74 ms /     1 runs   (  156.74 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1594.42 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2419.55 ms /    29 tokens (   83.43 ms per token,    11.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.40 ms /     3 runs   (  156.47 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2894.86 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1259.65 ms /    15 tokens (   83.98 ms per token,    11.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.16 ms /     3 runs   (  159.39 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1743.72 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1596.35 ms /    19 tokens (   84.02 ms per token,    11.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.56 ms /     3 runs   (  161.52 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2086.94 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2256.15 ms /    12 tokens (  188.01 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     223.25 ms /     1 runs   (  223.25 ms per token,     4.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    2483.06 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1368.27 ms /    16 tokens (   85.52 ms per token,    11.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     168.58 ms /     1 runs   (  168.58 ms per token,     5.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    1540.14 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1752.23 ms /    19 tokens (   92.22 ms per token,    10.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     482.73 ms /     3 runs   (  160.91 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2241.29 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1124.32 ms /    13 tokens (   86.49 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.89 ms /     1 runs   (  161.89 ms per token,     6.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1487.21 ms /    17 tokens (   87.48 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     485.95 ms /     3 runs   (  161.98 ms per token,     6.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1979.06 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1558.32 ms /    18 tokens (   86.57 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     517.55 ms /     3 runs   (  172.52 ms per token,     5.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    2083.29 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2726.70 ms /    18 tokens (  151.48 ms per token,     6.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     548.27 ms /     3 runs   (  182.76 ms per token,     5.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    3281.60 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1195.84 ms /    14 tokens (   85.42 ms per token,    11.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.15 ms /     3 runs   (  159.38 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1679.82 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1379.88 ms /    16 tokens (   86.24 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.85 ms /     1 runs   (  157.85 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1540.82 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1293.22 ms /    15 tokens (   86.21 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     486.56 ms /     3 runs   (  162.19 ms per token,     6.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1785.59 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1459.05 ms /    17 tokens (   85.83 ms per token,    11.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     481.31 ms /     3 runs   (  160.44 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1946.37 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1476.86 ms /    17 tokens (   86.87 ms per token,    11.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.00 ms /     1 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1639.09 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     969.70 ms /    11 tokens (   88.15 ms per token,    11.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.60 ms /     1 runs   (  195.60 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1168.96 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3194.24 ms /    23 tokens (  138.88 ms per token,     7.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.50 ms /     1 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    3355.87 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1280.50 ms /    15 tokens (   85.37 ms per token,    11.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.34 ms /     1 runs   (  161.34 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1445.10 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2053.79 ms /    24 tokens (   85.57 ms per token,    11.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.37 ms /     1 runs   (  165.37 ms per token,     6.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    2222.37 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1199.83 ms /    14 tokens (   85.70 ms per token,    11.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     166.46 ms /     1 runs   (  166.46 ms per token,     6.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    1369.56 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1143.00 ms /    13 tokens (   87.92 ms per token,    11.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     482.20 ms /     3 runs   (  160.73 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1631.12 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2086.84 ms /    24 tokens (   86.95 ms per token,    11.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     674.76 ms /     3 runs   (  224.92 ms per token,     4.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    2768.53 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2343.94 ms /    16 tokens (  146.50 ms per token,     6.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.46 ms /     1 runs   (  160.46 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2507.96 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1993.60 ms /    23 tokens (   86.68 ms per token,    11.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.99 ms /     1 runs   (  159.99 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2157.39 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1487.40 ms /    17 tokens (   87.49 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.47 ms /     3 runs   (  156.82 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1963.70 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1113.42 ms /    13 tokens (   85.65 ms per token,    11.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.62 ms /     1 runs   (  161.62 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1278.34 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1069.85 ms /    12 tokens (   89.15 ms per token,    11.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.35 ms /     1 runs   (  167.35 ms per token,     5.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    1240.31 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1430.37 ms /    17 tokens (   84.14 ms per token,    11.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.57 ms /     3 runs   (  156.86 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1907.08 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2821.79 ms /    19 tokens (  148.52 ms per token,     6.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.20 ms /     1 runs   (  199.20 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3024.56 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1615.18 ms /    19 tokens (   85.01 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.75 ms /     3 runs   (  158.58 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    2096.79 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1896.26 ms /    22 tokens (   86.19 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.62 ms /     1 runs   (  158.62 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2058.34 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1657.99 ms /    19 tokens (   87.26 ms per token,    11.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.50 ms /     1 runs   (  161.50 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1822.82 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1198.46 ms /    14 tokens (   85.60 ms per token,    11.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.09 ms /     1 runs   (  159.09 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1360.72 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1572.06 ms /    18 tokens (   87.34 ms per token,    11.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.38 ms /     1 runs   (  161.38 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1736.66 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2008.45 ms /    16 tokens (  125.53 ms per token,     7.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     227.89 ms /     1 runs   (  227.89 ms per token,     4.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2239.83 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1806.90 ms /    16 tokens (  112.93 ms per token,     8.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.89 ms /     1 runs   (  157.89 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1969.17 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1565.98 ms /    18 tokens (   87.00 ms per token,    11.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.62 ms /     3 runs   (  158.21 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2046.74 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1421.68 ms /    16 tokens (   88.85 ms per token,    11.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     174.23 ms /     1 runs   (  174.23 ms per token,     5.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    1599.18 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1972.56 ms /    23 tokens (   85.76 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     163.87 ms /     1 runs   (  163.87 ms per token,     6.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2140.16 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1421.09 ms /    16 tokens (   88.82 ms per token,    11.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     162.40 ms /     1 runs   (  162.40 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1586.68 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1390.94 ms /    16 tokens (   86.93 ms per token,    11.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.81 ms /     1 runs   (  157.81 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1551.98 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3050.87 ms /    21 tokens (  145.28 ms per token,     6.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.05 ms /     1 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    3213.11 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1208.30 ms /    14 tokens (   86.31 ms per token,    11.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.56 ms /     1 runs   (  159.56 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1371.01 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1126.68 ms /    13 tokens (   86.67 ms per token,    11.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.10 ms /     1 runs   (  161.10 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1291.09 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1124.08 ms /    13 tokens (   86.47 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     481.98 ms /     3 runs   (  160.66 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1611.98 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1971.48 ms /    23 tokens (   85.72 ms per token,    11.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     477.27 ms /     3 runs   (  159.09 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2454.64 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1284.31 ms /    15 tokens (   85.62 ms per token,    11.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.73 ms /     1 runs   (  165.73 ms per token,     6.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1453.27 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3198.42 ms /    23 tokens (  139.06 ms per token,     7.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     225.65 ms /     1 runs   (  225.65 ms per token,     4.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    3427.43 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1057.80 ms /    12 tokens (   88.15 ms per token,    11.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.32 ms /     1 runs   (  158.32 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1219.21 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 67 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.91 ms /     2 runs   (  159.95 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     322.93 ms /     3 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     974.38 ms /    11 tokens (   88.58 ms per token,    11.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     172.13 ms /     1 runs   (  172.13 ms per token,     5.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    1151.06 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2300.48 ms /    27 tokens (   85.20 ms per token,    11.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.78 ms /     3 runs   (  156.93 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2777.27 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1294.48 ms /    15 tokens (   86.30 ms per token,    11.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.66 ms /     3 runs   (  161.55 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1785.03 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1224.56 ms /    14 tokens (   87.47 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     488.87 ms /     3 runs   (  162.96 ms per token,     6.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1719.36 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3358.38 ms /    24 tokens (  139.93 ms per token,     7.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.50 ms /     1 runs   (  161.50 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3523.14 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1173.47 ms /    13 tokens (   90.27 ms per token,    11.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     481.78 ms /     3 runs   (  160.59 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1661.10 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1296.05 ms /    15 tokens (   86.40 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.26 ms /     1 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1459.54 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1379.89 ms /    16 tokens (   86.24 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     163.28 ms /     1 runs   (  163.28 ms per token,     6.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1546.65 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1515.77 ms /    17 tokens (   89.16 ms per token,    11.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.47 ms /     3 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1996.10 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1223.76 ms /    14 tokens (   87.41 ms per token,    11.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.75 ms /     1 runs   (  160.75 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1387.79 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1040.43 ms /    12 tokens (   86.70 ms per token,    11.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     466.31 ms /     3 runs   (  155.44 ms per token,     6.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1512.58 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3057.27 ms /    20 tokens (  152.86 ms per token,     6.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     488.39 ms /     3 runs   (  162.80 ms per token,     6.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3551.75 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1476.96 ms /    17 tokens (   86.88 ms per token,    11.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.29 ms /     1 runs   (  159.29 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1639.46 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1058.21 ms /    12 tokens (   88.18 ms per token,    11.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.91 ms /     1 runs   (  158.91 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1220.38 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1311.71 ms /    15 tokens (   87.45 ms per token,    11.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.53 ms /     1 runs   (  159.53 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1474.44 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1282.57 ms /    15 tokens (   85.50 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.63 ms /     1 runs   (  158.63 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1444.44 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2486.56 ms /    29 tokens (   85.74 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.57 ms /     3 runs   (  156.52 ms per token,     6.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2962.15 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2371.82 ms /    13 tokens (  182.45 ms per token,     5.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.80 ms /     1 runs   (  189.80 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2572.10 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1475.52 ms /    17 tokens (   86.80 ms per token,    11.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     162.62 ms /     1 runs   (  162.62 ms per token,     6.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1641.41 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1139.57 ms /    13 tokens (   87.66 ms per token,    11.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.38 ms /     1 runs   (  158.38 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1301.22 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1126.02 ms /    13 tokens (   86.62 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.53 ms /     1 runs   (  167.53 ms per token,     5.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    1296.69 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2526.05 ms /    29 tokens (   87.11 ms per token,    11.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.51 ms /     3 runs   (  157.17 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    3003.40 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1857.22 ms /    21 tokens (   88.44 ms per token,    11.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     491.02 ms /     3 runs   (  163.67 ms per token,     6.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2354.31 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2872.47 ms /    18 tokens (  159.58 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     482.10 ms /     3 runs   (  160.70 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    3360.50 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1374.09 ms /    16 tokens (   85.88 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     486.64 ms /     3 runs   (  162.21 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1866.69 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1487.72 ms /    17 tokens (   87.51 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.18 ms /     1 runs   (  160.18 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1652.16 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1393.46 ms /    16 tokens (   87.09 ms per token,    11.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.04 ms /     3 runs   (  157.01 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1870.45 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1289.77 ms /    15 tokens (   85.98 ms per token,    11.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     487.21 ms /     3 runs   (  162.40 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1782.92 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1151.48 ms /    13 tokens (   88.58 ms per token,    11.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.02 ms /     1 runs   (  159.02 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1314.08 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2894.45 ms /    18 tokens (  160.80 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     172.19 ms /     1 runs   (  172.19 ms per token,     5.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    3069.87 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1985.58 ms /    23 tokens (   86.33 ms per token,    11.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.05 ms /     1 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2147.75 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1283.58 ms /    15 tokens (   85.57 ms per token,    11.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.59 ms /     1 runs   (  161.59 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1448.84 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1546.61 ms /    18 tokens (   85.92 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     469.08 ms /     3 runs   (  156.36 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2023.72 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1330.06 ms /    15 tokens (   88.67 ms per token,    11.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.94 ms /     1 runs   (  160.94 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1494.23 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1307.52 ms /    15 tokens (   87.17 ms per token,    11.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.50 ms /     1 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1469.14 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2803.05 ms /    20 tokens (  140.15 ms per token,     7.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     630.50 ms /     3 runs   (  210.17 ms per token,     4.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    3439.94 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1215.55 ms /    14 tokens (   86.82 ms per token,    11.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.65 ms /     1 runs   (  158.65 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1377.35 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1038.34 ms /    12 tokens (   86.53 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.16 ms /     1 runs   (  158.16 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1199.64 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     976.80 ms /    11 tokens (   88.80 ms per token,    11.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     171.77 ms /     1 runs   (  171.77 ms per token,     5.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    1151.97 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2379.12 ms /    27 tokens (   88.12 ms per token,    11.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     490.72 ms /     3 runs   (  163.57 ms per token,     6.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2876.12 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1127.33 ms /    13 tokens (   86.72 ms per token,    11.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.42 ms /     1 runs   (  161.42 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1291.84 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     755.95 ms /     8 tokens (   94.49 ms per token,    10.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     166.84 ms /     1 runs   (  166.84 ms per token,     5.99 tokens per second)\n",
            "llama_perf_context_print:       total time =     926.09 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1942.08 ms /    18 tokens (  107.89 ms per token,     9.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     221.65 ms /     1 runs   (  221.65 ms per token,     4.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    2167.15 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2191.43 ms /    16 tokens (  136.96 ms per token,     7.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.40 ms /     3 runs   (  159.80 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2677.68 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2516.55 ms /    30 tokens (   83.88 ms per token,    11.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.97 ms /     1 runs   (  156.97 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2676.71 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1592.80 ms /    18 tokens (   88.49 ms per token,    11.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.09 ms /     3 runs   (  156.03 ms per token,     6.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    2066.67 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1665.53 ms /    19 tokens (   87.66 ms per token,    11.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.76 ms /     1 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1826.40 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1037.81 ms /    12 tokens (   86.48 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.26 ms /     1 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1201.19 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     927.10 ms /    10 tokens (   92.71 ms per token,    10.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     654.59 ms /     3 runs   (  218.20 ms per token,     4.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    1588.59 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2335.88 ms /    16 tokens (  145.99 ms per token,     6.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.93 ms /     3 runs   (  157.64 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2816.86 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1475.85 ms /    17 tokens (   86.81 ms per token,    11.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.91 ms /     3 runs   (  159.97 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1961.70 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1575.15 ms /    17 tokens (   92.66 ms per token,    10.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.42 ms /     3 runs   (  158.47 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    2056.59 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1484.02 ms /    17 tokens (   87.30 ms per token,    11.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.12 ms /     3 runs   (  158.37 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1964.92 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1727.70 ms /    20 tokens (   86.38 ms per token,    11.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     163.09 ms /     1 runs   (  163.09 ms per token,     6.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1894.04 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1856.91 ms /    15 tokens (  123.79 ms per token,     8.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     651.06 ms /     3 runs   (  217.02 ms per token,     4.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    2515.48 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1624.94 ms /    15 tokens (  108.33 ms per token,     9.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.52 ms /     1 runs   (  159.52 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1790.16 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1252.14 ms /    14 tokens (   89.44 ms per token,    11.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.85 ms /     1 runs   (  158.85 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1414.17 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1151.35 ms /    13 tokens (   88.57 ms per token,    11.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.62 ms /     1 runs   (  157.62 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1312.11 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1202.00 ms /    14 tokens (   85.86 ms per token,    11.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.14 ms /     3 runs   (  156.05 ms per token,     6.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1676.12 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1377.92 ms /    16 tokens (   86.12 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.68 ms /     3 runs   (  156.89 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1854.37 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1363.71 ms /    16 tokens (   85.23 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.31 ms /     3 runs   (  159.77 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1848.83 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2417.49 ms /    17 tokens (  142.21 ms per token,     7.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     648.57 ms /     3 runs   (  216.19 ms per token,     4.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    3072.62 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1267.02 ms /    15 tokens (   84.47 ms per token,    11.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     488.22 ms /     3 runs   (  162.74 ms per token,     6.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1761.51 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2673.34 ms /    32 tokens (   83.54 ms per token,    11.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.70 ms /     1 runs   (  158.70 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2837.21 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1655.22 ms /    19 tokens (   87.12 ms per token,    11.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.22 ms /     1 runs   (  165.22 ms per token,     6.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1823.73 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1528.25 ms /    18 tokens (   84.90 ms per token,    11.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.30 ms /     3 runs   (  161.43 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2018.72 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1114.06 ms /    13 tokens (   85.70 ms per token,    11.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     172.83 ms /     1 runs   (  172.83 ms per token,     5.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    1290.67 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2460.92 ms /    13 tokens (  189.30 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.71 ms /     3 runs   (  156.24 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2939.66 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2666.65 ms /    32 tokens (   83.33 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.37 ms /     1 runs   (  161.37 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2831.23 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1527.13 ms /    18 tokens (   84.84 ms per token,    11.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.74 ms /     3 runs   (  159.91 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2013.14 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     962.27 ms /    11 tokens (   87.48 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.11 ms /     1 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1123.51 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1556.46 ms /    18 tokens (   86.47 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     483.56 ms /     3 runs   (  161.19 ms per token,     6.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2045.80 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1131.45 ms /    13 tokens (   87.03 ms per token,    11.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.37 ms /     1 runs   (  159.37 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1295.54 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3277.66 ms /    24 tokens (  136.57 ms per token,     7.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.65 ms /     1 runs   (  159.65 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    3441.15 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1876.97 ms /    22 tokens (   85.32 ms per token,    11.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.56 ms /     1 runs   (  161.56 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2041.73 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1034.89 ms /    12 tokens (   86.24 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.84 ms /     1 runs   (  157.84 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1195.88 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1037.02 ms /    12 tokens (   86.42 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.07 ms /     1 runs   (  167.07 ms per token,     5.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1207.19 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1318.33 ms /    15 tokens (   87.89 ms per token,    11.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.37 ms /     1 runs   (  159.37 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1480.94 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2044.92 ms /    24 tokens (   85.21 ms per token,    11.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.71 ms /     1 runs   (  158.71 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2206.76 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2609.27 ms /    17 tokens (  153.49 ms per token,     6.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     215.22 ms /     1 runs   (  215.22 ms per token,     4.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    2827.99 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1181.73 ms /    12 tokens (   98.48 ms per token,    10.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.06 ms /     1 runs   (  158.06 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1343.04 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1471.90 ms /    17 tokens (   86.58 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     492.10 ms /     3 runs   (  164.03 ms per token,     6.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1970.22 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1719.01 ms /    20 tokens (   85.95 ms per token,    11.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.92 ms /     1 runs   (  157.92 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1883.49 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1102.40 ms /    13 tokens (   84.80 ms per token,    11.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.65 ms /     1 runs   (  160.65 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1266.24 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1358.15 ms /    16 tokens (   84.88 ms per token,    11.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.65 ms /     1 runs   (  158.65 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1519.96 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 64 prefix-match hit, remaining 7 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =     640.13 ms /     7 tokens (   91.45 ms per token,    10.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.47 ms /     1 runs   (  161.47 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =     804.80 ms /     8 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1388.05 ms /    16 tokens (   86.75 ms per token,    11.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     600.78 ms /     3 runs   (  200.26 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1995.74 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2600.26 ms /    18 tokens (  144.46 ms per token,     6.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.22 ms /     3 runs   (  157.41 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    3080.09 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1224.04 ms /    14 tokens (   87.43 ms per token,    11.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.05 ms /     1 runs   (  158.05 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1385.37 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1030.98 ms /    12 tokens (   85.91 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     482.09 ms /     3 runs   (  160.70 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1519.07 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1974.43 ms /    23 tokens (   85.84 ms per token,    11.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.50 ms /     1 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    2136.17 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1534.59 ms /    17 tokens (   90.27 ms per token,    11.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     487.39 ms /     3 runs   (  162.46 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2028.21 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1449.39 ms /    17 tokens (   85.26 ms per token,    11.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     588.32 ms /     3 runs   (  196.11 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2044.08 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2224.86 ms /    14 tokens (  158.92 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.83 ms /     1 runs   (  157.83 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2385.88 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1484.27 ms /    17 tokens (   87.31 ms per token,    11.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.45 ms /     1 runs   (  158.45 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1646.09 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1137.47 ms /    13 tokens (   87.50 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.58 ms /     1 runs   (  159.58 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1300.22 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1391.86 ms /    16 tokens (   86.99 ms per token,    11.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.89 ms /     1 runs   (  159.89 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1555.06 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1246.55 ms /    14 tokens (   89.04 ms per token,    11.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.12 ms /     3 runs   (  158.71 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1728.55 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1205.54 ms /    14 tokens (   86.11 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.92 ms /     1 runs   (  159.92 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1368.58 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1458.28 ms /    16 tokens (   91.14 ms per token,    10.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     485.70 ms /     3 runs   (  161.90 ms per token,     6.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1950.12 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2802.47 ms /    17 tokens (  164.85 ms per token,     6.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.82 ms /     3 runs   (  157.61 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    3281.20 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1295.46 ms /    15 tokens (   86.36 ms per token,    11.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.12 ms /     3 runs   (  156.04 ms per token,     6.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1769.41 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1740.95 ms /    20 tokens (   87.05 ms per token,    11.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     482.14 ms /     3 runs   (  160.71 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2229.20 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1639.88 ms /    19 tokens (   86.31 ms per token,    11.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     465.80 ms /     3 runs   (  155.27 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    2111.59 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1701.19 ms /    20 tokens (   85.06 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.54 ms /     3 runs   (  158.18 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2181.51 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1063.19 ms /    11 tokens (   96.65 ms per token,    10.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     219.98 ms /     1 runs   (  219.98 ms per token,     4.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    1286.57 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2759.97 ms /    19 tokens (  145.26 ms per token,     6.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.54 ms /     1 runs   (  157.54 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    2922.14 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1037.55 ms /    12 tokens (   86.46 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     166.25 ms /     1 runs   (  166.25 ms per token,     6.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1206.98 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1886.32 ms /    22 tokens (   85.74 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     167.09 ms /     1 runs   (  167.09 ms per token,     5.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    2056.73 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1342.06 ms /    16 tokens (   83.88 ms per token,    11.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.44 ms /     3 runs   (  158.15 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1822.64 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1387.68 ms /    16 tokens (   86.73 ms per token,    11.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.42 ms /     1 runs   (  157.42 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1548.69 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1233.72 ms /    14 tokens (   88.12 ms per token,    11.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     475.45 ms /     3 runs   (  158.48 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1715.41 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2864.37 ms /    19 tokens (  150.76 ms per token,     6.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     488.10 ms /     3 runs   (  162.70 ms per token,     6.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    3358.79 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1330.97 ms /    15 tokens (   88.73 ms per token,    11.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.12 ms /     3 runs   (  158.04 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1811.41 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1379.72 ms /    16 tokens (   86.23 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.40 ms /     3 runs   (  156.13 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1853.98 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2054.92 ms /    23 tokens (   89.34 ms per token,    11.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.31 ms /     1 runs   (  158.31 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2216.52 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1052.54 ms /    12 tokens (   87.71 ms per token,    11.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.65 ms /     1 runs   (  158.65 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1214.80 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1134.17 ms /    13 tokens (   87.24 ms per token,    11.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.27 ms /     1 runs   (  157.27 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1294.48 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1431.08 ms /    15 tokens (   95.41 ms per token,    10.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     652.92 ms /     3 runs   (  217.64 ms per token,     4.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    2090.64 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2307.97 ms /    18 tokens (  128.22 ms per token,     7.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.41 ms /     1 runs   (  160.41 ms per token,     6.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2471.57 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1955.44 ms /    23 tokens (   85.02 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.26 ms /     1 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    2118.90 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1693.77 ms /    19 tokens (   89.15 ms per token,    11.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.63 ms /     3 runs   (  159.88 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2179.28 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1146.03 ms /    13 tokens (   88.16 ms per token,    11.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.76 ms /     3 runs   (  159.59 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1630.59 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1530.21 ms /    18 tokens (   85.01 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     486.31 ms /     3 runs   (  162.10 ms per token,     6.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2022.53 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 61 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2428.11 ms /    18 tokens (  134.90 ms per token,     7.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     214.70 ms /     1 runs   (  214.70 ms per token,     4.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2646.34 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2311.41 ms /    24 tokens (   96.31 ms per token,    10.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     170.40 ms /     1 runs   (  170.40 ms per token,     5.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    2489.05 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1540.43 ms /    18 tokens (   85.58 ms per token,    11.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     482.25 ms /     3 runs   (  160.75 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2029.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1269.01 ms /    15 tokens (   84.60 ms per token,    11.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     470.57 ms /     3 runs   (  156.86 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1745.41 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1764.84 ms /    20 tokens (   88.24 ms per token,    11.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.99 ms /     1 runs   (  158.99 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1927.10 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1130.51 ms /    13 tokens (   86.96 ms per token,    11.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.49 ms /     1 runs   (  159.49 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1293.73 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1054.35 ms /    12 tokens (   87.86 ms per token,    11.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     222.36 ms /     1 runs   (  222.36 ms per token,     4.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1280.11 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3169.40 ms /    24 tokens (  132.06 ms per token,     7.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.80 ms /     1 runs   (  157.80 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    3330.39 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1524.42 ms /    18 tokens (   84.69 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     492.21 ms /     3 runs   (  164.07 ms per token,     6.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2022.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1376.38 ms /    16 tokens (   86.02 ms per token,    11.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.87 ms /     1 runs   (  156.87 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1536.45 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1508.86 ms /    17 tokens (   88.76 ms per token,    11.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.21 ms /     3 runs   (  158.74 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1991.05 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1786.84 ms /    21 tokens (   85.09 ms per token,    11.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.38 ms /     1 runs   (  158.38 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1948.38 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1644.11 ms /    18 tokens (   91.34 ms per token,    10.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     222.45 ms /     1 runs   (  222.45 ms per token,     4.50 tokens per second)\n",
            "llama_perf_context_print:       total time =    1870.13 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2707.26 ms /    20 tokens (  135.36 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.57 ms /     1 runs   (  165.57 ms per token,     6.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2878.14 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1556.23 ms /    18 tokens (   86.46 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.87 ms /     3 runs   (  156.29 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2031.25 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1868.60 ms /    22 tokens (   84.94 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.76 ms /     1 runs   (  158.76 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2030.62 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1201.39 ms /    14 tokens (   85.81 ms per token,    11.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.92 ms /     3 runs   (  157.31 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1679.29 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1275.56 ms /    15 tokens (   85.04 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.49 ms /     3 runs   (  161.50 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1767.03 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1299.06 ms /    15 tokens (   86.60 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     632.35 ms /     3 runs   (  210.78 ms per token,     4.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    1938.17 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2057.46 ms /    12 tokens (  171.45 ms per token,     5.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.96 ms /     1 runs   (  159.96 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2220.56 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1454.26 ms /    17 tokens (   85.54 ms per token,    11.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.23 ms /     1 runs   (  158.23 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1615.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1733.29 ms /    20 tokens (   86.66 ms per token,    11.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.39 ms /     1 runs   (  159.39 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1896.00 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1803.59 ms /    21 tokens (   85.89 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.24 ms /     3 runs   (  157.41 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    2281.92 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2063.60 ms /    24 tokens (   85.98 ms per token,    11.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.88 ms /     1 runs   (  159.88 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2226.72 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1827.26 ms /    20 tokens (   91.36 ms per token,    10.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     216.87 ms /     1 runs   (  216.87 ms per token,     4.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    2047.60 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2579.18 ms /    18 tokens (  143.29 ms per token,     6.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     477.53 ms /     3 runs   (  159.18 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    3062.62 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1965.10 ms /    23 tokens (   85.44 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.62 ms /     1 runs   (  159.62 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2127.85 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1498.58 ms /    17 tokens (   88.15 ms per token,    11.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.19 ms /     1 runs   (  159.19 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1660.99 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1487.80 ms /    17 tokens (   87.52 ms per token,    11.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     488.58 ms /     3 runs   (  162.86 ms per token,     6.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1982.31 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1286.01 ms /    15 tokens (   85.73 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.09 ms /     1 runs   (  159.09 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1448.27 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1141.45 ms /    13 tokens (   87.80 ms per token,    11.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     169.61 ms /     1 runs   (  169.61 ms per token,     5.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    1314.29 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3061.53 ms /    20 tokens (  153.08 ms per token,     6.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     174.63 ms /     1 runs   (  174.63 ms per token,     5.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    3239.41 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1279.71 ms /    15 tokens (   85.31 ms per token,    11.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.76 ms /     1 runs   (  157.76 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1442.22 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1538.14 ms /    18 tokens (   85.45 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     467.99 ms /     3 runs   (  156.00 ms per token,     6.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    2012.04 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1419.93 ms /    16 tokens (   88.75 ms per token,    11.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     479.56 ms /     3 runs   (  159.85 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1905.70 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1033.94 ms /    12 tokens (   86.16 ms per token,    11.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     471.43 ms /     3 runs   (  157.14 ms per token,     6.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1511.11 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1471.38 ms /    17 tokens (   86.55 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     467.33 ms /     3 runs   (  155.78 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1944.76 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2514.50 ms /    15 tokens (  167.63 ms per token,     5.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     219.56 ms /     1 runs   (  219.56 ms per token,     4.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    2737.92 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1553.07 ms /    17 tokens (   91.36 ms per token,    10.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     477.84 ms /     3 runs   (  159.28 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2037.07 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1371.90 ms /    16 tokens (   85.74 ms per token,    11.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.68 ms /     1 runs   (  159.68 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1534.77 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1947.38 ms /    23 tokens (   84.67 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     476.27 ms /     3 runs   (  158.76 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2429.66 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1359.33 ms /    16 tokens (   84.96 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.56 ms /     1 runs   (  158.56 ms per token,     6.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1521.33 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1612.02 ms /    19 tokens (   84.84 ms per token,    11.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     173.76 ms /     1 runs   (  173.76 ms per token,     5.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    1788.95 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1697.88 ms /    13 tokens (  130.61 ms per token,     7.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     227.55 ms /     1 runs   (  227.55 ms per token,     4.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1928.72 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2228.38 ms /    19 tokens (  117.28 ms per token,     8.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     465.38 ms /     3 runs   (  155.13 ms per token,     6.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    2699.61 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1298.56 ms /    15 tokens (   86.57 ms per token,    11.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     172.58 ms /     1 runs   (  172.58 ms per token,     5.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    1474.34 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2538.53 ms /    30 tokens (   84.62 ms per token,    11.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.18 ms /     1 runs   (  159.18 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2702.61 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1728.33 ms /    20 tokens (   86.42 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.10 ms /     1 runs   (  160.10 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1891.56 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1471.13 ms /    17 tokens (   86.54 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.74 ms /     1 runs   (  165.74 ms per token,     6.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1640.07 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1479.56 ms /    14 tokens (  105.68 ms per token,     9.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     210.90 ms /     1 runs   (  210.90 ms per token,     4.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    1694.55 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2819.47 ms /    23 tokens (  122.59 ms per token,     8.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.59 ms /     1 runs   (  161.59 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2984.33 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1635.34 ms /    19 tokens (   86.07 ms per token,    11.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.07 ms /     3 runs   (  158.02 ms per token,     6.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    2115.25 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1046.88 ms /    12 tokens (   87.24 ms per token,    11.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     165.29 ms /     1 runs   (  165.29 ms per token,     6.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1215.33 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1361.28 ms /    16 tokens (   85.08 ms per token,    11.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     465.98 ms /     3 runs   (  155.33 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1833.06 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 70 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     643.67 ms /     4 runs   (  160.92 ms per token,     6.21 tokens per second)\n",
            "llama_perf_context_print:       total time =     649.22 ms /     5 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1333.14 ms /    16 tokens (   83.32 ms per token,    12.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.41 ms /     1 runs   (  157.41 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1493.67 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1347.39 ms /    16 tokens (   84.21 ms per token,    11.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     608.17 ms /     3 runs   (  202.72 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    1962.16 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2834.01 ms /    22 tokens (  128.82 ms per token,     7.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.74 ms /     1 runs   (  156.74 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2994.55 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1112.71 ms /    13 tokens (   85.59 ms per token,    11.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     168.67 ms /     1 runs   (  168.67 ms per token,     5.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    1284.83 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1552.34 ms /    18 tokens (   86.24 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.59 ms /     3 runs   (  156.20 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2026.88 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1271.10 ms /    15 tokens (   84.74 ms per token,    11.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.62 ms /     1 runs   (  157.62 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1431.84 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1126.43 ms /    13 tokens (   86.65 ms per token,    11.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     155.77 ms /     1 runs   (  155.77 ms per token,     6.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1285.30 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1098.62 ms /    13 tokens (   84.51 ms per token,    11.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.76 ms /     1 runs   (  160.76 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1262.46 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3354.59 ms /    29 tokens (  115.68 ms per token,     8.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     221.25 ms /     1 runs   (  221.25 ms per token,     4.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    3579.41 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1833.20 ms /    18 tokens (  101.84 ms per token,     9.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.03 ms /     1 runs   (  157.03 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1994.04 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1112.47 ms /    13 tokens (   85.57 ms per token,    11.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     182.17 ms /     1 runs   (  182.17 ms per token,     5.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1298.09 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1665.56 ms /    19 tokens (   87.66 ms per token,    11.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.70 ms /     1 runs   (  156.70 ms per token,     6.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1825.55 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1037.12 ms /    12 tokens (   86.43 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.19 ms /     1 runs   (  159.19 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1199.41 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1211.04 ms /    14 tokens (   86.50 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     465.77 ms /     3 runs   (  155.26 ms per token,     6.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    1682.77 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1273.91 ms /    15 tokens (   84.93 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     480.80 ms /     3 runs   (  160.27 ms per token,     6.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1760.56 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1666.21 ms /    12 tokens (  138.85 ms per token,     7.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     222.90 ms /     1 runs   (  222.90 ms per token,     4.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    1892.51 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1545.25 ms /    11 tokens (  140.48 ms per token,     7.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     161.58 ms /     1 runs   (  161.58 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1712.20 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1553.33 ms /    18 tokens (   86.30 ms per token,    11.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     156.99 ms /     1 runs   (  156.99 ms per token,     6.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1713.99 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1465.11 ms /    17 tokens (   86.18 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.83 ms /     1 runs   (  158.83 ms per token,     6.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1627.13 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1382.52 ms /    16 tokens (   86.41 ms per token,    11.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     473.33 ms /     3 runs   (  157.78 ms per token,     6.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1862.18 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1793.64 ms /    21 tokens (   85.41 ms per token,    11.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     477.07 ms /     3 runs   (  159.02 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2276.58 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1261.16 ms /    14 tokens (   90.08 ms per token,    11.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     474.76 ms /     3 runs   (  158.25 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1741.80 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2938.93 ms /    19 tokens (  154.68 ms per token,     6.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     472.45 ms /     3 runs   (  157.48 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    3417.17 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1870.60 ms /    22 tokens (   85.03 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.07 ms /     1 runs   (  160.07 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2033.90 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1350.66 ms /    16 tokens (   84.42 ms per token,    11.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     179.46 ms /     1 runs   (  179.46 ms per token,     5.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1534.66 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1147.98 ms /    13 tokens (   88.31 ms per token,    11.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.40 ms /     1 runs   (  159.40 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1310.51 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1460.72 ms /    17 tokens (   85.92 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.86 ms /     3 runs   (  159.62 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1946.04 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1510.04 ms /    17 tokens (   88.83 ms per token,    11.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     478.35 ms /     3 runs   (  159.45 ms per token,     6.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1994.42 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    3001.59 ms /    20 tokens (  150.08 ms per token,     6.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     158.24 ms /     1 runs   (  158.24 ms per token,     6.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    3163.04 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1480.18 ms /    17 tokens (   87.07 ms per token,    11.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     160.02 ms /     1 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1643.34 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1135.05 ms /    13 tokens (   87.31 ms per token,    11.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     485.16 ms /     3 runs   (  161.72 ms per token,     6.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1626.22 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1172.19 ms /    13 tokens (   90.17 ms per token,    11.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.13 ms /     1 runs   (  159.13 ms per token,     6.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1334.62 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1717.52 ms /    20 tokens (   85.88 ms per token,    11.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     159.84 ms /     1 runs   (  159.84 ms per token,     6.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1880.49 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2026.75 ms /    24 tokens (   84.45 ms per token,    11.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     157.45 ms /     1 runs   (  157.45 ms per token,     6.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    2187.33 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1189.44 ms /    12 tokens (   99.12 ms per token,    10.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     214.52 ms /     1 runs   (  214.52 ms per token,     4.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    1407.48 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    2110.23 ms /    12 tokens (  175.85 ms per token,     5.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     173.37 ms /     1 runs   (  173.37 ms per token,     5.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    2287.07 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1442.35 ms /    17 tokens (   84.84 ms per token,    11.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     484.91 ms /     3 runs   (  161.64 ms per token,     6.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1933.67 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1287.17 ms /    15 tokens (   85.81 ms per token,    11.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     468.54 ms /     3 runs   (  156.18 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1761.62 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7149.21 ms\n",
            "llama_perf_context_print: prompt eval time =    1494.03 ms /    17 tokens (   87.88 ms per token,    11.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     467.66 ms /     3 runs   (  155.89 ms per token,     6.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    1967.52 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import load_dataset\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Download GGUF model from Hugging Face\n",
        "# ----------------------------\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"Deeps03/qwen2-1.5b-log-classifier\",  # your repo\n",
        "    filename=\"qwen2-1.5b-log-classifier-Q8_0.gguf\"  # pick Q4, Q5, or Q8\n",
        ")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Load dataset (small subset)\n",
        "# ----------------------------\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/generated_logs.jsonl\")\n",
        "test_data = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)[\"test\"]\n",
        "\n",
        "small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Inference loop\n",
        "# ----------------------------\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for ex in small_test:\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert log analysis assistant. Your task is to classify log messages into one of:\n",
        "'incident', 'preventive_action', or 'normal'.\n",
        "Provide only the classification word.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Classify the following log message:\n",
        "Log Entry: {ex['message']}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=5,\n",
        "        stop=[\"<|im_end|>\", \"\\n\"],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    # Extract model response\n",
        "    pred = output[\"choices\"][0][\"text\"].strip().lower()\n",
        "\n",
        "    # fallback if model returns junk\n",
        "    if pred not in [\"incident\", \"preventive_action\", \"normal\"]:\n",
        "        pred = \"normal\"\n",
        "\n",
        "    y_pred.append(pred)\n",
        "    y_true.append(ex[\"label\"].lower())\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Confusion Matrix\n",
        "# ----------------------------\n",
        "labels = [\"incident\", \"preventive_action\", \"normal\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix - Qwen2 Log Classifier (Q8_0)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7_Jfu52PgXSP",
        "outputId": "8cc27906-071d-4dfe-ef89-6337a7e023f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /root/.cache/huggingface/hub/models--Deeps03--qwen2-1.5b-log-classifier/snapshots/e182527b13a5113ab93fd40f379cb7a3f9c48a85/qwen2-1.5b-log-classifier-Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                          general.file_type u32              = 7\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q8_0:  197 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 1.53 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.9308 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.54 B\n",
            "print_info: general.name     = Qwen2 1.5B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151646\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151645 '<|im_end|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 338 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  1564.17 MiB\n",
            "......................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   112.00 MiB\n",
            "llama_kv_cache_unified: size =  112.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2704\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   302.18 MiB\n",
            "llama_context: graph nodes  = 1070\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '7', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151645', 'general.basename': 'Qwen2', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2 1.5B Instruct', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '1.5B', 'general.license': 'apache-2.0', 'qwen2.context_length': '32768', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '8960', 'qwen2.attention.head_count': '12'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    3650.11 ms /    74 tokens (   49.33 ms per token,    20.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.46 ms /     1 runs   (  191.46 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    3845.35 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1108.71 ms /    16 tokens (   69.29 ms per token,    14.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     257.49 ms /     1 runs   (  257.49 ms per token,     3.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    1369.91 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1832.37 ms /    13 tokens (  140.95 ms per token,     7.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     569.68 ms /     3 runs   (  189.89 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2410.70 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     625.37 ms /    12 tokens (   52.11 ms per token,    19.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.36 ms /     1 runs   (  197.36 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =     825.98 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     768.02 ms /    13 tokens (   59.08 ms per token,    16.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     585.97 ms /     3 runs   (  195.32 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1360.13 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     949.37 ms /    17 tokens (   55.85 ms per token,    17.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.56 ms /     3 runs   (  188.85 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1521.75 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1057.52 ms /    19 tokens (   55.66 ms per token,    17.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.59 ms /     3 runs   (  192.20 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1640.01 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1350.11 ms /    25 tokens (   54.00 ms per token,    18.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.61 ms /     1 runs   (  192.61 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1545.87 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     867.40 ms /    15 tokens (   57.83 ms per token,    17.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.58 ms /     3 runs   (  192.53 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1451.24 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1053.93 ms /    18 tokens (   58.55 ms per token,    17.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     734.21 ms /     3 runs   (  244.74 ms per token,     4.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1794.68 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1839.10 ms /    18 tokens (  102.17 ms per token,     9.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     585.62 ms /     3 runs   (  195.21 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2430.97 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     666.58 ms /    12 tokens (   55.55 ms per token,    18.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.16 ms /     1 runs   (  192.16 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =     861.88 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     742.56 ms /    14 tokens (   53.04 ms per token,    18.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.82 ms /     1 runs   (  203.82 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =     949.55 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1049.69 ms /    20 tokens (   52.48 ms per token,    19.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.35 ms /     1 runs   (  196.35 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1249.35 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     864.32 ms /    16 tokens (   54.02 ms per token,    18.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     567.18 ms /     3 runs   (  189.06 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1437.40 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     974.61 ms /    17 tokens (   57.33 ms per token,    17.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.75 ms /     3 runs   (  193.25 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1560.38 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     773.15 ms /    13 tokens (   59.47 ms per token,    16.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     207.47 ms /     1 runs   (  207.47 ms per token,     4.82 tokens per second)\n",
            "llama_perf_context_print:       total time =     983.77 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     838.05 ms /    16 tokens (   52.38 ms per token,    19.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.40 ms /     1 runs   (  193.40 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1034.72 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     734.45 ms /    14 tokens (   52.46 ms per token,    19.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     701.65 ms /     3 runs   (  233.88 ms per token,     4.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1442.82 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1829.95 ms /    14 tokens (  130.71 ms per token,     7.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.21 ms /     1 runs   (  191.21 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2029.58 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1195.32 ms /    21 tokens (   56.92 ms per token,    17.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     575.82 ms /     3 runs   (  191.94 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1777.06 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     734.58 ms /    14 tokens (   52.47 ms per token,    19.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     581.32 ms /     3 runs   (  193.77 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1321.83 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     933.61 ms /    18 tokens (   51.87 ms per token,    19.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.05 ms /     1 runs   (  188.05 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1124.88 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     753.42 ms /    14 tokens (   53.82 ms per token,    18.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.39 ms /     3 runs   (  189.46 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1327.66 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     956.03 ms /    17 tokens (   56.24 ms per token,    17.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     582.25 ms /     3 runs   (  194.09 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1544.42 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     776.56 ms /    13 tokens (   59.74 ms per token,    16.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.57 ms /     1 runs   (  193.57 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =     973.29 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     633.26 ms /    12 tokens (   52.77 ms per token,    18.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.56 ms /     1 runs   (  198.56 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =     834.98 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2019.15 ms /    17 tokens (  118.77 ms per token,     8.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     701.20 ms /     3 runs   (  233.73 ms per token,     4.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2727.02 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1107.36 ms /    22 tokens (   50.33 ms per token,    19.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.12 ms /     1 runs   (  199.12 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1309.87 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     734.46 ms /    14 tokens (   52.46 ms per token,    19.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.42 ms /     1 runs   (  192.42 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =     930.03 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     742.53 ms /    14 tokens (   53.04 ms per token,    18.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.98 ms /     1 runs   (  189.98 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =     935.70 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     650.97 ms /    12 tokens (   54.25 ms per token,    18.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.92 ms /     1 runs   (  188.92 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =     842.99 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     652.58 ms /    12 tokens (   54.38 ms per token,    18.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.78 ms /     1 runs   (  188.78 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =     844.41 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     926.42 ms /    18 tokens (   51.47 ms per token,    19.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.96 ms /     1 runs   (  188.96 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1118.38 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1130.57 ms /    22 tokens (   51.39 ms per token,    19.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.35 ms /     1 runs   (  190.35 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1324.17 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     925.74 ms /    18 tokens (   51.43 ms per token,    19.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.68 ms /     3 runs   (  193.23 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1511.38 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     637.47 ms /    12 tokens (   53.12 ms per token,    18.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.79 ms /     1 runs   (  201.79 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =     842.46 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2108.86 ms /    13 tokens (  162.22 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     258.30 ms /     1 runs   (  258.30 ms per token,     3.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    2370.98 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1052.09 ms /    20 tokens (   52.60 ms per token,    19.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.24 ms /     3 runs   (  190.75 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1630.48 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     976.91 ms /    17 tokens (   57.47 ms per token,    17.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.64 ms /     1 runs   (  198.64 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1178.92 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1241.90 ms /    23 tokens (   54.00 ms per token,    18.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.26 ms /     1 runs   (  191.26 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1436.44 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1066.63 ms /    19 tokens (   56.14 ms per token,    17.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.41 ms /     3 runs   (  192.14 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1648.92 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     720.89 ms /    14 tokens (   51.49 ms per token,    19.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     217.39 ms /     1 runs   (  217.39 ms per token,     4.60 tokens per second)\n",
            "llama_perf_context_print:       total time =     941.46 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1540.38 ms /    29 tokens (   53.12 ms per token,    18.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     601.00 ms /     3 runs   (  200.33 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2148.09 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     848.67 ms /    15 tokens (   56.58 ms per token,    17.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     238.45 ms /     1 runs   (  238.45 ms per token,     4.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1090.61 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2292.74 ms /    18 tokens (  127.37 ms per token,     7.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.35 ms /     3 runs   (  190.45 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2870.20 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     673.28 ms /    12 tokens (   56.11 ms per token,    17.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.75 ms /     1 runs   (  192.75 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =     869.20 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     660.09 ms /    12 tokens (   55.01 ms per token,    18.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.33 ms /     1 runs   (  189.33 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =     852.43 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1030.07 ms /    20 tokens (   51.50 ms per token,    19.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.93 ms /     1 runs   (  188.93 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1222.41 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     856.28 ms /    16 tokens (   53.52 ms per token,    18.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.87 ms /     3 runs   (  191.29 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1435.97 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     761.06 ms /    13 tokens (   58.54 ms per token,    17.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.38 ms /     1 runs   (  202.38 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =     967.03 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     755.57 ms /    13 tokens (   58.12 ms per token,    17.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.41 ms /     3 runs   (  190.80 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1333.98 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1124.65 ms /    22 tokens (   51.12 ms per token,    19.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.47 ms /     1 runs   (  192.47 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1320.35 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     791.21 ms /    13 tokens (   60.86 ms per token,    16.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.04 ms /     1 runs   (  192.04 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =     986.35 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2141.04 ms /    15 tokens (  142.74 ms per token,     7.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     251.90 ms /     1 runs   (  251.90 ms per token,     3.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    2398.56 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1088.07 ms /    20 tokens (   54.40 ms per token,    18.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.50 ms /     1 runs   (  200.50 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1291.87 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     641.77 ms /    12 tokens (   53.48 ms per token,    18.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.89 ms /     1 runs   (  190.89 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =     835.93 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     895.69 ms /    15 tokens (   59.71 ms per token,    16.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.65 ms /     1 runs   (  192.65 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1091.49 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     874.62 ms /    15 tokens (   58.31 ms per token,    17.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.83 ms /     1 runs   (  191.83 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1069.47 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     978.59 ms /    17 tokens (   57.56 ms per token,    17.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     584.66 ms /     3 runs   (  194.89 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1569.28 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     770.06 ms /    14 tokens (   55.00 ms per token,    18.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.44 ms /     1 runs   (  193.44 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =     966.78 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     887.32 ms /    15 tokens (   59.15 ms per token,    16.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.51 ms /     1 runs   (  199.51 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    1090.21 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1503.98 ms /    28 tokens (   53.71 ms per token,    18.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     620.72 ms /     3 runs   (  206.91 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    2130.85 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2598.36 ms /    19 tokens (  136.76 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     585.00 ms /     3 runs   (  195.00 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3190.61 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     669.99 ms /    12 tokens (   55.83 ms per token,    17.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.83 ms /     1 runs   (  191.83 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =     865.07 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     764.83 ms /    14 tokens (   54.63 ms per token,    18.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.69 ms /     1 runs   (  190.69 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =     960.48 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     965.53 ms /    17 tokens (   56.80 ms per token,    17.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.62 ms /     1 runs   (  187.62 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1156.32 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     779.19 ms /    13 tokens (   59.94 ms per token,    16.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.24 ms /     1 runs   (  191.24 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =     973.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     745.47 ms /    14 tokens (   53.25 ms per token,    18.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.41 ms /     1 runs   (  191.41 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =     940.44 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     765.42 ms /    14 tokens (   54.67 ms per token,    18.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.25 ms /     3 runs   (  189.41 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1339.78 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1562.05 ms /    29 tokens (   53.86 ms per token,    18.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     569.69 ms /     3 runs   (  189.90 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2137.69 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1538.01 ms /    16 tokens (   96.13 ms per token,    10.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     731.55 ms /     3 runs   (  243.85 ms per token,     4.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2276.30 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1105.24 ms /    15 tokens (   73.68 ms per token,    13.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.17 ms /     1 runs   (  202.17 ms per token,     4.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    1312.77 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 62 prefix-match hit, remaining 9 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     564.36 ms /     9 tokens (   62.71 ms per token,    15.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.17 ms /     1 runs   (  191.17 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =     758.75 ms /    10 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     915.37 ms /    15 tokens (   61.02 ms per token,    16.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     575.45 ms /     3 runs   (  191.82 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1496.84 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     869.94 ms /    15 tokens (   58.00 ms per token,    17.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.44 ms /     1 runs   (  188.44 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1061.51 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1276.52 ms /    23 tokens (   55.50 ms per token,    18.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     208.98 ms /     1 runs   (  208.98 ms per token,     4.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    1489.16 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     775.19 ms /    13 tokens (   59.63 ms per token,    16.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.85 ms /     1 runs   (  192.85 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =     971.26 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 31 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1651.52 ms /    31 tokens (   53.27 ms per token,    18.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.52 ms /     1 runs   (  191.52 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1846.38 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     950.13 ms /    18 tokens (   52.78 ms per token,    18.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     585.69 ms /     3 runs   (  195.23 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1541.85 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2558.63 ms /    21 tokens (  121.84 ms per token,     8.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     565.56 ms /     3 runs   (  188.52 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    3131.92 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1266.20 ms /    23 tokens (   55.05 ms per token,    18.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.79 ms /     1 runs   (  191.79 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1461.17 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     769.62 ms /    13 tokens (   59.20 ms per token,    16.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.84 ms /     1 runs   (  188.84 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =     961.54 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     634.41 ms /    12 tokens (   52.87 ms per token,    18.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.06 ms /     1 runs   (  198.06 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =     835.62 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1190.13 ms /    21 tokens (   56.67 ms per token,    17.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     593.38 ms /     3 runs   (  197.79 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1790.14 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     930.51 ms /    18 tokens (   51.70 ms per token,    19.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     570.56 ms /     3 runs   (  190.19 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1507.00 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     960.28 ms /    17 tokens (   56.49 ms per token,    17.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     555.89 ms /     3 runs   (  185.30 ms per token,     5.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1521.82 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     783.90 ms /    13 tokens (   60.30 ms per token,    16.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     247.54 ms /     1 runs   (  247.54 ms per token,     4.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1035.99 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2226.13 ms /    17 tokens (  130.95 ms per token,     7.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.89 ms /     3 runs   (  191.30 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2808.89 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     832.09 ms /    16 tokens (   52.01 ms per token,    19.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     560.14 ms /     3 runs   (  186.71 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1399.36 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1042.54 ms /    19 tokens (   54.87 ms per token,    18.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.71 ms /     1 runs   (  193.71 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1239.56 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1055.61 ms /    20 tokens (   52.78 ms per token,    18.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.84 ms /     3 runs   (  188.95 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1628.77 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1241.59 ms /    23 tokens (   53.98 ms per token,    18.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.52 ms /     1 runs   (  188.52 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1433.47 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     776.30 ms /    13 tokens (   59.72 ms per token,    16.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.99 ms /     1 runs   (  188.99 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =     968.44 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     775.66 ms /    13 tokens (   59.67 ms per token,    16.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.77 ms /     1 runs   (  189.77 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =     968.41 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     945.81 ms /    18 tokens (   52.55 ms per token,    19.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     633.67 ms /     3 runs   (  211.22 ms per token,     4.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    1585.85 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2324.10 ms /    19 tokens (  122.32 ms per token,     8.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     570.03 ms /     3 runs   (  190.01 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2901.46 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     892.55 ms /    15 tokens (   59.50 ms per token,    16.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.05 ms /     1 runs   (  194.05 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1089.79 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1007.01 ms /    17 tokens (   59.24 ms per token,    16.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     581.58 ms /     3 runs   (  193.86 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1594.52 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     820.48 ms /    14 tokens (   58.61 ms per token,    17.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.58 ms /     3 runs   (  193.19 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1405.95 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     865.18 ms /    15 tokens (   57.68 ms per token,    17.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     583.29 ms /     3 runs   (  194.43 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1454.35 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     771.30 ms /    14 tokens (   55.09 ms per token,    18.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.09 ms /     3 runs   (  188.70 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1343.39 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     957.71 ms /    18 tokens (   53.21 ms per token,    18.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     594.67 ms /     3 runs   (  198.22 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1558.42 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1913.08 ms /    16 tokens (  119.57 ms per token,     8.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     270.49 ms /     1 runs   (  270.49 ms per token,     3.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    2187.01 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1093.82 ms /    14 tokens (   78.13 ms per token,    12.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.89 ms /     1 runs   (  193.89 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1290.97 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1164.40 ms /    22 tokens (   52.93 ms per token,    18.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.89 ms /     1 runs   (  205.89 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    1373.83 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     848.23 ms /    16 tokens (   53.01 ms per token,    18.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.24 ms /     1 runs   (  199.24 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1050.67 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     763.19 ms /    13 tokens (   58.71 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.26 ms /     3 runs   (  192.09 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1346.10 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     853.76 ms /    16 tokens (   53.36 ms per token,    18.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.40 ms /     3 runs   (  192.13 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1436.16 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     994.05 ms /    17 tokens (   58.47 ms per token,    17.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.88 ms /     1 runs   (  190.88 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1188.21 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1255.75 ms /    24 tokens (   52.32 ms per token,    19.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.38 ms /     1 runs   (  191.38 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1450.40 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     658.89 ms /    12 tokens (   54.91 ms per token,    18.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.62 ms /     1 runs   (  202.62 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:       total time =     864.78 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1519.59 ms /    16 tokens (   94.97 ms per token,    10.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     247.41 ms /     1 runs   (  247.41 ms per token,     4.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1770.19 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2138.16 ms /    29 tokens (   73.73 ms per token,    13.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.20 ms /     1 runs   (  201.20 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    2342.64 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1008.62 ms /    17 tokens (   59.33 ms per token,    16.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.18 ms /     1 runs   (  192.18 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1204.06 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1189.08 ms /    21 tokens (   56.62 ms per token,    17.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.96 ms /     3 runs   (  191.32 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1769.08 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     825.14 ms /    13 tokens (   63.47 ms per token,    15.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     594.49 ms /     3 runs   (  198.16 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1425.80 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     898.98 ms /    15 tokens (   59.93 ms per token,    16.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     578.31 ms /     3 runs   (  192.77 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1483.21 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     787.69 ms /    14 tokens (   56.26 ms per token,    17.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.10 ms /     1 runs   (  195.10 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =     986.23 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     995.69 ms /    17 tokens (   58.57 ms per token,    17.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.06 ms /     1 runs   (  196.06 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1195.10 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1223.60 ms /    15 tokens (   81.57 ms per token,    12.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     728.51 ms /     3 runs   (  242.84 ms per token,     4.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1958.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1528.01 ms /    13 tokens (  117.54 ms per token,     8.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.24 ms /     1 runs   (  194.24 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1725.47 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     987.49 ms /    17 tokens (   58.09 ms per token,    17.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     580.20 ms /     3 runs   (  193.40 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1573.78 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     793.55 ms /    13 tokens (   61.04 ms per token,    16.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.74 ms /     1 runs   (  192.74 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =     989.59 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     964.62 ms /    18 tokens (   53.59 ms per token,    18.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.68 ms /     1 runs   (  194.68 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1162.45 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1228.32 ms /    22 tokens (   55.83 ms per token,    17.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     586.42 ms /     3 runs   (  195.47 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1820.70 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1000.61 ms /    17 tokens (   58.86 ms per token,    16.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.97 ms /     3 runs   (  192.66 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1584.45 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1137.68 ms /    22 tokens (   51.71 ms per token,    19.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.32 ms /     1 runs   (  194.32 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1335.24 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     759.83 ms /    14 tokens (   54.27 ms per token,    18.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     701.27 ms /     3 runs   (  233.76 ms per token,     4.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1467.72 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2003.52 ms /    15 tokens (  133.57 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     588.50 ms /     3 runs   (  196.17 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2600.70 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1252.03 ms /    24 tokens (   52.17 ms per token,    19.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.54 ms /     3 runs   (  193.18 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1837.86 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     954.88 ms /    18 tokens (   53.05 ms per token,    18.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.74 ms /     3 runs   (  188.91 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1527.50 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     895.91 ms /    15 tokens (   59.73 ms per token,    16.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.55 ms /     3 runs   (  191.52 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1476.53 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     961.87 ms /    17 tokens (   56.58 ms per token,    17.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.91 ms /     1 runs   (  193.91 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1158.98 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     976.21 ms /    17 tokens (   57.42 ms per token,    17.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.17 ms /     3 runs   (  188.72 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1548.22 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1430.57 ms /    28 tokens (   51.09 ms per token,    19.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     725.76 ms /     3 runs   (  241.92 ms per token,     4.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2163.17 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1956.35 ms /    17 tokens (  115.08 ms per token,     8.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.57 ms /     1 runs   (  196.57 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2157.20 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     952.45 ms /    18 tokens (   52.91 ms per token,    18.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.43 ms /     3 runs   (  190.48 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1529.68 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     762.83 ms /    14 tokens (   54.49 ms per token,    18.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.63 ms /     1 runs   (  188.63 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =     954.91 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1066.17 ms /    19 tokens (   56.11 ms per token,    17.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.28 ms /     1 runs   (  193.28 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1262.65 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1266.89 ms /    24 tokens (   52.79 ms per token,    18.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.96 ms /     1 runs   (  191.96 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1462.14 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1197.71 ms /    22 tokens (   54.44 ms per token,    18.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.27 ms /     1 runs   (  194.27 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1395.22 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1292.78 ms /    23 tokens (   56.21 ms per token,    17.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.04 ms /     1 runs   (  190.04 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1486.02 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1013.12 ms /    20 tokens (   50.66 ms per token,    19.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.54 ms /     1 runs   (  185.54 ms per token,     5.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1201.83 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2720.29 ms /    23 tokens (  118.27 ms per token,     8.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.18 ms /     1 runs   (  196.18 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2919.72 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     847.32 ms /    16 tokens (   52.96 ms per token,    18.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.74 ms /     1 runs   (  186.74 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1037.26 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     767.22 ms /    13 tokens (   59.02 ms per token,    16.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.32 ms /     1 runs   (  186.32 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =     956.68 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1027.55 ms /    19 tokens (   54.08 ms per token,    18.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.04 ms /     3 runs   (  191.35 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1607.54 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     708.99 ms /    14 tokens (   50.64 ms per token,    19.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.21 ms /     1 runs   (  194.21 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =     906.19 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1206.52 ms /    23 tokens (   52.46 ms per token,    19.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     561.68 ms /     3 runs   (  187.23 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1774.66 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1509.21 ms /    30 tokens (   50.31 ms per token,    19.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.76 ms /     1 runs   (  187.76 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1700.07 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     821.30 ms /    13 tokens (   63.18 ms per token,    15.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.38 ms /     1 runs   (  193.38 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1018.20 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1568.56 ms /    16 tokens (   98.04 ms per token,    10.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     744.06 ms /     3 runs   (  248.02 ms per token,     4.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2319.19 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1352.19 ms /    19 tokens (   71.17 ms per token,    14.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.80 ms /     1 runs   (  193.80 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1549.19 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     626.52 ms /    12 tokens (   52.21 ms per token,    19.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.23 ms /     1 runs   (  189.23 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =     818.89 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1064.56 ms /    20 tokens (   53.23 ms per token,    18.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.41 ms /     3 runs   (  191.14 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1643.73 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1045.77 ms /    19 tokens (   55.04 ms per token,    18.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     562.77 ms /     3 runs   (  187.59 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1614.62 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     743.06 ms /    14 tokens (   53.08 ms per token,    18.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.36 ms /     1 runs   (  191.36 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =     938.88 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     853.76 ms /    16 tokens (   53.36 ms per token,    18.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.31 ms /     1 runs   (  191.31 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1048.25 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     801.25 ms /    13 tokens (   61.63 ms per token,    16.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.09 ms /     1 runs   (  197.09 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1001.76 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     946.28 ms /    18 tokens (   52.57 ms per token,    19.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.44 ms /     1 runs   (  198.44 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1148.27 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1006.61 ms /    18 tokens (   55.92 ms per token,    17.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     726.77 ms /     3 runs   (  242.26 ms per token,     4.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1740.26 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1834.62 ms /    17 tokens (  107.92 ms per token,     9.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.61 ms /     1 runs   (  188.61 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    2026.44 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     646.36 ms /    12 tokens (   53.86 ms per token,    18.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.75 ms /     1 runs   (  186.75 ms per token,     5.35 tokens per second)\n",
            "llama_perf_context_print:       total time =     836.20 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     919.64 ms /    18 tokens (   51.09 ms per token,    19.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.13 ms /     3 runs   (  188.71 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1491.77 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1185.11 ms /    21 tokens (   56.43 ms per token,    17.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     556.68 ms /     3 runs   (  185.56 ms per token,     5.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1748.77 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1302.56 ms /    24 tokens (   54.27 ms per token,    18.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.81 ms /     1 runs   (  200.81 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    1506.86 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     810.45 ms /    15 tokens (   54.03 ms per token,    18.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.53 ms /     1 runs   (  195.53 ms per token,     5.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1009.56 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     915.87 ms /    17 tokens (   53.87 ms per token,    18.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.88 ms /     1 runs   (  198.88 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1118.04 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     924.01 ms /    18 tokens (   51.33 ms per token,    19.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     556.12 ms /     3 runs   (  185.37 ms per token,     5.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    1485.92 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2599.20 ms /    18 tokens (  144.40 ms per token,     6.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.42 ms /     3 runs   (  192.14 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    3181.75 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1044.53 ms /    20 tokens (   52.23 ms per token,    19.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.11 ms /     3 runs   (  188.70 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1616.44 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     722.48 ms /    14 tokens (   51.61 ms per token,    19.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     567.29 ms /     3 runs   (  189.10 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1295.70 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     926.38 ms /    18 tokens (   51.47 ms per token,    19.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.67 ms /     1 runs   (  189.67 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1119.19 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 61 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     811.50 ms /    13 tokens (   62.42 ms per token,    16.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.25 ms /     1 runs   (  193.25 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1009.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     866.42 ms /    15 tokens (   57.76 ms per token,    17.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     560.68 ms /     3 runs   (  186.89 ms per token,     5.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1433.04 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     886.87 ms /    15 tokens (   59.12 ms per token,    16.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.89 ms /     1 runs   (  189.89 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1079.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     750.28 ms /    14 tokens (   53.59 ms per token,    18.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.85 ms /     1 runs   (  193.85 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =     947.44 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     721.12 ms /    11 tokens (   65.56 ms per token,    15.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     259.67 ms /     1 runs   (  259.67 ms per token,     3.85 tokens per second)\n",
            "llama_perf_context_print:       total time =     984.12 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2005.10 ms /    13 tokens (  154.24 ms per token,     6.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.41 ms /     1 runs   (  186.41 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    2194.72 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     973.13 ms /    18 tokens (   54.06 ms per token,    18.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     611.86 ms /     3 runs   (  203.95 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    1591.25 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1177.11 ms /    21 tokens (   56.05 ms per token,    17.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     591.24 ms /     3 runs   (  197.08 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1774.50 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     969.00 ms /    15 tokens (   64.60 ms per token,    15.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     580.94 ms /     3 runs   (  193.65 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1556.23 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 59 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     485.02 ms /     8 tokens (   60.63 ms per token,    16.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.61 ms /     1 runs   (  197.61 ms per token,     5.06 tokens per second)\n",
            "llama_perf_context_print:       total time =     685.73 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     785.89 ms /    13 tokens (   60.45 ms per token,    16.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.70 ms /     1 runs   (  201.70 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =     990.78 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     853.92 ms /    16 tokens (   53.37 ms per token,    18.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.84 ms /     1 runs   (  197.84 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1054.88 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1270.73 ms /    24 tokens (   52.95 ms per token,    18.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.27 ms /     1 runs   (  191.27 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1465.29 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1014.47 ms /    13 tokens (   78.04 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     251.32 ms /     1 runs   (  251.32 ms per token,     3.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    1269.41 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1730.94 ms /    12 tokens (  144.25 ms per token,     6.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.96 ms /     1 runs   (  192.96 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1927.14 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1157.82 ms /    21 tokens (   55.13 ms per token,    18.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.41 ms /     1 runs   (  189.41 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1350.59 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     841.80 ms /    16 tokens (   52.61 ms per token,    19.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     570.29 ms /     3 runs   (  190.10 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1417.90 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     758.33 ms /    14 tokens (   54.17 ms per token,    18.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.59 ms /     1 runs   (  189.59 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =     951.04 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     735.06 ms /    14 tokens (   52.50 ms per token,    19.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.85 ms /     1 runs   (  200.85 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =     939.18 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     953.93 ms /    18 tokens (   53.00 ms per token,    18.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     569.21 ms /     3 runs   (  189.74 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1529.13 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     784.23 ms /    13 tokens (   60.33 ms per token,    16.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.39 ms /     1 runs   (  186.39 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =     973.84 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     880.87 ms /    15 tokens (   58.72 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.51 ms /     1 runs   (  190.51 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1074.66 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     953.95 ms /    17 tokens (   56.11 ms per token,    17.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     613.94 ms /     3 runs   (  204.65 ms per token,     4.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    1574.10 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2166.39 ms /    16 tokens (  135.40 ms per token,     7.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     561.31 ms /     3 runs   (  187.10 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2733.77 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     967.28 ms /    18 tokens (   53.74 ms per token,    18.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     575.77 ms /     3 runs   (  191.92 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1549.00 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     955.01 ms /    18 tokens (   53.06 ms per token,    18.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.23 ms /     3 runs   (  191.41 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1535.29 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     868.28 ms /    16 tokens (   54.27 ms per token,    18.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.57 ms /     3 runs   (  191.19 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1447.91 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     634.52 ms /    12 tokens (   52.88 ms per token,    18.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     580.63 ms /     3 runs   (  193.54 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1220.98 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 58 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     552.95 ms /    10 tokens (   55.30 ms per token,    18.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     565.88 ms /     3 runs   (  188.63 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1124.76 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     847.27 ms /    16 tokens (   52.95 ms per token,    18.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.28 ms /     1 runs   (  194.28 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1044.71 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     774.36 ms /    13 tokens (   59.57 ms per token,    16.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.15 ms /     3 runs   (  192.38 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1358.12 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2091.05 ms /    13 tokens (  160.85 ms per token,     6.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     249.48 ms /     1 runs   (  249.48 ms per token,     4.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    2345.75 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1191.38 ms /    21 tokens (   56.73 ms per token,    17.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     600.80 ms /     3 runs   (  200.27 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1799.16 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     877.71 ms /    15 tokens (   58.51 ms per token,    17.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.33 ms /     1 runs   (  192.33 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1073.35 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     854.36 ms /    16 tokens (   53.40 ms per token,    18.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.99 ms /     1 runs   (  188.99 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1046.52 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 33 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1762.45 ms /    33 tokens (   53.41 ms per token,    18.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.79 ms /     1 runs   (  188.79 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1954.53 ms /    34 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     660.82 ms /    12 tokens (   55.07 ms per token,    18.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.39 ms /     1 runs   (  190.39 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     854.23 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     753.32 ms /    14 tokens (   53.81 ms per token,    18.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     558.64 ms /     3 runs   (  186.21 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1317.93 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     667.41 ms /    12 tokens (   55.62 ms per token,    17.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.24 ms /     1 runs   (  187.24 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =     857.81 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1095.76 ms /    20 tokens (   54.79 ms per token,    18.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     724.35 ms /     3 runs   (  241.45 ms per token,     4.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1826.64 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1730.40 ms /    16 tokens (  108.15 ms per token,     9.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.47 ms /     1 runs   (  200.47 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1939.79 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     851.01 ms /    15 tokens (   56.73 ms per token,    17.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.47 ms /     3 runs   (  190.82 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1429.57 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     955.62 ms /    18 tokens (   53.09 ms per token,    18.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     578.75 ms /     3 runs   (  192.92 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1540.39 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1080.23 ms /    19 tokens (   56.85 ms per token,    17.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     562.58 ms /     3 runs   (  187.53 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1648.64 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     967.36 ms /    17 tokens (   56.90 ms per token,    17.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.62 ms /     1 runs   (  190.62 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1161.18 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     676.07 ms /    12 tokens (   56.34 ms per token,    17.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.11 ms /     1 runs   (  191.11 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =     870.36 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     888.18 ms /    16 tokens (   55.51 ms per token,    18.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.81 ms /     1 runs   (  193.81 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1085.19 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1263.92 ms /    24 tokens (   52.66 ms per token,    18.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     584.32 ms /     3 runs   (  194.77 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1854.33 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2292.93 ms /    16 tokens (  143.31 ms per token,     6.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     581.41 ms /     3 runs   (  193.80 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2881.00 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     964.85 ms /    18 tokens (   53.60 ms per token,    18.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     567.40 ms /     3 runs   (  189.13 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1538.25 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     766.96 ms /    14 tokens (   54.78 ms per token,    18.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.77 ms /     1 runs   (  189.77 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =     959.98 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     983.43 ms /    17 tokens (   57.85 ms per token,    17.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.40 ms /     1 runs   (  191.40 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1178.14 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1282.79 ms /    23 tokens (   55.77 ms per token,    17.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.65 ms /     1 runs   (  199.65 ms per token,     5.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    1485.59 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     842.61 ms /    16 tokens (   52.66 ms per token,    18.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.07 ms /     1 runs   (  192.07 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1037.92 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     897.56 ms /    16 tokens (   56.10 ms per token,    17.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.18 ms /     1 runs   (  189.18 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1090.21 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     844.28 ms /    16 tokens (   52.77 ms per token,    18.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.43 ms /     1 runs   (  189.43 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1037.64 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1254.66 ms /    22 tokens (   57.03 ms per token,    17.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     238.56 ms /     1 runs   (  238.56 ms per token,     4.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1496.59 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2128.75 ms /    17 tokens (  125.22 ms per token,     7.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.49 ms /     1 runs   (  192.49 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2327.06 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1569.25 ms /    29 tokens (   54.11 ms per token,    18.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.54 ms /     3 runs   (  192.51 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2152.89 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     904.47 ms /    15 tokens (   60.30 ms per token,    16.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.93 ms /     3 runs   (  191.31 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1484.79 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1061.80 ms /    19 tokens (   55.88 ms per token,    17.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     567.00 ms /     3 runs   (  189.00 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1634.69 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     666.65 ms /    12 tokens (   55.55 ms per token,    18.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.97 ms /     1 runs   (  191.97 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =     861.78 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     857.53 ms /    16 tokens (   53.60 ms per token,    18.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.50 ms /     1 runs   (  192.50 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1053.35 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1077.68 ms /    19 tokens (   56.72 ms per token,    17.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     589.75 ms /     3 runs   (  196.58 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1673.39 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1444.72 ms /    13 tokens (  111.13 ms per token,     9.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     255.23 ms /     1 runs   (  255.23 ms per token,     3.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    1703.43 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1574.76 ms /    17 tokens (   92.63 ms per token,    10.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     578.85 ms /     3 runs   (  192.95 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2161.36 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     974.53 ms /    18 tokens (   54.14 ms per token,    18.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     561.61 ms /     3 runs   (  187.20 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1541.99 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     949.60 ms /    18 tokens (   52.76 ms per token,    18.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.50 ms /     3 runs   (  190.50 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1526.94 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     763.32 ms /    14 tokens (   54.52 ms per token,    18.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.44 ms /     3 runs   (  189.48 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1337.61 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     841.18 ms /    16 tokens (   52.57 ms per token,    19.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.25 ms /     1 runs   (  190.25 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1034.58 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     880.35 ms /    15 tokens (   58.69 ms per token,    17.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.13 ms /     3 runs   (  188.71 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1452.48 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     981.58 ms /    17 tokens (   57.74 ms per token,    17.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     581.32 ms /     3 runs   (  193.77 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1569.11 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2469.32 ms /    17 tokens (  145.25 ms per token,     6.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.81 ms /     1 runs   (  203.81 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    2676.34 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     757.91 ms /    11 tokens (   68.90 ms per token,    14.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.41 ms /     1 runs   (  194.41 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =     955.70 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1298.15 ms /    23 tokens (   56.44 ms per token,    17.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.23 ms /     1 runs   (  191.23 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1492.56 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     888.90 ms /    15 tokens (   59.26 ms per token,    16.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.04 ms /     1 runs   (  197.04 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1089.22 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1248.58 ms /    24 tokens (   52.02 ms per token,    19.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.55 ms /     1 runs   (  190.55 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1442.33 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     785.43 ms /    14 tokens (   56.10 ms per token,    17.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.08 ms /     1 runs   (  194.08 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =     982.63 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     814.58 ms /    13 tokens (   62.66 ms per token,    15.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     575.15 ms /     3 runs   (  191.72 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1395.71 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1252.56 ms /    24 tokens (   52.19 ms per token,    19.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.32 ms /     3 runs   (  190.44 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1829.75 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1106.00 ms /    16 tokens (   69.13 ms per token,    14.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     260.51 ms /     1 runs   (  260.51 ms per token,     3.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    1370.01 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2299.94 ms /    23 tokens (  100.00 ms per token,    10.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.59 ms /     1 runs   (  193.59 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2501.03 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     993.23 ms /    17 tokens (   58.43 ms per token,    17.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     585.18 ms /     3 runs   (  195.06 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1584.33 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     790.11 ms /    13 tokens (   60.78 ms per token,    16.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.88 ms /     1 runs   (  198.88 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =     992.64 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     678.90 ms /    12 tokens (   56.58 ms per token,    17.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.01 ms /     1 runs   (  191.01 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =     873.19 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     998.99 ms /    17 tokens (   58.76 ms per token,    17.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     580.69 ms /     3 runs   (  193.56 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1585.69 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1096.03 ms /    19 tokens (   57.69 ms per token,    17.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.11 ms /     1 runs   (  194.11 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1293.39 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1089.57 ms /    19 tokens (   57.35 ms per token,    17.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.61 ms /     3 runs   (  193.20 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1675.17 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1189.83 ms /    22 tokens (   54.08 ms per token,    18.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     253.27 ms /     1 runs   (  253.27 ms per token,     3.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    1447.52 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2408.05 ms /    19 tokens (  126.74 ms per token,     7.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.45 ms /     1 runs   (  201.45 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    2615.60 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     749.00 ms /    14 tokens (   53.50 ms per token,    18.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.41 ms /     1 runs   (  197.41 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =     949.54 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     935.75 ms /    18 tokens (   51.99 ms per token,    19.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.47 ms /     1 runs   (  198.47 ms per token,     5.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1137.43 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     862.86 ms /    16 tokens (   53.93 ms per token,    18.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.14 ms /     1 runs   (  196.14 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1062.29 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     865.25 ms /    16 tokens (   54.08 ms per token,    18.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.46 ms /     1 runs   (  195.46 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1063.91 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1024.83 ms /    18 tokens (   56.93 ms per token,    17.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.15 ms /     3 runs   (  191.38 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1605.29 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     860.05 ms /    16 tokens (   53.75 ms per token,    18.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.21 ms /     1 runs   (  191.21 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1054.66 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1279.61 ms /    23 tokens (   55.64 ms per token,    17.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.34 ms /     1 runs   (  199.34 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1482.16 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     874.79 ms /    16 tokens (   54.67 ms per token,    18.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     231.84 ms /     1 runs   (  231.84 ms per token,     4.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1110.27 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2233.68 ms /    16 tokens (  139.60 ms per token,     7.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.99 ms /     1 runs   (  193.99 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2433.29 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1186.75 ms /    21 tokens (   56.51 ms per token,    17.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.50 ms /     1 runs   (  192.50 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1382.51 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     776.96 ms /    14 tokens (   55.50 ms per token,    18.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.68 ms /     1 runs   (  194.68 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =     974.89 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     805.10 ms /    13 tokens (   61.93 ms per token,    16.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.45 ms /     1 runs   (  196.45 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1004.69 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     804.97 ms /    13 tokens (   61.92 ms per token,    16.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     588.56 ms /     3 runs   (  196.19 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1399.49 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1280.25 ms /    23 tokens (   55.66 ms per token,    17.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.67 ms /     3 runs   (  190.89 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1861.11 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     912.68 ms /    15 tokens (   60.85 ms per token,    16.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.99 ms /     1 runs   (  197.99 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1114.11 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1285.60 ms /    23 tokens (   55.90 ms per token,    17.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.49 ms /     1 runs   (  193.49 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1482.36 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     954.59 ms /    12 tokens (   79.55 ms per token,    12.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     248.23 ms /     1 runs   (  248.23 ms per token,     4.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1206.32 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 67 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     495.43 ms /     2 runs   (  247.71 ms per token,     4.04 tokens per second)\n",
            "llama_perf_context_print:       total time =     500.10 ms /     3 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1544.10 ms /    11 tokens (  140.37 ms per token,     7.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.76 ms /     1 runs   (  198.76 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1747.33 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1500.15 ms /    27 tokens (   55.56 ms per token,    18.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.06 ms /     3 runs   (  192.35 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2083.30 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     909.36 ms /    15 tokens (   60.62 ms per token,    16.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     583.91 ms /     3 runs   (  194.64 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1499.25 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     762.66 ms /    14 tokens (   54.48 ms per token,    18.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     586.21 ms /     3 runs   (  195.40 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1354.73 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1249.16 ms /    24 tokens (   52.05 ms per token,    19.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.72 ms /     1 runs   (  192.72 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1445.13 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     821.88 ms /    13 tokens (   63.22 ms per token,    15.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.04 ms /     3 runs   (  193.01 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1406.85 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     896.09 ms /    15 tokens (   59.74 ms per token,    16.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.63 ms /     1 runs   (  194.63 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1094.06 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1164.86 ms /    16 tokens (   72.80 ms per token,    13.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     283.63 ms /     1 runs   (  283.63 ms per token,     3.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    1451.93 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2038.80 ms /    17 tokens (  119.93 ms per token,     8.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     581.00 ms /     3 runs   (  193.67 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2625.74 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     762.65 ms /    14 tokens (   54.48 ms per token,    18.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.21 ms /     1 runs   (  201.21 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:       total time =     967.09 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     675.61 ms /    12 tokens (   56.30 ms per token,    17.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =     583.80 ms /     3 runs   (  194.60 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1265.90 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1064.08 ms /    20 tokens (   53.20 ms per token,    18.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.89 ms /     3 runs   (  193.30 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1650.19 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     974.67 ms /    17 tokens (   57.33 ms per token,    17.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.84 ms /     1 runs   (  197.84 ms per token,     5.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1177.04 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     650.00 ms /    12 tokens (   54.17 ms per token,    18.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.28 ms /     1 runs   (  191.28 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =     844.46 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     902.35 ms /    15 tokens (   60.16 ms per token,    16.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.03 ms /     1 runs   (  195.03 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1101.18 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     868.43 ms /    15 tokens (   57.90 ms per token,    17.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.25 ms /     1 runs   (  199.25 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1072.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2540.07 ms /    29 tokens (   87.59 ms per token,    11.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     741.72 ms /     3 runs   (  247.24 ms per token,     4.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    3288.46 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     766.33 ms /    13 tokens (   58.95 ms per token,    16.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.11 ms /     1 runs   (  199.11 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =     968.74 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     960.79 ms /    17 tokens (   56.52 ms per token,    17.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.36 ms /     1 runs   (  196.36 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1166.50 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     775.07 ms /    13 tokens (   59.62 ms per token,    16.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     202.12 ms /     1 runs   (  202.12 ms per token,     4.95 tokens per second)\n",
            "llama_perf_context_print:       total time =     980.38 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     778.30 ms /    13 tokens (   59.87 ms per token,    16.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     204.09 ms /     1 runs   (  204.09 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:       total time =     986.52 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1621.58 ms /    29 tokens (   55.92 ms per token,    17.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.26 ms /     3 runs   (  192.42 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2205.06 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1171.41 ms /    21 tokens (   55.78 ms per token,    17.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.89 ms /     3 runs   (  191.63 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1752.43 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     971.20 ms /    18 tokens (   53.96 ms per token,    18.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     583.18 ms /     3 runs   (  194.39 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1560.59 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2110.64 ms /    16 tokens (  131.91 ms per token,     7.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     678.43 ms /     3 runs   (  226.14 ms per token,     4.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    2795.91 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     968.34 ms /    17 tokens (   56.96 ms per token,    17.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.20 ms /     1 runs   (  191.20 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1162.90 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     865.92 ms /    16 tokens (   54.12 ms per token,    18.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.72 ms /     3 runs   (  188.91 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1438.69 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     938.21 ms /    15 tokens (   62.55 ms per token,    15.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     578.59 ms /     3 runs   (  192.86 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1522.86 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     769.32 ms /    13 tokens (   59.18 ms per token,    16.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.13 ms /     1 runs   (  197.13 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =     969.68 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     941.22 ms /    18 tokens (   52.29 ms per token,    19.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.74 ms /     1 runs   (  198.74 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1143.33 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1249.85 ms /    23 tokens (   54.34 ms per token,    18.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.20 ms /     1 runs   (  193.20 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1446.37 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     883.61 ms /    15 tokens (   58.91 ms per token,    16.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.40 ms /     1 runs   (  193.40 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1080.22 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1021.00 ms /    18 tokens (   56.72 ms per token,    17.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     732.16 ms /     3 runs   (  244.05 ms per token,     4.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1759.55 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1741.74 ms /    15 tokens (  116.12 ms per token,     8.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.56 ms /     1 runs   (  201.56 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    1951.57 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     878.06 ms /    15 tokens (   58.54 ms per token,    17.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.28 ms /     1 runs   (  196.28 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1083.08 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1048.34 ms /    20 tokens (   52.42 ms per token,    19.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.83 ms /     3 runs   (  190.61 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1626.12 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     759.78 ms /    14 tokens (   54.27 ms per token,    18.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.46 ms /     1 runs   (  192.46 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =     955.56 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     668.72 ms /    12 tokens (   55.73 ms per token,    17.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.56 ms /     1 runs   (  188.56 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =     860.30 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     722.91 ms /    11 tokens (   65.72 ms per token,    15.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.35 ms /     1 runs   (  194.35 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =     920.57 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1438.72 ms /    27 tokens (   53.29 ms per token,    18.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     583.05 ms /     3 runs   (  194.35 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2028.54 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     785.01 ms /    13 tokens (   60.39 ms per token,    16.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.62 ms /     1 runs   (  189.62 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =     977.82 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     476.86 ms /     8 tokens (   59.61 ms per token,    16.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.68 ms /     1 runs   (  192.68 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =     672.94 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1949.29 ms /    18 tokens (  108.29 ms per token,     9.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     257.75 ms /     1 runs   (  257.75 ms per token,     3.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2212.16 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1159.32 ms /    16 tokens (   72.46 ms per token,    13.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     570.62 ms /     3 runs   (  190.21 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1736.05 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1510.66 ms /    30 tokens (   50.36 ms per token,    19.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.53 ms /     1 runs   (  191.53 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1705.37 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     948.07 ms /    18 tokens (   52.67 ms per token,    18.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     559.36 ms /     3 runs   (  186.45 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1513.32 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1074.67 ms /    19 tokens (   56.56 ms per token,    17.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.94 ms /     1 runs   (  194.94 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1272.79 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     653.54 ms /    12 tokens (   54.46 ms per token,    18.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.42 ms /     1 runs   (  190.42 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     847.17 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     580.81 ms /    10 tokens (   58.08 ms per token,    17.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.26 ms /     3 runs   (  191.09 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1159.97 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     845.00 ms /    16 tokens (   52.81 ms per token,    18.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     604.55 ms /     3 runs   (  201.52 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    1455.87 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1125.16 ms /    17 tokens (   66.19 ms per token,    15.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     722.47 ms /     3 runs   (  240.82 ms per token,     4.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1854.37 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1808.19 ms /    17 tokens (  106.36 ms per token,     9.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     581.36 ms /     3 runs   (  193.78 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2395.40 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     987.06 ms /    17 tokens (   58.06 ms per token,    17.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.04 ms /     3 runs   (  192.01 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1569.06 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1057.88 ms /    20 tokens (   52.89 ms per token,    18.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.51 ms /     1 runs   (  189.51 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1250.77 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     884.92 ms /    15 tokens (   58.99 ms per token,    16.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.48 ms /     3 runs   (  189.49 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1459.41 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     880.94 ms /    15 tokens (   58.73 ms per token,    17.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.90 ms /     1 runs   (  193.90 ms per token,     5.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1078.00 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     762.50 ms /    14 tokens (   54.46 ms per token,    18.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.63 ms /     1 runs   (  190.63 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     956.36 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     789.87 ms /    13 tokens (   60.76 ms per token,    16.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.61 ms /     1 runs   (  190.61 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     983.90 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     764.60 ms /    14 tokens (   54.61 ms per token,    18.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     569.42 ms /     3 runs   (  189.81 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1339.83 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2070.59 ms /    16 tokens (  129.41 ms per token,     7.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     659.38 ms /     3 runs   (  219.79 ms per token,     4.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    2737.94 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     863.50 ms /    16 tokens (   53.97 ms per token,    18.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.39 ms /     3 runs   (  190.46 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1440.82 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1013.30 ms /    17 tokens (   59.61 ms per token,    16.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.06 ms /     3 runs   (  191.02 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1592.27 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     875.06 ms /    15 tokens (   58.34 ms per token,    17.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.38 ms /     3 runs   (  188.79 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1448.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1633.82 ms /    32 tokens (   51.06 ms per token,    19.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.47 ms /     1 runs   (  190.47 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1827.46 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1085.61 ms /    19 tokens (   57.14 ms per token,    17.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     191.10 ms /     1 runs   (  191.10 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.90 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     945.82 ms /    18 tokens (   52.55 ms per token,    19.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.98 ms /     3 runs   (  192.33 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1528.86 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1059.61 ms /    13 tokens (   81.51 ms per token,    12.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     246.05 ms /     1 runs   (  246.05 ms per token,     4.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1309.02 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1868.28 ms /    13 tokens (  143.71 ms per token,     6.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     575.05 ms /     3 runs   (  191.68 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2449.30 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1716.79 ms /    32 tokens (   53.65 ms per token,    18.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.18 ms /     1 runs   (  194.18 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1914.26 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     974.35 ms /    18 tokens (   54.13 ms per token,    18.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     585.01 ms /     3 runs   (  195.00 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1565.71 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     697.28 ms /    11 tokens (   63.39 ms per token,    15.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.92 ms /     1 runs   (  196.92 ms per token,     5.08 tokens per second)\n",
            "llama_perf_context_print:       total time =     897.45 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     939.56 ms /    18 tokens (   52.20 ms per token,    19.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.89 ms /     3 runs   (  191.30 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1521.28 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     789.07 ms /    13 tokens (   60.70 ms per token,    16.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.13 ms /     1 runs   (  190.13 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =     982.45 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1253.77 ms /    24 tokens (   52.24 ms per token,    19.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.56 ms /     1 runs   (  190.56 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1447.49 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2220.08 ms /    22 tokens (  100.91 ms per token,     9.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     246.28 ms /     1 runs   (  246.28 ms per token,     4.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    2475.66 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     898.86 ms /    12 tokens (   74.90 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.78 ms /     1 runs   (  192.78 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1094.87 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     650.61 ms /    12 tokens (   54.22 ms per token,    18.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.61 ms /     1 runs   (  198.61 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =     852.40 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     870.81 ms /    15 tokens (   58.05 ms per token,    17.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.31 ms /     1 runs   (  194.31 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1068.42 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1247.77 ms /    24 tokens (   51.99 ms per token,    19.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.51 ms /     1 runs   (  188.51 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1439.50 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     957.71 ms /    17 tokens (   56.34 ms per token,    17.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.80 ms /     1 runs   (  186.80 ms per token,     5.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1147.76 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     692.18 ms /    12 tokens (   57.68 ms per token,    17.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.29 ms /     1 runs   (  189.29 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =     884.74 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     982.05 ms /    17 tokens (   57.77 ms per token,    17.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.66 ms /     3 runs   (  190.89 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1560.74 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1027.14 ms /    20 tokens (   51.36 ms per token,    19.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.82 ms /     1 runs   (  190.82 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1221.13 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     773.00 ms /    13 tokens (   59.46 ms per token,    16.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.38 ms /     1 runs   (  200.38 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =     976.72 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2063.95 ms /    16 tokens (  129.00 ms per token,     7.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     250.74 ms /     1 runs   (  250.74 ms per token,     3.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2318.18 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 64 prefix-match hit, remaining 7 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     561.05 ms /     7 tokens (   80.15 ms per token,    12.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     200.50 ms /     1 runs   (  200.50 ms per token,     4.99 tokens per second)\n",
            "llama_perf_context_print:       total time =     764.77 ms /     8 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     827.49 ms /    16 tokens (   51.72 ms per token,    19.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.98 ms /     3 runs   (  191.66 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1408.40 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     943.35 ms /    18 tokens (   52.41 ms per token,    19.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     562.24 ms /     3 runs   (  187.41 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1511.86 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     762.87 ms /    14 tokens (   54.49 ms per token,    18.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.02 ms /     1 runs   (  190.02 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =     956.08 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     657.01 ms /    12 tokens (   54.75 ms per token,    18.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.27 ms /     3 runs   (  189.42 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1231.56 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1221.44 ms /    23 tokens (   53.11 ms per token,    18.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.54 ms /     1 runs   (  186.54 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1411.32 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1000.70 ms /    17 tokens (   58.86 ms per token,    16.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     560.06 ms /     3 runs   (  186.69 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1566.96 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     948.28 ms /    17 tokens (   55.78 ms per token,    17.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     670.47 ms /     3 runs   (  223.49 ms per token,     4.47 tokens per second)\n",
            "llama_perf_context_print:       total time =    1625.05 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2026.34 ms /    14 tokens (  144.74 ms per token,     6.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.01 ms /     1 runs   (  187.01 ms per token,     5.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    2220.34 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     969.76 ms /    17 tokens (   57.04 ms per token,    17.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     184.57 ms /     1 runs   (  184.57 ms per token,     5.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    1157.48 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     791.67 ms /    13 tokens (   60.90 ms per token,    16.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.10 ms /     1 runs   (  187.10 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =     981.94 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     840.24 ms /    16 tokens (   52.51 ms per token,    19.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.15 ms /     1 runs   (  185.15 ms per token,     5.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    1028.51 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     756.44 ms /    14 tokens (   54.03 ms per token,    18.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.11 ms /     3 runs   (  188.70 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1328.43 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     743.79 ms /    14 tokens (   53.13 ms per token,    18.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.34 ms /     1 runs   (  188.34 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:       total time =     935.21 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     856.17 ms /    16 tokens (   53.51 ms per token,    18.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     573.56 ms /     3 runs   (  191.19 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1435.65 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     965.65 ms /    17 tokens (   56.80 ms per token,    17.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     557.64 ms /     3 runs   (  185.88 ms per token,     5.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1529.06 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     886.03 ms /    15 tokens (   59.07 ms per token,    16.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     690.42 ms /     3 runs   (  230.14 ms per token,     4.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1583.27 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2153.02 ms /    20 tokens (  107.65 ms per token,     9.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.62 ms /     3 runs   (  191.54 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2733.91 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1090.66 ms /    19 tokens (   57.40 ms per token,    17.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     575.75 ms /     3 runs   (  191.91 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1673.75 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1067.23 ms /    20 tokens (   53.36 ms per token,    18.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     579.36 ms /     3 runs   (  193.12 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1652.60 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     690.09 ms /    11 tokens (   62.74 ms per token,    15.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.43 ms /     1 runs   (  194.43 ms per token,     5.14 tokens per second)\n",
            "llama_perf_context_print:       total time =     888.04 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1071.44 ms /    19 tokens (   56.39 ms per token,    17.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.24 ms /     1 runs   (  190.24 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1264.95 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     666.25 ms /    12 tokens (   55.52 ms per token,    18.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.90 ms /     1 runs   (  198.90 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =     868.33 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1144.64 ms /    22 tokens (   52.03 ms per token,    19.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.98 ms /     1 runs   (  190.98 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1338.82 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     861.46 ms /    16 tokens (   53.84 ms per token,    18.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     644.30 ms /     3 runs   (  214.77 ms per token,     4.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    1512.08 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2117.81 ms /    16 tokens (  132.36 ms per token,     7.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.91 ms /     1 runs   (  198.91 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2324.01 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     799.03 ms /    14 tokens (   57.07 ms per token,    17.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     576.84 ms /     3 runs   (  192.28 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1381.81 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1053.81 ms /    19 tokens (   55.46 ms per token,    18.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.61 ms /     3 runs   (  190.87 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1632.39 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     854.19 ms /    15 tokens (   56.95 ms per token,    17.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     584.38 ms /     3 runs   (  194.79 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1444.63 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     867.32 ms /    16 tokens (   54.21 ms per token,    18.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     565.33 ms /     3 runs   (  188.44 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1438.58 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1310.93 ms /    23 tokens (   57.00 ms per token,    17.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.62 ms /     1 runs   (  192.62 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1506.87 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     654.04 ms /    12 tokens (   54.50 ms per token,    18.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.67 ms /     1 runs   (  203.67 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =     860.88 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     756.45 ms /    13 tokens (   58.19 ms per token,    17.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.23 ms /     1 runs   (  195.23 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =     954.87 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2134.24 ms /    15 tokens (  142.28 ms per token,     7.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     629.94 ms /     3 runs   (  209.98 ms per token,     4.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    2770.35 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     939.99 ms /    18 tokens (   52.22 ms per token,    19.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.15 ms /     1 runs   (  187.15 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1130.27 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1250.07 ms /    23 tokens (   54.35 ms per token,    18.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.22 ms /     1 runs   (  188.22 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1441.63 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1063.43 ms /    19 tokens (   55.97 ms per token,    17.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     559.71 ms /     3 runs   (  186.57 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1629.02 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     803.07 ms /    13 tokens (   61.77 ms per token,    16.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     565.73 ms /     3 runs   (  188.58 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1374.59 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     922.86 ms /    18 tokens (   51.27 ms per token,    19.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     559.14 ms /     3 runs   (  186.38 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1487.78 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 61 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     922.46 ms /    18 tokens (   51.25 ms per token,    19.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.98 ms /     1 runs   (  187.98 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1113.70 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1203.53 ms /    24 tokens (   50.15 ms per token,    19.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     201.46 ms /     1 runs   (  201.46 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    1408.76 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2462.67 ms /    18 tokens (  136.82 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     565.68 ms /     3 runs   (  188.56 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    3034.20 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     881.78 ms /    15 tokens (   58.79 ms per token,    17.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     571.88 ms /     3 runs   (  190.63 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1459.66 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1052.32 ms /    20 tokens (   52.62 ms per token,    19.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.82 ms /     1 runs   (  190.82 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1248.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     797.61 ms /    13 tokens (   61.35 ms per token,    16.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.50 ms /     1 runs   (  190.50 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =     991.22 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     687.08 ms /    12 tokens (   57.26 ms per token,    17.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.64 ms /     1 runs   (  187.64 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =     877.88 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1212.33 ms /    24 tokens (   50.51 ms per token,    19.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     205.70 ms /     1 runs   (  205.70 ms per token,     4.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    1421.21 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     959.92 ms /    18 tokens (   53.33 ms per token,    18.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.85 ms /     3 runs   (  191.62 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1541.28 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     853.66 ms /    16 tokens (   53.35 ms per token,    18.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     203.76 ms /     1 runs   (  203.76 ms per token,     4.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    1060.66 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1960.29 ms /    17 tokens (  115.31 ms per token,     8.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     725.41 ms /     3 runs   (  241.80 ms per token,     4.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2696.79 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1168.08 ms /    21 tokens (   55.62 ms per token,    17.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     197.18 ms /     1 runs   (  197.18 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1368.53 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     935.43 ms /    18 tokens (   51.97 ms per token,    19.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     196.34 ms /     1 runs   (  196.34 ms per token,     5.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1134.96 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1033.56 ms /    20 tokens (   51.68 ms per token,    19.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.77 ms /     1 runs   (  188.77 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1225.49 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     929.01 ms /    18 tokens (   51.61 ms per token,    19.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     564.05 ms /     3 runs   (  188.02 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1499.07 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1117.45 ms /    22 tokens (   50.79 ms per token,    19.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.89 ms /     1 runs   (  192.89 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1313.50 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     768.47 ms /    14 tokens (   54.89 ms per token,    18.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     570.82 ms /     3 runs   (  190.27 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    1345.26 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     884.50 ms /    15 tokens (   58.97 ms per token,    16.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     561.92 ms /     3 runs   (  187.31 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1452.38 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1193.86 ms /    15 tokens (   79.59 ms per token,    12.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     719.32 ms /     3 runs   (  239.77 ms per token,     4.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1919.72 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1289.04 ms /    12 tokens (  107.42 ms per token,     9.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     198.74 ms /     1 runs   (  198.74 ms per token,     5.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1492.66 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     964.51 ms /    17 tokens (   56.74 ms per token,    17.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     199.16 ms /     1 runs   (  199.16 ms per token,     5.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1170.38 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1043.53 ms /    20 tokens (   52.18 ms per token,    19.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.24 ms /     1 runs   (  192.24 ms per token,     5.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1239.10 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1210.02 ms /    21 tokens (   57.62 ms per token,    17.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.81 ms /     3 runs   (  190.94 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1788.84 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1215.77 ms /    24 tokens (   50.66 ms per token,    19.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.70 ms /     1 runs   (  189.70 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1408.79 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1090.16 ms /    20 tokens (   54.51 ms per token,    18.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.91 ms /     1 runs   (  185.91 ms per token,     5.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.10 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     937.78 ms /    18 tokens (   52.10 ms per token,    19.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.54 ms /     3 runs   (  188.85 ms per token,     5.30 tokens per second)\n",
            "llama_perf_context_print:       total time =    1510.43 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1295.81 ms /    23 tokens (   56.34 ms per token,    17.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     247.49 ms /     1 runs   (  247.49 ms per token,     4.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1547.68 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2234.99 ms /    17 tokens (  131.47 ms per token,     7.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.99 ms /     1 runs   (  192.99 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2431.21 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     973.65 ms /    17 tokens (   57.27 ms per token,    17.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.57 ms /     3 runs   (  189.52 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1548.16 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     882.63 ms /    15 tokens (   58.84 ms per token,    16.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.71 ms /     1 runs   (  186.71 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =    1072.53 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     803.13 ms /    13 tokens (   61.78 ms per token,    16.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.34 ms /     1 runs   (  187.34 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =     993.59 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1055.68 ms /    20 tokens (   52.78 ms per token,    18.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.26 ms /     1 runs   (  189.26 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    1248.15 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     876.08 ms /    15 tokens (   58.41 ms per token,    17.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.12 ms /     1 runs   (  188.12 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1067.38 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     963.45 ms /    18 tokens (   53.53 ms per token,    18.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     564.97 ms /     3 runs   (  188.32 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1534.33 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     862.28 ms /    16 tokens (   53.89 ms per token,    18.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     563.70 ms /     3 runs   (  187.90 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1432.07 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     919.02 ms /    12 tokens (   76.58 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     723.59 ms /     3 runs   (  241.20 ms per token,     4.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1651.17 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1700.92 ms /    17 tokens (  100.05 ms per token,     9.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     574.07 ms /     3 runs   (  191.36 ms per token,     5.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    2281.24 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     891.81 ms /    15 tokens (   59.45 ms per token,    16.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.19 ms /     1 runs   (  194.19 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1089.41 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1002.71 ms /    17 tokens (   58.98 ms per token,    16.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     572.25 ms /     3 runs   (  190.75 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1581.06 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     869.17 ms /    16 tokens (   54.32 ms per token,    18.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.20 ms /     1 runs   (  189.20 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1061.54 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1275.50 ms /    23 tokens (   55.46 ms per token,    18.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     564.02 ms /     3 runs   (  188.01 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1845.42 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     847.50 ms /    16 tokens (   52.97 ms per token,    18.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.94 ms /     1 runs   (  187.94 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1038.65 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1064.67 ms /    19 tokens (   56.04 ms per token,    17.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.66 ms /     1 runs   (  187.66 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1255.38 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     800.44 ms /    13 tokens (   61.57 ms per token,    16.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     225.87 ms /     1 runs   (  225.87 ms per token,     4.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    1029.74 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2450.98 ms /    19 tokens (  129.00 ms per token,     7.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     568.60 ms /     3 runs   (  189.53 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    3025.62 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     887.66 ms /    15 tokens (   59.18 ms per token,    16.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.87 ms /     1 runs   (  190.87 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    1081.77 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1513.89 ms /    30 tokens (   50.46 ms per token,    19.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.44 ms /     1 runs   (  187.44 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1704.78 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1038.24 ms /    20 tokens (   51.91 ms per token,    19.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.10 ms /     1 runs   (  188.10 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1229.51 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1025.18 ms /    17 tokens (   60.30 ms per token,    16.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     188.87 ms /     1 runs   (  188.87 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1217.18 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     755.82 ms /    14 tokens (   53.99 ms per token,    18.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.58 ms /     1 runs   (  186.58 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:       total time =     945.66 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1238.67 ms /    23 tokens (   53.86 ms per token,    18.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     195.28 ms /     1 runs   (  195.28 ms per token,     5.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1437.06 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1045.47 ms /    19 tokens (   55.02 ms per token,    18.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     605.13 ms /     3 runs   (  201.71 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    1656.75 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1818.14 ms /    12 tokens (  151.51 ms per token,     6.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     238.99 ms /     1 runs   (  238.99 ms per token,     4.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2066.58 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     844.82 ms /    16 tokens (   52.80 ms per token,    18.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     558.80 ms /     3 runs   (  186.27 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1409.63 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 70 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     738.22 ms /     4 runs   (  184.56 ms per token,     5.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     744.14 ms /     5 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     840.41 ms /    16 tokens (   52.53 ms per token,    19.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.09 ms /     1 runs   (  186.09 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    1029.62 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     839.33 ms /    16 tokens (   52.46 ms per token,    19.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     569.38 ms /     3 runs   (  189.79 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1414.69 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1128.45 ms /    22 tokens (   51.29 ms per token,    19.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.14 ms /     1 runs   (  187.14 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1318.73 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     794.53 ms /    13 tokens (   61.12 ms per token,    16.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     186.23 ms /     1 runs   (  186.23 ms per token,     5.37 tokens per second)\n",
            "llama_perf_context_print:       total time =     983.91 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     941.28 ms /    18 tokens (   52.29 ms per token,    19.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     561.38 ms /     3 runs   (  187.12 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    1508.48 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     883.21 ms /    15 tokens (   58.88 ms per token,    16.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.89 ms /     1 runs   (  194.89 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1081.30 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1902.27 ms /    13 tokens (  146.33 ms per token,     6.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     257.57 ms /     1 runs   (  257.57 ms per token,     3.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2163.27 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1007.80 ms /    13 tokens (   77.52 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.54 ms /     1 runs   (  190.54 ms per token,     5.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    1201.61 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1529.64 ms /    29 tokens (   52.75 ms per token,    18.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.43 ms /     1 runs   (  193.43 ms per token,     5.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1727.12 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     977.91 ms /    18 tokens (   54.33 ms per token,    18.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.88 ms /     1 runs   (  187.88 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1168.97 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     803.35 ms /    13 tokens (   61.80 ms per token,    16.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.81 ms /     1 runs   (  187.81 ms per token,     5.32 tokens per second)\n",
            "llama_perf_context_print:       total time =     994.44 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1059.17 ms /    19 tokens (   55.75 ms per token,    17.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     194.97 ms /     1 runs   (  194.97 ms per token,     5.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1257.37 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     650.17 ms /    12 tokens (   54.18 ms per token,    18.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     184.51 ms /     1 runs   (  184.51 ms per token,     5.42 tokens per second)\n",
            "llama_perf_context_print:       total time =     837.85 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     778.90 ms /    14 tokens (   55.64 ms per token,    17.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     569.35 ms /     3 runs   (  189.78 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    1353.95 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     859.99 ms /    15 tokens (   57.33 ms per token,    17.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     560.74 ms /     3 runs   (  186.91 ms per token,     5.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    1426.65 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     858.41 ms /    12 tokens (   71.53 ms per token,    13.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     231.66 ms /     1 runs   (  231.66 ms per token,     4.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    1093.56 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1853.47 ms /    11 tokens (  168.50 ms per token,     5.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     184.33 ms /     1 runs   (  184.33 ms per token,     5.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    2040.95 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     958.56 ms /    18 tokens (   53.25 ms per token,    18.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.02 ms /     1 runs   (  189.02 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1150.77 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     989.45 ms /    17 tokens (   58.20 ms per token,    17.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.94 ms /     1 runs   (  192.94 ms per token,     5.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1185.57 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     867.93 ms /    16 tokens (   54.25 ms per token,    18.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.90 ms /     3 runs   (  188.97 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1440.65 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1170.11 ms /    21 tokens (   55.72 ms per token,    17.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     562.51 ms /     3 runs   (  187.50 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1738.42 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     764.38 ms /    14 tokens (   54.60 ms per token,    18.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.77 ms /     3 runs   (  188.92 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1337.04 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1052.48 ms /    19 tokens (   55.39 ms per token,    18.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     562.53 ms /     3 runs   (  187.51 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1621.05 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1149.22 ms /    22 tokens (   52.24 ms per token,    19.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     248.28 ms /     1 runs   (  248.28 ms per token,     4.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1401.04 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2129.14 ms /    16 tokens (  133.07 ms per token,     7.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.12 ms /     1 runs   (  187.12 ms per token,     5.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2320.70 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     780.00 ms /    13 tokens (   60.00 ms per token,    16.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     189.29 ms /     1 runs   (  189.29 ms per token,     5.28 tokens per second)\n",
            "llama_perf_context_print:       total time =     972.45 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     982.95 ms /    17 tokens (   57.82 ms per token,    17.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     577.51 ms /     3 runs   (  192.50 ms per token,     5.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1566.91 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1019.46 ms /    17 tokens (   59.97 ms per token,    16.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     564.94 ms /     3 runs   (  188.31 ms per token,     5.31 tokens per second)\n",
            "llama_perf_context_print:       total time =    1590.25 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1023.37 ms /    20 tokens (   51.17 ms per token,    19.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     185.89 ms /     1 runs   (  185.89 ms per token,     5.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    1212.64 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     969.95 ms /    17 tokens (   57.06 ms per token,    17.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     193.99 ms /     1 runs   (  193.99 ms per token,     5.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1167.35 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     803.90 ms /    13 tokens (   61.84 ms per token,    16.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     562.65 ms /     3 runs   (  187.55 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    1372.62 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     797.94 ms /    13 tokens (   61.38 ms per token,    16.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.54 ms /     1 runs   (  187.54 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:       total time =     988.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    1407.72 ms /    20 tokens (   70.39 ms per token,    14.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     245.57 ms /     1 runs   (  245.57 ms per token,     4.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1656.78 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =    2202.00 ms /    24 tokens (   91.75 ms per token,    10.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.94 ms /     1 runs   (  190.94 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    2397.49 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     709.71 ms /    12 tokens (   59.14 ms per token,    16.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     192.00 ms /     1 runs   (  192.00 ms per token,     5.21 tokens per second)\n",
            "llama_perf_context_print:       total time =     904.95 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     686.88 ms /    12 tokens (   57.24 ms per token,    17.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     190.21 ms /     1 runs   (  190.21 ms per token,     5.26 tokens per second)\n",
            "llama_perf_context_print:       total time =     880.35 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     981.53 ms /    17 tokens (   57.74 ms per token,    17.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     602.00 ms /     3 runs   (  200.66 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    1589.58 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     858.48 ms /    15 tokens (   57.23 ms per token,    17.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =     567.35 ms /     3 runs   (  189.12 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1432.13 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    3650.30 ms\n",
            "llama_perf_context_print: prompt eval time =     962.40 ms /    17 tokens (   56.61 ms per token,    17.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     566.64 ms /     3 runs   (  188.88 ms per token,     5.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    1534.89 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from datasets import load_dataset\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Download GGUF model from Hugging Face\n",
        "# ----------------------------\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"Deeps03/qwen2-1.5b-log-classifier\",  # your repo\n",
        "    filename=\"qwen2-1.5b-log-classifier-f16.gguf\"  # pick Q4, Q5, or Q8\n",
        ")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=4096,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Load dataset (small subset)\n",
        "# ----------------------------\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/generated_logs.jsonl\")\n",
        "test_data = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)[\"test\"]\n",
        "\n",
        "small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Inference loop\n",
        "# ----------------------------\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for ex in small_test:\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert log analysis assistant. Your task is to classify log messages into one of:\n",
        "'incident', 'preventive_action', or 'normal'.\n",
        "Provide only the classification word.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Classify the following log message:\n",
        "Log Entry: {ex['message']}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=5,\n",
        "        stop=[\"<|im_end|>\", \"\\n\"],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    # Extract model response\n",
        "    pred = output[\"choices\"][0][\"text\"].strip().lower()\n",
        "\n",
        "    # fallback if model returns junk\n",
        "    if pred not in [\"incident\", \"preventive_action\", \"normal\"]:\n",
        "        pred = \"normal\"\n",
        "\n",
        "    y_pred.append(pred)\n",
        "    y_true.append(ex[\"label\"].lower())\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Confusion Matrix\n",
        "# ----------------------------\n",
        "labels = [\"incident\", \"preventive_action\", \"normal\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix - Qwen2 Log Classifier (F16)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "11ce012d66ab4b138d2a0caf091b4402",
            "91662a460eab46a4a6aff44c88fe6128",
            "ed2d904efcf14a43b90abfd9a4482395",
            "5c78b1e42f154eb5ab825eddbbbb8b9c",
            "8dfb17cdc04841d2a7bcfd289bc32ce0",
            "24454100e7284d9c989ca418bdd0aeb1",
            "5a9cd70455a94e61b70bfa9cee9c9b5d",
            "888c6f75148a4c83b8368d6303c0d8d4",
            "fd1e3d5c0bce4ce6b0ebe5f5827c2b61",
            "6dcbdd1ebb944e5fa944d4fb8bd0d93f",
            "a6d0476209a84037a931fab5024c0350"
          ]
        },
        "id": "5dsOpE75mzA0",
        "outputId": "98a8b672-e7c5-46b2-b4c6-ae670e971166"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11ce012d66ab4b138d2a0caf091b4402",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "qwen2-1.5b-log-classifier-f16.gguf:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 338 tensors from /root/.cache/huggingface/hub/models--Deeps03--qwen2-1.5b-log-classifier/snapshots/e182527b13a5113ab93fd40f379cb7a3f9c48a85/qwen2-1.5b-log-classifier-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2 1.5B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   9:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  10:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  12:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  13:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  14:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  15:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151646]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151646]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151645\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = F16\n",
            "print_info: file size   = 2.87 GiB (16.00 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.9308 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.54 B\n",
            "print_info: general.name     = Qwen2 1.5B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151646\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151645 '<|im_end|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (f16) (and 338 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  2943.83 MiB\n",
            "......................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   112.00 MiB\n",
            "llama_kv_cache_unified: size =  112.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2704\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   302.18 MiB\n",
            "llama_context: graph nodes  = 1070\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '1', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151645', 'general.basename': 'Qwen2', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2 1.5B Instruct', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '1.5B', 'general.license': 'apache-2.0', 'qwen2.context_length': '32768', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '8960', 'qwen2.attention.head_count': '12'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    4870.42 ms /    74 tokens (   65.82 ms per token,    15.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.60 ms /     1 runs   (  322.60 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    5196.09 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1138.88 ms /    16 tokens (   71.18 ms per token,    14.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     310.47 ms /     1 runs   (  310.47 ms per token,     3.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1452.50 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     941.87 ms /    13 tokens (   72.45 ms per token,    13.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1045.14 ms /     3 runs   (  348.38 ms per token,     2.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    1993.28 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1827.01 ms /    12 tokens (  152.25 ms per token,     6.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.31 ms /     1 runs   (  320.31 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2150.60 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     965.81 ms /    13 tokens (   74.29 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.75 ms /     3 runs   (  317.25 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1923.66 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1193.69 ms /    17 tokens (   70.22 ms per token,    14.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.43 ms /     3 runs   (  316.81 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2150.23 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1321.71 ms /    19 tokens (   69.56 ms per token,    14.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     946.54 ms /     3 runs   (  315.51 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2274.61 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 25 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1717.59 ms /    25 tokens (   68.70 ms per token,    14.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.60 ms /     1 runs   (  313.60 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2034.48 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1063.95 ms /    15 tokens (   70.93 ms per token,    14.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1071.38 ms /     3 runs   (  357.13 ms per token,     2.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    2141.93 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2005.56 ms /    18 tokens (  111.42 ms per token,     8.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     986.40 ms /     3 runs   (  328.80 ms per token,     3.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2997.93 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1254.54 ms /    18 tokens (   69.70 ms per token,    14.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     938.20 ms /     3 runs   (  312.73 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2198.62 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     877.57 ms /    12 tokens (   73.13 ms per token,    13.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.56 ms /     1 runs   (  319.56 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1200.35 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1003.39 ms /    14 tokens (   71.67 ms per token,    13.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.52 ms /     1 runs   (  318.52 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1325.36 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1387.33 ms /    20 tokens (   69.37 ms per token,    14.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.89 ms /     1 runs   (  324.89 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1715.47 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1126.71 ms /    16 tokens (   70.42 ms per token,    14.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =     966.72 ms /     3 runs   (  322.24 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2099.41 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2419.57 ms /    17 tokens (  142.33 ms per token,     7.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.62 ms /     3 runs   (  316.54 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    3375.46 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     944.18 ms /    13 tokens (   72.63 ms per token,    13.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.75 ms /     1 runs   (  320.75 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1268.11 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1151.63 ms /    16 tokens (   71.98 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.19 ms /     1 runs   (  314.19 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1469.12 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1015.41 ms /    14 tokens (   72.53 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     946.27 ms /     3 runs   (  315.42 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1967.47 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     999.75 ms /    14 tokens (   71.41 ms per token,    14.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.05 ms /     1 runs   (  318.05 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1320.94 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1458.93 ms /    21 tokens (   69.47 ms per token,    14.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.19 ms /     3 runs   (  317.73 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2418.16 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1512.01 ms /    14 tokens (  108.00 ms per token,     9.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1072.37 ms /     3 runs   (  357.46 ms per token,     2.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    2590.96 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1312.88 ms /    18 tokens (   72.94 ms per token,    13.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.38 ms /     1 runs   (  318.38 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1634.79 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1022.74 ms /    14 tokens (   73.05 ms per token,    13.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.56 ms /     3 runs   (  318.19 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1983.62 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1188.79 ms /    17 tokens (   69.93 ms per token,    14.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     944.72 ms /     3 runs   (  314.91 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2139.29 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1000.74 ms /    13 tokens (   76.98 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     311.55 ms /     1 runs   (  311.55 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1315.42 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     874.24 ms /    12 tokens (   72.85 ms per token,    13.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.05 ms /     1 runs   (  316.05 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1193.30 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1193.76 ms /    17 tokens (   70.22 ms per token,    14.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1002.02 ms /     3 runs   (  334.01 ms per token,     2.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2201.81 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2650.37 ms /    22 tokens (  120.47 ms per token,     8.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.30 ms /     1 runs   (  313.30 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2968.53 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1008.57 ms /    14 tokens (   72.04 ms per token,    13.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.68 ms /     1 runs   (  315.68 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1327.41 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1005.85 ms /    14 tokens (   71.85 ms per token,    13.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.10 ms /     1 runs   (  322.10 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1331.45 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     863.98 ms /    12 tokens (   72.00 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.27 ms /     1 runs   (  317.27 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1184.41 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     886.88 ms /    12 tokens (   73.91 ms per token,    13.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.78 ms /     1 runs   (  312.78 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1202.86 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1247.30 ms /    18 tokens (   69.29 ms per token,    14.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.30 ms /     1 runs   (  319.30 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1569.83 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1555.36 ms /    22 tokens (   70.70 ms per token,    14.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.86 ms /     1 runs   (  320.86 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1879.62 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2314.88 ms /    18 tokens (  128.60 ms per token,     7.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1043.39 ms /     3 runs   (  347.80 ms per token,     2.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    3364.57 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     885.23 ms /    12 tokens (   73.77 ms per token,    13.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.89 ms /     1 runs   (  317.89 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1206.53 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     923.36 ms /    13 tokens (   71.03 ms per token,    14.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     321.92 ms /     1 runs   (  321.92 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1248.54 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1373.17 ms /    20 tokens (   68.66 ms per token,    14.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     946.78 ms /     3 runs   (  315.59 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2325.84 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1182.81 ms /    17 tokens (   69.58 ms per token,    14.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.13 ms /     1 runs   (  316.13 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1502.11 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1596.87 ms /    23 tokens (   69.43 ms per token,    14.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.12 ms /     1 runs   (  315.12 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1915.14 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1387.91 ms /    19 tokens (   73.05 ms per token,    13.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1085.59 ms /     3 runs   (  361.86 ms per token,     2.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    2479.96 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1617.16 ms /    14 tokens (  115.51 ms per token,     8.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.81 ms /     1 runs   (  320.81 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1941.17 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1945.12 ms /    29 tokens (   67.07 ms per token,    14.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     941.81 ms /     3 runs   (  313.94 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2892.80 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1057.17 ms /    15 tokens (   70.48 ms per token,    14.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.35 ms /     1 runs   (  320.35 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1380.66 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1287.31 ms /    18 tokens (   71.52 ms per token,    13.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     947.19 ms /     3 runs   (  315.73 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2240.37 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     867.90 ms /    12 tokens (   72.32 ms per token,    13.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.53 ms /     1 runs   (  314.53 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1185.40 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     883.12 ms /    12 tokens (   73.59 ms per token,    13.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.19 ms /     1 runs   (  317.19 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1203.33 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2704.40 ms /    20 tokens (  135.22 ms per token,     7.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.15 ms /     1 runs   (  319.15 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3026.60 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1120.10 ms /    16 tokens (   70.01 ms per token,    14.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     945.23 ms /     3 runs   (  315.08 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2071.21 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     954.99 ms /    13 tokens (   73.46 ms per token,    13.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.88 ms /     1 runs   (  312.88 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1270.93 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     955.47 ms /    13 tokens (   73.50 ms per token,    13.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.16 ms /     3 runs   (  317.06 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1912.51 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1555.32 ms /    22 tokens (   70.70 ms per token,    14.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.45 ms /     1 runs   (  319.45 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1877.98 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     936.22 ms /    13 tokens (   72.02 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.06 ms /     1 runs   (  312.06 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1251.37 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1060.89 ms /    15 tokens (   70.73 ms per token,    14.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     356.22 ms /     1 runs   (  356.22 ms per token,     2.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    1420.34 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2549.71 ms /    20 tokens (  127.49 ms per token,     7.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     326.91 ms /     1 runs   (  326.91 ms per token,     3.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    2882.51 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     891.29 ms /    12 tokens (   74.27 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.13 ms /     1 runs   (  320.13 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1214.66 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1083.15 ms /    15 tokens (   72.21 ms per token,    13.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     326.51 ms /     1 runs   (  326.51 ms per token,     3.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1412.80 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1160.12 ms /    15 tokens (   77.34 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     328.25 ms /     1 runs   (  328.25 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1491.56 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1201.20 ms /    17 tokens (   70.66 ms per token,    14.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     964.71 ms /     3 runs   (  321.57 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2172.31 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1018.03 ms /    14 tokens (   72.72 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.03 ms /     1 runs   (  318.03 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1339.41 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1083.21 ms /    15 tokens (   72.21 ms per token,    13.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     336.56 ms /     1 runs   (  336.56 ms per token,     2.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    1423.60 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    3280.25 ms /    28 tokens (  117.15 ms per token,     8.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     978.59 ms /     3 runs   (  326.20 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    4265.05 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1364.24 ms /    19 tokens (   71.80 ms per token,    13.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.80 ms /     3 runs   (  316.93 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2321.30 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     882.76 ms /    12 tokens (   73.56 ms per token,    13.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.41 ms /     1 runs   (  313.41 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1199.44 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1015.08 ms /    14 tokens (   72.51 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     339.80 ms /     1 runs   (  339.80 ms per token,     2.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    1357.98 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1204.10 ms /    17 tokens (   70.83 ms per token,    14.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.87 ms /     1 runs   (  317.87 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1525.27 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     950.60 ms /    13 tokens (   73.12 ms per token,    13.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.63 ms /     1 runs   (  322.63 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1276.44 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1141.01 ms /    14 tokens (   81.50 ms per token,    12.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     366.38 ms /     1 runs   (  366.38 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    1510.75 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2025.62 ms /    14 tokens (  144.69 ms per token,     6.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     948.61 ms /     3 runs   (  316.20 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2980.17 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1944.19 ms /    29 tokens (   67.04 ms per token,    14.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     955.86 ms /     3 runs   (  318.62 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2905.98 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1130.19 ms /    16 tokens (   70.64 ms per token,    14.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.52 ms /     3 runs   (  316.51 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2085.53 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1066.00 ms /    15 tokens (   71.07 ms per token,    14.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.42 ms /     1 runs   (  327.42 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1396.56 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 62 prefix-match hit, remaining 9 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     699.95 ms /     9 tokens (   77.77 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.97 ms /     1 runs   (  318.97 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1022.10 ms /    10 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1099.38 ms /    15 tokens (   73.29 ms per token,    13.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1098.77 ms /     3 runs   (  366.26 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    2204.66 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1678.03 ms /    15 tokens (  111.87 ms per token,     8.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.75 ms /     1 runs   (  327.75 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    2008.94 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1591.99 ms /    23 tokens (   69.22 ms per token,    14.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.86 ms /     1 runs   (  327.86 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1923.48 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     973.37 ms /    13 tokens (   74.87 ms per token,    13.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.50 ms /     1 runs   (  318.50 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1295.04 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 31 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2132.35 ms /    31 tokens (   68.79 ms per token,    14.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.97 ms /     1 runs   (  320.97 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2456.59 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1272.69 ms /    18 tokens (   70.71 ms per token,    14.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     955.55 ms /     3 runs   (  318.52 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2234.07 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1510.45 ms /    21 tokens (   71.93 ms per token,    13.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1089.63 ms /     3 runs   (  363.21 ms per token,     2.75 tokens per second)\n",
            "llama_perf_context_print:       total time =    2606.80 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2114.61 ms /    23 tokens (   91.94 ms per token,    10.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.90 ms /     1 runs   (  316.90 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2434.67 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     963.07 ms /    13 tokens (   74.08 ms per token,    13.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.11 ms /     1 runs   (  317.11 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1283.34 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     878.52 ms /    12 tokens (   73.21 ms per token,    13.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     330.60 ms /     1 runs   (  330.60 ms per token,     3.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1212.27 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1443.31 ms /    21 tokens (   68.73 ms per token,    14.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     945.47 ms /     3 runs   (  315.16 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2394.57 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1272.90 ms /    18 tokens (   70.72 ms per token,    14.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     938.15 ms /     3 runs   (  312.72 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2216.99 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1207.72 ms /    17 tokens (   71.04 ms per token,    14.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1039.64 ms /     3 runs   (  346.55 ms per token,     2.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    2253.99 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1829.86 ms /    13 tokens (  140.76 ms per token,     7.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.24 ms /     1 runs   (  315.24 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2148.51 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1206.59 ms /    17 tokens (   70.98 ms per token,    14.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     970.51 ms /     3 runs   (  323.50 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2183.33 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1150.16 ms /    16 tokens (   71.89 ms per token,    13.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.46 ms /     3 runs   (  316.82 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2106.67 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1347.10 ms /    19 tokens (   70.90 ms per token,    14.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     311.96 ms /     1 runs   (  311.96 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1662.50 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1400.18 ms /    20 tokens (   70.01 ms per token,    14.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =     968.54 ms /     3 runs   (  322.85 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2375.03 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2250.21 ms /    23 tokens (   97.84 ms per token,    10.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     377.16 ms /     1 runs   (  377.16 ms per token,     2.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    2630.96 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1354.77 ms /    13 tokens (  104.21 ms per token,     9.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     329.09 ms /     1 runs   (  329.09 ms per token,     3.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1687.96 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1004.69 ms /    13 tokens (   77.28 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.96 ms /     1 runs   (  318.96 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1326.80 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1262.87 ms /    18 tokens (   70.16 ms per token,    14.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     944.65 ms /     3 runs   (  314.88 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2213.39 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1322.80 ms /    19 tokens (   69.62 ms per token,    14.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.77 ms /     3 runs   (  316.93 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2279.38 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1071.14 ms /    15 tokens (   71.41 ms per token,    14.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.43 ms /     1 runs   (  315.43 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1389.74 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1231.13 ms /    17 tokens (   72.42 ms per token,    13.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     968.54 ms /     3 runs   (  322.85 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2205.83 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2282.68 ms /    14 tokens (  163.05 ms per token,     6.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.06 ms /     3 runs   (  316.35 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    3238.52 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1071.50 ms /    15 tokens (   71.43 ms per token,    14.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.37 ms /     3 runs   (  316.79 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2027.72 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1025.01 ms /    14 tokens (   73.22 ms per token,    13.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.50 ms /     3 runs   (  316.50 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1980.91 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1281.64 ms /    18 tokens (   71.20 ms per token,    14.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.60 ms /     3 runs   (  317.53 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2240.84 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1188.82 ms /    16 tokens (   74.30 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.13 ms /     1 runs   (  327.13 ms per token,     3.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1519.13 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1072.85 ms /    14 tokens (   76.63 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     368.71 ms /     1 runs   (  368.71 ms per token,     2.71 tokens per second)\n",
            "llama_perf_context_print:       total time =    1444.84 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2541.14 ms /    22 tokens (  115.51 ms per token,     8.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.44 ms /     1 runs   (  315.44 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2863.26 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1160.65 ms /    16 tokens (   72.54 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     328.19 ms /     1 runs   (  328.19 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1492.25 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     953.08 ms /    13 tokens (   73.31 ms per token,    13.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     929.54 ms /     3 runs   (  309.85 ms per token,     3.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1888.73 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1168.10 ms /    16 tokens (   73.01 ms per token,    13.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     941.35 ms /     3 runs   (  313.78 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2115.23 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1248.15 ms /    17 tokens (   73.42 ms per token,    13.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.77 ms /     1 runs   (  315.77 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1567.18 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1646.97 ms /    24 tokens (   68.62 ms per token,    14.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     339.30 ms /     1 runs   (  339.30 ms per token,     2.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    1989.76 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1762.52 ms /    12 tokens (  146.88 ms per token,     6.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     382.41 ms /     1 runs   (  382.41 ms per token,     2.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    2148.42 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1222.01 ms /    16 tokens (   76.38 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.07 ms /     1 runs   (  314.07 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1539.34 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1959.16 ms /    29 tokens (   67.56 ms per token,    14.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.94 ms /     1 runs   (  318.94 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2281.33 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1204.43 ms /    17 tokens (   70.85 ms per token,    14.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     331.59 ms /     1 runs   (  331.59 ms per token,     3.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1539.21 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1458.54 ms /    21 tokens (   69.45 ms per token,    14.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     941.15 ms /     3 runs   (  313.72 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2405.35 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     955.82 ms /    13 tokens (   73.52 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     938.31 ms /     3 runs   (  312.77 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1900.21 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2174.30 ms /    15 tokens (  144.95 ms per token,     6.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1006.51 ms /     3 runs   (  335.50 ms per token,     2.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    3188.00 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1047.37 ms /    14 tokens (   74.81 ms per token,    13.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.31 ms /     1 runs   (  323.31 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1373.91 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1205.02 ms /    17 tokens (   70.88 ms per token,    14.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.12 ms /     1 runs   (  325.12 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1533.48 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1104.68 ms /    15 tokens (   73.65 ms per token,    13.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     955.71 ms /     3 runs   (  318.57 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2066.33 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     959.50 ms /    13 tokens (   73.81 ms per token,    13.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.33 ms /     1 runs   (  312.33 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1275.04 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1257.24 ms /    17 tokens (   73.96 ms per token,    13.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     963.06 ms /     3 runs   (  321.02 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2226.30 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1015.83 ms /    13 tokens (   78.14 ms per token,    12.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     375.94 ms /     1 runs   (  375.94 ms per token,     2.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    1398.67 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2250.90 ms /    18 tokens (  125.05 ms per token,     8.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.15 ms /     1 runs   (  312.15 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2566.44 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1563.65 ms /    22 tokens (   71.08 ms per token,    14.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.95 ms /     3 runs   (  320.32 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2530.49 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1217.08 ms /    17 tokens (   71.59 ms per token,    13.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.88 ms /     3 runs   (  319.29 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2180.92 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1538.80 ms /    22 tokens (   69.95 ms per token,    14.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.14 ms /     1 runs   (  323.14 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1865.12 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1040.41 ms /    14 tokens (   74.31 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     948.48 ms /     3 runs   (  316.16 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1995.03 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2062.26 ms /    15 tokens (  137.48 ms per token,     7.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1066.85 ms /     3 runs   (  355.62 ms per token,     2.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    3135.89 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1659.69 ms /    24 tokens (   69.15 ms per token,    14.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.66 ms /     3 runs   (  317.55 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2618.62 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1261.48 ms /    18 tokens (   70.08 ms per token,    14.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.67 ms /     3 runs   (  317.22 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2219.48 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1089.24 ms /    15 tokens (   72.62 ms per token,    13.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     976.04 ms /     3 runs   (  325.35 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    2071.21 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1316.88 ms /    17 tokens (   77.46 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.15 ms /     1 runs   (  316.15 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1636.16 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1473.13 ms /    17 tokens (   86.65 ms per token,    11.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1103.41 ms /     3 runs   (  367.80 ms per token,     2.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    2583.01 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2268.80 ms /    28 tokens (   81.03 ms per token,    12.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     945.07 ms /     3 runs   (  315.02 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    3219.74 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1214.58 ms /    17 tokens (   71.45 ms per token,    14.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     332.45 ms /     1 runs   (  332.45 ms per token,     3.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    1550.19 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1257.67 ms /    18 tokens (   69.87 ms per token,    14.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     955.10 ms /     3 runs   (  318.37 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2218.56 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1014.04 ms /    14 tokens (   72.43 ms per token,    13.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.50 ms /     1 runs   (  317.50 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1334.96 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1366.70 ms /    19 tokens (   71.93 ms per token,    13.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.92 ms /     1 runs   (  320.92 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1690.83 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2657.17 ms /    24 tokens (  110.72 ms per token,     9.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     388.58 ms /     1 runs   (  388.58 ms per token,     2.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    3050.63 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1659.24 ms /    22 tokens (   75.42 ms per token,    13.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.79 ms /     1 runs   (  325.79 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1990.67 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1633.09 ms /    23 tokens (   71.00 ms per token,    14.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.78 ms /     1 runs   (  324.78 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1961.25 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1421.92 ms /    20 tokens (   71.10 ms per token,    14.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.56 ms /     1 runs   (  320.56 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1745.79 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1625.31 ms /    23 tokens (   70.67 ms per token,    14.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.13 ms /     1 runs   (  317.13 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1945.71 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1154.94 ms /    16 tokens (   72.18 ms per token,    13.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.00 ms /     1 runs   (  322.00 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1482.39 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     951.22 ms /    13 tokens (   73.17 ms per token,    13.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     365.76 ms /     1 runs   (  365.76 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    1320.39 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2503.78 ms /    19 tokens (  131.78 ms per token,     7.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     958.16 ms /     3 runs   (  319.39 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3469.88 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1027.19 ms /    14 tokens (   73.37 ms per token,    13.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.87 ms /     1 runs   (  312.87 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1343.30 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1626.59 ms /    23 tokens (   70.72 ms per token,    14.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     955.47 ms /     3 runs   (  318.49 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2587.92 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2070.84 ms /    30 tokens (   69.03 ms per token,    14.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.19 ms /     1 runs   (  319.19 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2393.28 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1010.75 ms /    13 tokens (   77.75 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.79 ms /     1 runs   (  318.79 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1332.71 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1223.01 ms /    16 tokens (   76.44 ms per token,    13.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1089.02 ms /     3 runs   (  363.01 ms per token,     2.75 tokens per second)\n",
            "llama_perf_context_print:       total time =    2318.61 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1947.74 ms /    19 tokens (  102.51 ms per token,     9.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.67 ms /     1 runs   (  317.67 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2273.70 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     919.03 ms /    12 tokens (   76.59 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.47 ms /     1 runs   (  322.47 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1244.72 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1422.05 ms /    20 tokens (   71.10 ms per token,    14.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     959.85 ms /     3 runs   (  319.95 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2387.76 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1371.44 ms /    19 tokens (   72.18 ms per token,    13.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     961.75 ms /     3 runs   (  320.58 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2339.40 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1036.85 ms /    14 tokens (   74.06 ms per token,    13.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.39 ms /     1 runs   (  317.39 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1357.39 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1163.85 ms /    16 tokens (   72.74 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.33 ms /     1 runs   (  325.33 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1493.08 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2221.78 ms /    13 tokens (  170.91 ms per token,     5.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     373.94 ms /     1 runs   (  373.94 ms per token,     2.67 tokens per second)\n",
            "llama_perf_context_print:       total time =    2598.98 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1308.80 ms /    18 tokens (   72.71 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.89 ms /     1 runs   (  319.89 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1631.85 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1312.39 ms /    18 tokens (   72.91 ms per token,    13.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     961.99 ms /     3 runs   (  320.66 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2280.25 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1250.74 ms /    17 tokens (   73.57 ms per token,    13.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.79 ms /     1 runs   (  313.79 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1567.69 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     899.46 ms /    12 tokens (   74.95 ms per token,    13.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.30 ms /     1 runs   (  318.30 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1221.05 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1311.26 ms /    18 tokens (   72.85 ms per token,    13.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     944.47 ms /     3 runs   (  314.82 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2261.65 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2646.28 ms /    21 tokens (  126.01 ms per token,     7.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1059.70 ms /     3 runs   (  353.23 ms per token,     2.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    3713.18 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1746.88 ms /    24 tokens (   72.79 ms per token,    13.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.11 ms /     1 runs   (  327.11 ms per token,     3.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    2077.22 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1139.63 ms /    15 tokens (   75.98 ms per token,    13.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.09 ms /     1 runs   (  322.09 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1465.06 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1244.54 ms /    17 tokens (   73.21 ms per token,    13.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.05 ms /     1 runs   (  323.05 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1570.89 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1278.21 ms /    18 tokens (   71.01 ms per token,    14.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.33 ms /     3 runs   (  318.11 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2238.40 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1296.07 ms /    18 tokens (   72.00 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1015.18 ms /     3 runs   (  338.39 ms per token,     2.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    2317.51 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2552.22 ms /    20 tokens (  127.61 ms per token,     7.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.06 ms /     3 runs   (  319.02 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3519.07 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1044.25 ms /    14 tokens (   74.59 ms per token,    13.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     972.53 ms /     3 runs   (  324.18 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    2022.71 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1294.50 ms /    18 tokens (   71.92 ms per token,    13.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.02 ms /     1 runs   (  315.02 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1612.77 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 61 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     987.14 ms /    13 tokens (   75.93 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.00 ms /     1 runs   (  318.00 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1308.45 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1077.89 ms /    15 tokens (   71.86 ms per token,    13.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     964.95 ms /     3 runs   (  321.65 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2048.72 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1098.52 ms /    15 tokens (   73.23 ms per token,    13.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     367.91 ms /     1 runs   (  367.91 ms per token,     2.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    1469.88 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2270.35 ms /    14 tokens (  162.17 ms per token,     6.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.95 ms /     1 runs   (  317.95 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2593.38 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     848.81 ms /    11 tokens (   77.16 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.83 ms /     1 runs   (  324.83 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1176.79 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     974.54 ms /    13 tokens (   74.96 ms per token,    13.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     335.14 ms /     1 runs   (  335.14 ms per token,     2.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    1313.05 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1358.28 ms /    18 tokens (   75.46 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.60 ms /     3 runs   (  317.87 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2317.83 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1467.71 ms /    21 tokens (   69.89 ms per token,    14.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.56 ms /     3 runs   (  316.52 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2423.64 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1098.63 ms /    15 tokens (   73.24 ms per token,    13.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     970.05 ms /     3 runs   (  323.35 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2074.48 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 59 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1721.83 ms /     8 tokens (  215.23 ms per token,     4.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     396.28 ms /     1 runs   (  396.28 ms per token,     2.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    2121.52 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1113.40 ms /    13 tokens (   85.65 ms per token,    11.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     329.01 ms /     1 runs   (  329.01 ms per token,     3.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    1445.63 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1179.64 ms /    16 tokens (   73.73 ms per token,    13.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.94 ms /     1 runs   (  312.94 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1495.74 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1655.14 ms /    24 tokens (   68.96 ms per token,    14.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     311.94 ms /     1 runs   (  311.94 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1970.29 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     963.56 ms /    13 tokens (   74.12 ms per token,    13.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.98 ms /     1 runs   (  318.98 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1285.56 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     898.90 ms /    12 tokens (   74.91 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.17 ms /     1 runs   (  316.17 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1218.31 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1468.25 ms /    21 tokens (   69.92 ms per token,    14.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.30 ms /     1 runs   (  319.30 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1790.76 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1269.12 ms /    16 tokens (   79.32 ms per token,    12.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1106.55 ms /     3 runs   (  368.85 ms per token,     2.71 tokens per second)\n",
            "llama_perf_context_print:       total time =    2382.21 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1573.31 ms /    14 tokens (  112.38 ms per token,     8.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.87 ms /     1 runs   (  318.87 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1895.44 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1041.82 ms /    14 tokens (   74.42 ms per token,    13.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.24 ms /     1 runs   (  323.24 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1368.21 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1322.06 ms /    18 tokens (   73.45 ms per token,    13.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     958.80 ms /     3 runs   (  319.60 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2286.84 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     982.49 ms /    13 tokens (   75.58 ms per token,    13.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     338.58 ms /     1 runs   (  338.58 ms per token,     2.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    1324.54 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1240.20 ms /    15 tokens (   82.68 ms per token,    12.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     330.82 ms /     1 runs   (  330.82 ms per token,     3.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    1574.19 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1237.34 ms /    17 tokens (   72.78 ms per token,    13.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     971.94 ms /     3 runs   (  323.98 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2215.19 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2296.24 ms /    16 tokens (  143.52 ms per token,     6.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1013.67 ms /     3 runs   (  337.89 ms per token,     2.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    3316.21 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1270.65 ms /    18 tokens (   70.59 ms per token,    14.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.83 ms /     3 runs   (  319.28 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2234.42 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1277.20 ms /    18 tokens (   70.96 ms per token,    14.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     966.31 ms /     3 runs   (  322.10 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2249.58 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1155.92 ms /    16 tokens (   72.24 ms per token,    13.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.85 ms /     3 runs   (  320.28 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2122.66 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     920.80 ms /    12 tokens (   76.73 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     977.96 ms /     3 runs   (  325.99 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1904.79 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 58 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     854.88 ms /    10 tokens (   85.49 ms per token,    11.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1082.10 ms /     3 runs   (  360.70 ms per token,     2.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    1946.40 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1719.09 ms /    16 tokens (  107.44 ms per token,     9.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.87 ms /     1 runs   (  322.87 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2045.29 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     974.34 ms /    13 tokens (   74.95 ms per token,    13.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     956.22 ms /     3 runs   (  318.74 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1936.62 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     971.10 ms /    13 tokens (   74.70 ms per token,    13.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.85 ms /     1 runs   (  322.85 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1297.09 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1467.84 ms /    21 tokens (   69.90 ms per token,    14.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     955.13 ms /     3 runs   (  318.38 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2428.82 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1114.71 ms /    15 tokens (   74.31 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.95 ms /     1 runs   (  322.95 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1440.90 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1220.91 ms /    16 tokens (   76.31 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.96 ms /     1 runs   (  318.96 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1543.05 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 33 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    3509.51 ms /    33 tokens (  106.35 ms per token,     9.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     329.53 ms /     1 runs   (  329.53 ms per token,     3.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3842.26 ms /    34 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     915.83 ms /    12 tokens (   76.32 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     342.52 ms /     1 runs   (  342.52 ms per token,     2.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    1262.34 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1044.45 ms /    14 tokens (   74.60 ms per token,    13.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     959.80 ms /     3 runs   (  319.93 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2010.38 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     981.88 ms /    12 tokens (   81.82 ms per token,    12.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.42 ms /     1 runs   (  315.42 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1300.38 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1452.22 ms /    20 tokens (   72.61 ms per token,    13.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     975.40 ms /     3 runs   (  325.13 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    2434.02 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1248.86 ms /    16 tokens (   78.05 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.13 ms /     1 runs   (  317.13 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1569.26 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1677.57 ms /    15 tokens (  111.84 ms per token,     8.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1100.13 ms /     3 runs   (  366.71 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    2784.19 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1342.92 ms /    18 tokens (   74.61 ms per token,    13.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     968.61 ms /     3 runs   (  322.87 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2319.64 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1428.40 ms /    19 tokens (   75.18 ms per token,    13.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     970.39 ms /     3 runs   (  323.46 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2405.15 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1236.66 ms /    17 tokens (   72.74 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.18 ms /     1 runs   (  325.18 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1565.12 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     905.82 ms /    12 tokens (   75.49 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     321.53 ms /     1 runs   (  321.53 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1230.53 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1189.17 ms /    16 tokens (   74.32 ms per token,    13.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.68 ms /     1 runs   (  318.68 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1511.13 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2034.39 ms /    24 tokens (   84.77 ms per token,    11.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1094.19 ms /     3 runs   (  364.73 ms per token,     2.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    3135.27 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1383.61 ms /    16 tokens (   86.48 ms per token,    11.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     969.19 ms /     3 runs   (  323.06 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2358.80 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1329.63 ms /    18 tokens (   73.87 ms per token,    13.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     962.41 ms /     3 runs   (  320.80 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2298.17 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1050.48 ms /    14 tokens (   75.03 ms per token,    13.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.07 ms /     1 runs   (  320.07 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1373.81 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1229.88 ms /    17 tokens (   72.35 ms per token,    13.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.24 ms /     1 runs   (  323.24 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1556.33 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1590.53 ms /    23 tokens (   69.15 ms per token,    14.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.56 ms /     1 runs   (  313.56 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1907.40 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1301.31 ms /    16 tokens (   81.33 ms per token,    12.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     364.99 ms /     1 runs   (  364.99 ms per token,     2.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    1669.74 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2145.57 ms /    16 tokens (  134.10 ms per token,     7.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.59 ms /     1 runs   (  317.59 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2466.42 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1140.46 ms /    16 tokens (   71.28 ms per token,    14.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.46 ms /     1 runs   (  324.46 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1468.15 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1551.64 ms /    22 tokens (   70.53 ms per token,    14.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.71 ms /     1 runs   (  318.71 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1873.45 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1213.78 ms /    17 tokens (   71.40 ms per token,    14.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.12 ms /     1 runs   (  320.12 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1537.11 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1975.42 ms /    29 tokens (   68.12 ms per token,    14.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     958.28 ms /     3 runs   (  319.43 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2939.69 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1086.01 ms /    15 tokens (   72.40 ms per token,    13.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1027.33 ms /     3 runs   (  342.44 ms per token,     2.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    2120.68 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2219.49 ms /    19 tokens (  116.82 ms per token,     8.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     944.92 ms /     3 runs   (  314.97 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    3172.57 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     891.67 ms /    12 tokens (   74.31 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     309.27 ms /     1 runs   (  309.27 ms per token,     3.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1204.13 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1160.56 ms /    16 tokens (   72.53 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.77 ms /     1 runs   (  325.77 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1489.51 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1332.12 ms /    19 tokens (   70.11 ms per token,    14.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.23 ms /     3 runs   (  319.08 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2295.25 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1012.03 ms /    13 tokens (   77.85 ms per token,    12.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.35 ms /     1 runs   (  318.35 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1333.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1229.49 ms /    17 tokens (   72.32 ms per token,    13.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     947.83 ms /     3 runs   (  315.94 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2183.43 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2645.81 ms /    18 tokens (  146.99 ms per token,     6.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     956.07 ms /     3 runs   (  318.69 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3607.78 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1289.55 ms /    18 tokens (   71.64 ms per token,    13.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.22 ms /     3 runs   (  319.07 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2252.61 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1047.69 ms /    14 tokens (   74.83 ms per token,    13.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.24 ms /     3 runs   (  317.75 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2006.99 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1160.38 ms /    16 tokens (   72.52 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.84 ms /     1 runs   (  323.84 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1487.51 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1104.11 ms /    15 tokens (   73.61 ms per token,    13.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.47 ms /     3 runs   (  316.49 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2059.57 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1311.42 ms /    17 tokens (   77.14 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1094.95 ms /     3 runs   (  364.98 ms per token,     2.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    2412.99 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1769.23 ms /    17 tokens (  104.07 ms per token,     9.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.30 ms /     1 runs   (  316.30 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2088.67 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     867.66 ms /    11 tokens (   78.88 ms per token,    12.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     326.26 ms /     1 runs   (  326.26 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1197.02 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1605.22 ms /    23 tokens (   69.79 ms per token,    14.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.67 ms /     1 runs   (  315.67 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1924.06 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1098.56 ms /    15 tokens (   73.24 ms per token,    13.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.28 ms /     1 runs   (  319.28 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1420.99 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1681.53 ms /    24 tokens (   70.06 ms per token,    14.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.67 ms /     1 runs   (  327.67 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    2012.45 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1037.83 ms /    14 tokens (   74.13 ms per token,    13.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.07 ms /     1 runs   (  320.07 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1362.08 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     970.41 ms /    13 tokens (   74.65 ms per token,    13.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1099.44 ms /     3 runs   (  366.48 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    2076.37 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2441.22 ms /    24 tokens (  101.72 ms per token,     9.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     959.93 ms /     3 runs   (  319.98 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3407.48 ms /    27 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1172.15 ms /    16 tokens (   73.26 ms per token,    13.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.81 ms /     1 runs   (  319.81 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1495.43 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1625.22 ms /    23 tokens (   70.66 ms per token,    14.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     326.14 ms /     1 runs   (  326.14 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1954.65 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1250.63 ms /    17 tokens (   73.57 ms per token,    13.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =     962.09 ms /     3 runs   (  320.70 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2218.62 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     969.68 ms /    13 tokens (   74.59 ms per token,    13.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.62 ms /     1 runs   (  312.62 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1285.44 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     929.62 ms /    12 tokens (   77.47 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     366.75 ms /     1 runs   (  366.75 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    1299.74 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2377.68 ms /    17 tokens (  139.86 ms per token,     7.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.85 ms /     3 runs   (  317.28 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    3337.08 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1355.73 ms /    19 tokens (   71.35 ms per token,    14.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.48 ms /     1 runs   (  318.48 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1677.51 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1349.72 ms /    19 tokens (   71.04 ms per token,    14.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.81 ms /     3 runs   (  317.94 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2309.90 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1560.81 ms /    22 tokens (   70.95 ms per token,    14.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.54 ms /     1 runs   (  315.54 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1881.65 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1373.83 ms /    19 tokens (   72.31 ms per token,    13.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.01 ms /     1 runs   (  317.01 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1694.00 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1094.07 ms /    14 tokens (   78.15 ms per token,    12.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     374.41 ms /     1 runs   (  374.41 ms per token,     2.67 tokens per second)\n",
            "llama_perf_context_print:       total time =    1472.02 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2363.68 ms /    18 tokens (  131.32 ms per token,     7.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     321.27 ms /     1 runs   (  321.27 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2688.11 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1173.99 ms /    16 tokens (   73.37 ms per token,    13.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.51 ms /     1 runs   (  320.51 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1498.01 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1176.13 ms /    16 tokens (   73.51 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.30 ms /     1 runs   (  320.30 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1499.96 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1368.38 ms /    18 tokens (   76.02 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     967.11 ms /     3 runs   (  322.37 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2341.52 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1164.25 ms /    16 tokens (   72.77 ms per token,    13.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.49 ms /     1 runs   (  318.49 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1485.91 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1612.22 ms /    23 tokens (   70.10 ms per token,    14.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.34 ms /     1 runs   (  323.34 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1938.74 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2370.63 ms /    16 tokens (  148.16 ms per token,     6.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     366.68 ms /     1 runs   (  366.68 ms per token,     2.73 tokens per second)\n",
            "llama_perf_context_print:       total time =    2740.57 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1176.64 ms /    16 tokens (   73.54 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.91 ms /     1 runs   (  317.91 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1498.00 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1492.47 ms /    21 tokens (   71.07 ms per token,    14.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.07 ms /     1 runs   (  324.07 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1819.72 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1032.31 ms /    14 tokens (   73.74 ms per token,    13.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.18 ms /     1 runs   (  319.18 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1354.61 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     987.37 ms /    13 tokens (   75.95 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.73 ms /     1 runs   (  319.73 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1310.25 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     974.77 ms /    13 tokens (   74.98 ms per token,    13.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     961.19 ms /     3 runs   (  320.40 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1941.87 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1616.68 ms /    23 tokens (   70.29 ms per token,    14.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1077.74 ms /     3 runs   (  359.25 ms per token,     2.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    2701.26 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1902.37 ms /    15 tokens (  126.82 ms per token,     7.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.94 ms /     1 runs   (  322.94 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2228.74 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1674.58 ms /    23 tokens (   72.81 ms per token,    13.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.90 ms /     1 runs   (  319.90 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1997.71 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     919.30 ms /    12 tokens (   76.61 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.58 ms /     1 runs   (  318.58 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1241.02 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 67 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     648.21 ms /     2 runs   (  324.10 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =     651.18 ms /     3 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     842.29 ms /    11 tokens (   76.57 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.03 ms /     1 runs   (  315.03 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1160.47 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1891.86 ms /    27 tokens (   70.07 ms per token,    14.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.45 ms /     3 runs   (  316.48 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2847.03 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1095.53 ms /    15 tokens (   73.04 ms per token,    13.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1069.80 ms /     3 runs   (  356.60 ms per token,     2.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    2171.75 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2023.66 ms /    14 tokens (  144.55 ms per token,     6.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.76 ms /     3 runs   (  317.25 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2981.67 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1708.70 ms /    24 tokens (   71.20 ms per token,    14.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.40 ms /     1 runs   (  319.40 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2031.41 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     993.23 ms /    13 tokens (   76.40 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =     945.93 ms /     3 runs   (  315.31 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1945.04 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1105.92 ms /    15 tokens (   73.73 ms per token,    13.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.73 ms /     1 runs   (  325.73 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1434.86 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1169.93 ms /    16 tokens (   73.12 ms per token,    13.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.52 ms /     1 runs   (  313.52 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1486.87 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1250.21 ms /    17 tokens (   73.54 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1027.01 ms /     3 runs   (  342.34 ms per token,     2.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    2283.71 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2022.53 ms /    14 tokens (  144.47 ms per token,     6.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.52 ms /     1 runs   (  325.52 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    2351.29 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     929.74 ms /    12 tokens (   77.48 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     966.79 ms /     3 runs   (  322.26 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1902.52 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1458.73 ms /    20 tokens (   72.94 ms per token,    13.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.59 ms /     3 runs   (  317.86 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2418.37 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1246.13 ms /    17 tokens (   73.30 ms per token,    13.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     321.19 ms /     1 runs   (  321.19 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1570.54 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     923.36 ms /    12 tokens (   76.95 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     334.79 ms /     1 runs   (  334.79 ms per token,     2.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    1261.58 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1168.41 ms /    15 tokens (   77.89 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.42 ms /     1 runs   (  317.42 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1489.16 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1214.66 ms /    15 tokens (   80.98 ms per token,    12.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     362.89 ms /     1 runs   (  362.89 ms per token,     2.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    1581.32 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2894.47 ms /    29 tokens (   99.81 ms per token,    10.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.19 ms /     3 runs   (  316.73 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    3850.53 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1028.36 ms /    13 tokens (   79.10 ms per token,    12.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.17 ms /     1 runs   (  319.17 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1350.74 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1273.72 ms /    17 tokens (   74.92 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.29 ms /     1 runs   (  323.29 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1600.38 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     980.77 ms /    13 tokens (   75.44 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.71 ms /     1 runs   (  316.71 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1302.35 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     989.11 ms /    13 tokens (   76.09 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.20 ms /     1 runs   (  320.20 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1312.56 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2043.62 ms /    29 tokens (   70.47 ms per token,    14.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1049.08 ms /     3 runs   (  349.69 ms per token,     2.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3099.17 ms /    32 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2383.36 ms /    21 tokens (  113.49 ms per token,     8.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.46 ms /     3 runs   (  320.15 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    3349.72 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1333.61 ms /    18 tokens (   74.09 ms per token,    13.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.50 ms /     3 runs   (  316.50 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2289.42 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1179.27 ms /    16 tokens (   73.70 ms per token,    13.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     961.85 ms /     3 runs   (  320.62 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2147.05 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1227.99 ms /    17 tokens (   72.23 ms per token,    13.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.07 ms /     1 runs   (  319.07 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1550.36 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1199.76 ms /    16 tokens (   74.99 ms per token,    13.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     956.20 ms /     3 runs   (  318.73 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2161.93 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2156.20 ms /    15 tokens (  143.75 ms per token,     6.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1020.55 ms /     3 runs   (  340.18 ms per token,     2.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    3182.48 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1016.26 ms /    13 tokens (   78.17 ms per token,    12.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     325.72 ms /     1 runs   (  325.72 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1345.15 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1286.54 ms /    18 tokens (   71.47 ms per token,    13.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.07 ms /     1 runs   (  312.07 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1601.74 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1612.94 ms /    23 tokens (   70.13 ms per token,    14.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.94 ms /     1 runs   (  316.94 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1933.14 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1086.73 ms /    15 tokens (   72.45 ms per token,    13.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.70 ms /     1 runs   (  319.70 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1409.73 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1328.10 ms /    18 tokens (   73.78 ms per token,    13.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.11 ms /     3 runs   (  317.37 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2286.64 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1293.14 ms /    15 tokens (   86.21 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     355.94 ms /     1 runs   (  355.94 ms per token,     2.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    1652.49 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2046.58 ms /    15 tokens (  136.44 ms per token,     7.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =     329.13 ms /     1 runs   (  329.13 ms per token,     3.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2381.29 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1413.03 ms /    20 tokens (   70.65 ms per token,    14.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.51 ms /     3 runs   (  317.50 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2372.40 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1037.02 ms /    14 tokens (   74.07 ms per token,    13.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.53 ms /     1 runs   (  317.53 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1357.69 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     895.62 ms /    12 tokens (   74.64 ms per token,    13.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.28 ms /     1 runs   (  315.28 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1214.02 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     843.43 ms /    11 tokens (   76.68 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.49 ms /     1 runs   (  317.49 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1164.20 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 27 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1871.78 ms /    27 tokens (   69.33 ms per token,    14.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     956.91 ms /     3 runs   (  318.97 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2834.55 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1869.49 ms /    13 tokens (  143.81 ms per token,     6.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     376.15 ms /     1 runs   (  376.15 ms per token,     2.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2249.14 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     846.83 ms /     8 tokens (  105.85 ms per token,     9.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.51 ms /     1 runs   (  323.51 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1174.56 ms /     9 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1317.09 ms /    18 tokens (   73.17 ms per token,    13.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.13 ms /     1 runs   (  319.13 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1639.47 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1194.04 ms /    16 tokens (   74.63 ms per token,    13.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.27 ms /     3 runs   (  316.76 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2150.07 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2046.08 ms /    30 tokens (   68.20 ms per token,    14.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.00 ms /     1 runs   (  322.00 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2371.76 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1295.70 ms /    18 tokens (   71.98 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     956.32 ms /     3 runs   (  318.77 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2258.49 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2010.78 ms /    19 tokens (  105.83 ms per token,     9.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     381.63 ms /     1 runs   (  381.63 ms per token,     2.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    2395.86 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1247.56 ms /    12 tokens (  103.96 ms per token,     9.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     333.60 ms /     1 runs   (  333.60 ms per token,     3.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    1584.56 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 60 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     786.97 ms /    10 tokens (   78.70 ms per token,    12.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.10 ms /     3 runs   (  317.70 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1746.23 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1145.88 ms /    16 tokens (   71.62 ms per token,    13.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     962.84 ms /     3 runs   (  320.95 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2114.72 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1233.09 ms /    17 tokens (   72.53 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.33 ms /     3 runs   (  317.11 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2191.39 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1229.78 ms /    17 tokens (   72.34 ms per token,    13.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.40 ms /     3 runs   (  318.13 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2190.21 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1316.13 ms /    17 tokens (   77.42 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1093.90 ms /     3 runs   (  364.63 ms per token,     2.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    2419.55 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2025.56 ms /    20 tokens (  101.28 ms per token,     9.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     334.34 ms /     1 runs   (  334.34 ms per token,     2.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2363.65 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1106.99 ms /    15 tokens (   73.80 ms per token,    13.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =     943.91 ms /     3 runs   (  314.64 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2056.75 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1095.93 ms /    15 tokens (   73.06 ms per token,    13.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     310.68 ms /     1 runs   (  310.68 ms per token,     3.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1409.83 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1048.76 ms /    14 tokens (   74.91 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.42 ms /     1 runs   (  318.42 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1370.37 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     965.83 ms /    13 tokens (   74.29 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.78 ms /     1 runs   (  314.78 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1283.79 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1028.47 ms /    14 tokens (   73.46 ms per token,    13.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     947.66 ms /     3 runs   (  315.89 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1982.33 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1726.66 ms /    16 tokens (  107.92 ms per token,     9.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1111.98 ms /     3 runs   (  370.66 ms per token,     2.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    2845.17 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1229.82 ms /    16 tokens (   76.86 ms per token,    13.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.45 ms /     3 runs   (  320.15 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2196.71 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1239.38 ms /    17 tokens (   72.90 ms per token,    13.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     946.78 ms /     3 runs   (  315.59 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2191.98 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1088.88 ms /    15 tokens (   72.59 ms per token,    13.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.03 ms /     3 runs   (  320.01 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2054.82 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2190.01 ms /    32 tokens (   68.44 ms per token,    14.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.66 ms /     1 runs   (  323.66 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2517.03 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1456.22 ms /    19 tokens (   76.64 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.08 ms /     1 runs   (  389.08 ms per token,     2.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1855.46 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2215.44 ms /    18 tokens (  123.08 ms per token,     8.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     947.31 ms /     3 runs   (  315.77 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    3168.66 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     982.89 ms /    13 tokens (   75.61 ms per token,    13.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.06 ms /     1 runs   (  320.06 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1306.11 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     977.99 ms /    13 tokens (   75.23 ms per token,    13.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.58 ms /     3 runs   (  318.19 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1938.43 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2190.87 ms /    32 tokens (   68.46 ms per token,    14.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.95 ms /     1 runs   (  316.95 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2511.09 ms /    33 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1307.17 ms /    18 tokens (   72.62 ms per token,    13.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.33 ms /     3 runs   (  316.78 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2263.71 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1011.10 ms /    11 tokens (   91.92 ms per token,    10.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =     371.23 ms /     1 runs   (  371.23 ms per token,     2.69 tokens per second)\n",
            "llama_perf_context_print:       total time =    1386.64 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2155.89 ms /    18 tokens (  119.77 ms per token,     8.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     940.99 ms /     3 runs   (  313.66 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3102.88 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     977.49 ms /    13 tokens (   75.19 ms per token,    13.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.31 ms /     1 runs   (  315.31 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1295.99 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1705.80 ms /    24 tokens (   71.08 ms per token,    14.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.90 ms /     1 runs   (  318.90 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2027.87 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1559.72 ms /    22 tokens (   70.90 ms per token,    14.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     309.73 ms /     1 runs   (  309.73 ms per token,     3.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1872.65 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     919.97 ms /    12 tokens (   76.66 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.98 ms /     1 runs   (  313.98 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1237.09 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     903.14 ms /    12 tokens (   75.26 ms per token,    13.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.69 ms /     1 runs   (  314.69 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1220.97 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1244.80 ms /    15 tokens (   82.99 ms per token,    12.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     385.63 ms /     1 runs   (  385.63 ms per token,     2.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    1633.77 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2539.50 ms /    24 tokens (  105.81 ms per token,     9.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.78 ms /     1 runs   (  318.78 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2863.80 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1210.96 ms /    17 tokens (   71.23 ms per token,    14.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.76 ms /     1 runs   (  322.76 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1537.61 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     925.58 ms /    12 tokens (   77.13 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =     350.68 ms /     1 runs   (  350.68 ms per token,     2.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    1279.93 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1774.69 ms /    17 tokens (  104.39 ms per token,     9.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     970.91 ms /     3 runs   (  323.64 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2751.74 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1423.82 ms /    20 tokens (   71.19 ms per token,    14.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.07 ms /     1 runs   (  323.07 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1750.03 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     972.34 ms /    13 tokens (   74.80 ms per token,    13.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     321.61 ms /     1 runs   (  321.61 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1297.33 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2321.19 ms /    16 tokens (  145.07 ms per token,     6.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.26 ms /     1 runs   (  320.26 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2646.62 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 64 prefix-match hit, remaining 7 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     603.50 ms /     7 tokens (   86.22 ms per token,    11.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     310.88 ms /     1 runs   (  310.88 ms per token,     3.22 tokens per second)\n",
            "llama_perf_context_print:       total time =     917.61 ms /     8 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1160.35 ms /    16 tokens (   72.52 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     946.33 ms /     3 runs   (  315.44 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2112.56 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1297.36 ms /    18 tokens (   72.08 ms per token,    13.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     938.98 ms /     3 runs   (  312.99 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2242.44 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1023.99 ms /    14 tokens (   73.14 ms per token,    13.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.83 ms /     1 runs   (  317.83 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1344.96 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     924.41 ms /    12 tokens (   77.03 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =     946.56 ms /     3 runs   (  315.52 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1876.88 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2081.45 ms /    23 tokens (   90.50 ms per token,    11.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     364.82 ms /     1 runs   (  364.82 ms per token,     2.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    2449.66 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1699.96 ms /    17 tokens (  100.00 ms per token,    10.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.73 ms /     3 runs   (  320.24 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2667.10 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1248.62 ms /    17 tokens (   73.45 ms per token,    13.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.81 ms /     3 runs   (  316.94 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2205.71 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1055.42 ms /    14 tokens (   75.39 ms per token,    13.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.87 ms /     1 runs   (  318.87 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1379.08 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1227.24 ms /    17 tokens (   72.19 ms per token,    13.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.01 ms /     1 runs   (  322.01 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1552.72 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1010.61 ms /    13 tokens (   77.74 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =     327.65 ms /     1 runs   (  327.65 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1341.66 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1168.04 ms /    16 tokens (   73.00 ms per token,    13.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     312.75 ms /     1 runs   (  312.75 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    1483.90 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2144.17 ms /    14 tokens (  153.15 ms per token,     6.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     979.35 ms /     3 runs   (  326.45 ms per token,     3.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    3129.77 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1044.68 ms /    14 tokens (   74.62 ms per token,    13.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.73 ms /     1 runs   (  314.73 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1362.58 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1190.79 ms /    16 tokens (   74.42 ms per token,    13.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =     939.44 ms /     3 runs   (  313.15 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2136.41 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1233.94 ms /    17 tokens (   72.58 ms per token,    13.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.40 ms /     3 runs   (  316.47 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2189.53 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1088.40 ms /    15 tokens (   72.56 ms per token,    13.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.28 ms /     3 runs   (  317.43 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2046.66 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1435.92 ms /    20 tokens (   71.80 ms per token,    13.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1081.16 ms /     3 runs   (  360.39 ms per token,     2.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    2525.66 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 57 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1811.51 ms /    19 tokens (   95.34 ms per token,    10.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.94 ms /     3 runs   (  318.31 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2774.06 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1438.15 ms /    20 tokens (   71.91 ms per token,    13.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.61 ms /     3 runs   (  317.20 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2397.74 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     843.72 ms /    11 tokens (   76.70 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.99 ms /     1 runs   (  313.99 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1160.91 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1340.52 ms /    19 tokens (   70.55 ms per token,    14.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.07 ms /     1 runs   (  320.07 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1663.79 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     912.14 ms /    12 tokens (   76.01 ms per token,    13.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.68 ms /     1 runs   (  314.68 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1230.22 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1590.72 ms /    22 tokens (   72.31 ms per token,    13.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     389.15 ms /     1 runs   (  389.15 ms per token,     2.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    1983.25 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2229.88 ms /    16 tokens (  139.37 ms per token,     7.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =     967.42 ms /     3 runs   (  322.47 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3205.55 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1195.12 ms /    16 tokens (   74.70 ms per token,    13.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.30 ms /     1 runs   (  320.30 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1518.96 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1048.31 ms /    14 tokens (   74.88 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.80 ms /     3 runs   (  317.60 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2007.13 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1393.36 ms /    19 tokens (   73.33 ms per token,    13.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     932.87 ms /     3 runs   (  310.96 ms per token,     3.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2332.14 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1095.67 ms /    15 tokens (   73.04 ms per token,    13.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =     950.40 ms /     3 runs   (  316.80 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2053.63 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1355.18 ms /    16 tokens (   84.70 ms per token,    11.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1076.32 ms /     3 runs   (  358.77 ms per token,     2.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    2444.15 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1914.55 ms /    23 tokens (   83.24 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =     311.71 ms /     1 runs   (  311.71 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2229.55 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     902.07 ms /    12 tokens (   75.17 ms per token,    13.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.69 ms /     1 runs   (  314.69 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1220.08 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     971.66 ms /    13 tokens (   74.74 ms per token,    13.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.90 ms /     1 runs   (  314.90 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1289.76 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1102.91 ms /    15 tokens (   73.53 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.46 ms /     3 runs   (  319.15 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2066.24 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1339.85 ms /    18 tokens (   74.44 ms per token,    13.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.89 ms /     1 runs   (  318.89 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1661.94 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1675.14 ms /    23 tokens (   72.83 ms per token,    13.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.15 ms /     1 runs   (  315.15 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1993.62 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2524.70 ms /    19 tokens (  132.88 ms per token,     7.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     956.97 ms /     3 runs   (  318.99 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3487.71 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     966.90 ms /    13 tokens (   74.38 ms per token,    13.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     951.10 ms /     3 runs   (  317.03 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1924.92 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1333.45 ms /    18 tokens (   74.08 ms per token,    13.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =     964.73 ms /     3 runs   (  321.58 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2304.02 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 61 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1285.79 ms /    18 tokens (   71.43 ms per token,    14.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.30 ms /     1 runs   (  324.30 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1613.23 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1677.03 ms /    24 tokens (   69.88 ms per token,    14.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.31 ms /     1 runs   (  322.31 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2002.60 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1718.63 ms /    18 tokens (   95.48 ms per token,    10.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1154.02 ms /     3 runs   (  384.67 ms per token,     2.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    2889.00 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1107.85 ms /    15 tokens (   73.86 ms per token,    13.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =     938.93 ms /     3 runs   (  312.98 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2054.81 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1428.12 ms /    20 tokens (   71.41 ms per token,    14.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     311.06 ms /     1 runs   (  311.06 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1742.33 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     975.68 ms /    13 tokens (   75.05 ms per token,    13.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.16 ms /     1 runs   (  316.16 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1295.01 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     906.16 ms /    12 tokens (   75.51 ms per token,    13.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.89 ms /     1 runs   (  317.89 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1227.51 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1675.60 ms /    24 tokens (   69.82 ms per token,    14.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     311.64 ms /     1 runs   (  311.64 ms per token,     3.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    1992.08 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1288.15 ms /    18 tokens (   71.56 ms per token,    13.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1035.85 ms /     3 runs   (  345.28 ms per token,     2.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    2330.33 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1971.52 ms /    16 tokens (  123.22 ms per token,     8.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.81 ms /     1 runs   (  313.81 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2292.64 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1218.35 ms /    17 tokens (   71.67 ms per token,    13.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1004.50 ms /     3 runs   (  334.83 ms per token,     2.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2229.17 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1512.30 ms /    21 tokens (   72.01 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.04 ms /     1 runs   (  324.04 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1839.62 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1308.69 ms /    18 tokens (   72.71 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.96 ms /     1 runs   (  320.96 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    1633.70 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1431.95 ms /    20 tokens (   71.60 ms per token,    13.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.43 ms /     1 runs   (  319.43 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1754.54 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1309.45 ms /    18 tokens (   72.75 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1035.47 ms /     3 runs   (  345.16 ms per token,     2.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    2351.37 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2325.22 ms /    22 tokens (  105.69 ms per token,     9.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     328.35 ms /     1 runs   (  328.35 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    2656.78 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1083.23 ms /    14 tokens (   77.37 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.71 ms /     3 runs   (  318.24 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    2043.66 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1096.10 ms /    15 tokens (   73.07 ms per token,    13.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =     968.39 ms /     3 runs   (  322.80 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2070.46 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1087.34 ms /    15 tokens (   72.49 ms per token,    13.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =     953.01 ms /     3 runs   (  317.67 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2046.43 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     905.51 ms /    12 tokens (   75.46 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.01 ms /     1 runs   (  317.01 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1225.72 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1225.68 ms /    17 tokens (   72.10 ms per token,    13.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.44 ms /     1 runs   (  315.44 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1544.29 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2672.61 ms /    20 tokens (  133.63 ms per token,     7.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.73 ms /     1 runs   (  316.73 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2992.51 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1477.36 ms /    21 tokens (   70.35 ms per token,    14.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     944.48 ms /     3 runs   (  314.83 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2427.71 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1678.87 ms /    24 tokens (   69.95 ms per token,    14.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.58 ms /     1 runs   (  323.58 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2005.66 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1447.60 ms /    20 tokens (   72.38 ms per token,    13.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.90 ms /     1 runs   (  316.90 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1767.91 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1319.73 ms /    18 tokens (   73.32 ms per token,    13.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =     949.68 ms /     3 runs   (  316.56 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    2275.54 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2769.02 ms /    23 tokens (  120.39 ms per token,     8.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.74 ms /     1 runs   (  316.74 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    3092.33 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1244.33 ms /    17 tokens (   73.20 ms per token,    13.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.29 ms /     1 runs   (  315.29 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1562.85 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1251.87 ms /    17 tokens (   73.64 ms per token,    13.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =     936.57 ms /     3 runs   (  312.19 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2194.44 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1121.73 ms /    15 tokens (   74.78 ms per token,    13.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.18 ms /     1 runs   (  317.18 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1442.06 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     992.13 ms /    13 tokens (   76.32 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.00 ms /     1 runs   (  320.00 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1315.33 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1417.60 ms /    20 tokens (   70.88 ms per token,    14.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     313.67 ms /     1 runs   (  313.67 ms per token,     3.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1734.83 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1109.98 ms /    15 tokens (   74.00 ms per token,    13.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =     339.38 ms /     1 runs   (  339.38 ms per token,     2.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    1452.77 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2392.24 ms /    18 tokens (  132.90 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =     942.03 ms /     3 runs   (  314.01 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    3341.63 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1178.94 ms /    16 tokens (   73.68 ms per token,    13.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =     936.17 ms /     3 runs   (  312.06 ms per token,     3.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2121.23 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     903.99 ms /    12 tokens (   75.33 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.64 ms /     3 runs   (  319.21 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1867.52 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1223.28 ms /    17 tokens (   71.96 ms per token,    13.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     943.09 ms /     3 runs   (  314.36 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2172.15 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1078.94 ms /    15 tokens (   71.93 ms per token,    13.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     314.23 ms /     1 runs   (  314.23 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    1396.37 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1350.20 ms /    17 tokens (   79.42 ms per token,    12.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1076.63 ms /     3 runs   (  358.88 ms per token,     2.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    2437.19 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1493.38 ms /    16 tokens (   93.34 ms per token,    10.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.73 ms /     1 runs   (  316.73 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1813.25 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1621.19 ms /    23 tokens (   70.49 ms per token,    14.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =     952.60 ms /     3 runs   (  317.53 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2579.81 ms /    26 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1155.53 ms /    16 tokens (   72.22 ms per token,    13.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =     318.01 ms /     1 runs   (  318.01 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1476.77 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1367.12 ms /    19 tokens (   71.95 ms per token,    13.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.64 ms /     1 runs   (  315.64 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1685.95 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     996.51 ms /    13 tokens (   76.65 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =     310.05 ms /     1 runs   (  310.05 ms per token,     3.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    1309.72 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1398.10 ms /    19 tokens (   73.58 ms per token,    13.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1011.01 ms /     3 runs   (  337.00 ms per token,     2.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    2415.62 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2031.49 ms /    15 tokens (  135.43 ms per token,     7.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.15 ms /     1 runs   (  319.15 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2357.89 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 30 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2055.18 ms /    30 tokens (   68.51 ms per token,    14.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =     331.68 ms /     1 runs   (  331.68 ms per token,     3.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    2390.15 ms /    31 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1407.06 ms /    20 tokens (   70.35 ms per token,    14.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.74 ms /     1 runs   (  319.74 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1729.97 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1218.79 ms /    17 tokens (   71.69 ms per token,    13.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.22 ms /     1 runs   (  319.22 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1541.49 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1047.60 ms /    14 tokens (   74.83 ms per token,    13.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.44 ms /     1 runs   (  316.44 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1367.21 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 23 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1645.94 ms /    23 tokens (   71.56 ms per token,    13.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.13 ms /     1 runs   (  316.13 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1965.33 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2439.81 ms /    19 tokens (  128.41 ms per token,     7.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     991.55 ms /     3 runs   (  330.52 ms per token,     3.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3437.36 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     878.65 ms /    12 tokens (   73.22 ms per token,    13.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =     323.38 ms /     1 runs   (  323.38 ms per token,     3.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    1206.29 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1166.07 ms /    16 tokens (   72.88 ms per token,    13.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     942.46 ms /     3 runs   (  314.15 ms per token,     3.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2114.44 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 70 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =    1270.98 ms /     4 runs   (  317.75 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1276.82 ms /     5 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1165.87 ms /    16 tokens (   72.87 ms per token,    13.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =     326.25 ms /     1 runs   (  326.25 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1495.29 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1157.98 ms /    16 tokens (   72.37 ms per token,    13.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.41 ms /     3 runs   (  320.14 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2124.49 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2463.04 ms /    22 tokens (  111.96 ms per token,     8.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =     384.52 ms /     1 runs   (  384.52 ms per token,     2.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    2851.09 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1049.12 ms /    13 tokens (   80.70 ms per token,    12.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.31 ms /     1 runs   (  322.31 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1374.59 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1336.95 ms /    18 tokens (   74.28 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =     962.98 ms /     3 runs   (  320.99 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2305.96 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1125.87 ms /    15 tokens (   75.06 ms per token,    13.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.88 ms /     1 runs   (  315.88 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1444.98 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     978.22 ms /    13 tokens (   75.25 ms per token,    13.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =     331.89 ms /     1 runs   (  331.89 ms per token,     3.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    1313.29 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     960.76 ms /    13 tokens (   73.90 ms per token,    13.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     328.31 ms /     1 runs   (  328.31 ms per token,     3.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    1292.30 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 29 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2014.81 ms /    29 tokens (   69.48 ms per token,    14.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.72 ms /     1 runs   (  320.72 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2339.10 ms /    30 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2404.12 ms /    18 tokens (  133.56 ms per token,     7.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.50 ms /     1 runs   (  324.50 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    2736.73 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     970.81 ms /    13 tokens (   74.68 ms per token,    13.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.03 ms /     1 runs   (  316.03 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1290.06 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1341.62 ms /    19 tokens (   70.61 ms per token,    14.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =     330.04 ms /     1 runs   (  330.04 ms per token,     3.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    1674.92 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     904.47 ms /    12 tokens (   75.37 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     324.70 ms /     1 runs   (  324.70 ms per token,     3.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    1232.42 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 58 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1042.98 ms /    14 tokens (   74.50 ms per token,    13.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =     964.43 ms /     3 runs   (  321.48 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2013.50 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1102.36 ms /    15 tokens (   73.49 ms per token,    13.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     957.26 ms /     3 runs   (  319.09 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2065.67 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     902.53 ms /    12 tokens (   75.21 ms per token,    13.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =     367.26 ms /     1 runs   (  367.26 ms per token,     2.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    1273.23 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 11 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1841.99 ms /    11 tokens (  167.45 ms per token,     5.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =     320.50 ms /     1 runs   (  320.50 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2167.51 ms /    12 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 18 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1310.13 ms /    18 tokens (   72.78 ms per token,    13.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =     321.97 ms /     1 runs   (  321.97 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    1635.42 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 57 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1231.34 ms /    17 tokens (   72.43 ms per token,    13.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.65 ms /     1 runs   (  319.65 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1554.32 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1167.76 ms /    16 tokens (   72.98 ms per token,    13.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     962.50 ms /     3 runs   (  320.83 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2136.40 ms /    19 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 21 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1484.20 ms /    21 tokens (   70.68 ms per token,    14.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     962.62 ms /     3 runs   (  320.87 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2452.93 ms /    24 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 14 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1032.57 ms /    14 tokens (   73.75 ms per token,    13.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1019.71 ms /     3 runs   (  339.90 ms per token,     2.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    2059.44 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 19 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    2376.67 ms /    19 tokens (  125.09 ms per token,     7.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =     958.90 ms /     3 runs   (  319.63 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3346.67 ms /    22 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1543.91 ms /    22 tokens (   70.18 ms per token,    14.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.26 ms /     1 runs   (  317.26 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1864.48 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1182.80 ms /    16 tokens (   73.92 ms per token,    13.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =     315.32 ms /     1 runs   (  315.32 ms per token,     3.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    1501.43 ms /    17 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     970.81 ms /    13 tokens (   74.68 ms per token,    13.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.07 ms /     1 runs   (  319.07 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1294.36 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1238.61 ms /    17 tokens (   72.86 ms per token,    13.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =     964.93 ms /     3 runs   (  321.64 ms per token,     3.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2209.51 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1216.27 ms /    17 tokens (   71.55 ms per token,    13.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1085.04 ms /     3 runs   (  361.68 ms per token,     2.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    2310.83 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1934.05 ms /    20 tokens (   96.70 ms per token,    10.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.16 ms /     1 runs   (  322.16 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2259.50 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1232.93 ms /    17 tokens (   72.53 ms per token,    13.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =     316.46 ms /     1 runs   (  316.46 ms per token,     3.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    1552.58 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     991.27 ms /    13 tokens (   76.25 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =     954.14 ms /     3 runs   (  318.05 ms per token,     3.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    1951.42 ms /    16 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 56 prefix-match hit, remaining 13 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =     972.87 ms /    13 tokens (   74.84 ms per token,    13.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     310.77 ms /     1 runs   (  310.77 ms per token,     3.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    1286.80 ms /    14 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 20 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1446.57 ms /    20 tokens (   72.33 ms per token,    13.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     317.91 ms /     1 runs   (  317.91 ms per token,     3.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    1767.68 ms /    21 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 24 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1673.45 ms /    24 tokens (   69.73 ms per token,    14.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =     319.47 ms /     1 runs   (  319.47 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    1996.30 ms /    25 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1642.32 ms /    12 tokens (  136.86 ms per token,     7.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     388.20 ms /     1 runs   (  388.20 ms per token,     2.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    2034.08 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 56 prefix-match hit, remaining 12 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1108.41 ms /    12 tokens (   92.37 ms per token,    10.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =     322.88 ms /     1 runs   (  322.88 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    1434.50 ms /    13 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1296.12 ms /    17 tokens (   76.24 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =     959.86 ms /     3 runs   (  319.95 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2261.95 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 15 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1170.75 ms /    15 tokens (   78.05 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =     966.99 ms /     3 runs   (  322.33 ms per token,     3.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2143.83 ms /    18 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n",
            "Llama.generate: 55 prefix-match hit, remaining 17 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    4870.65 ms\n",
            "llama_perf_context_print: prompt eval time =    1281.36 ms /    17 tokens (   75.37 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =     960.70 ms /     3 runs   (  320.23 ms per token,     3.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2248.32 ms /    20 tokens\n",
            "llama_perf_context_print:    graphs reused =          2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "011f6b2593b14eadab1f4f2a5edb3a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f5ecc38f9c34056afebc11cd2ba0751": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "140e59a01e84407ca9542f8728a738b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "169bb1693d914e6e84b46b21b334bff6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b38335e9f8c4b24a092bf1b6df0bc58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2e0c9a7240834b8696e92dcf683efb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_140e59a01e84407ca9542f8728a738b1",
            "placeholder": "​",
            "style": "IPY_MODEL_a68155d675474b7c84153852bf825e04",
            "value": "New Data Upload                         : 100%"
          }
        },
        "35e81869d55545c1a223da69937f28a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3f0aaf6fe22d49989ff765fd28a3c2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c1ee615b716443b8855c8ff84431d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69fb510685a1486490bd8996440e211f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd001676ab4443a9822ba4e00b3ee46",
            "placeholder": "​",
            "style": "IPY_MODEL_bf9f85df75f1454eb40f04db5102e9eb",
            "value": " 2.66GB / 2.66GB, 93.9MB/s  "
          }
        },
        "6ed150eff3fe456da7ddab6b62dcccbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73bcb76d98e84793879a91504933c3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73fdf897095e4d81916126600b1cb866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74b589d83f2a4599b12a61d94e98a0e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77087f5e92624d158822106e6031c451": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79f3d1a30e3748a3b43769e2dd2b2b96",
            "max": 3092769120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd6a1ba75d5741249c08818fd61f301b",
            "value": 3092506976
          }
        },
        "79f3d1a30e3748a3b43769e2dd2b2b96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd001676ab4443a9822ba4e00b3ee46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81c97deb048647beab4e5450e9380b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ed150eff3fe456da7ddab6b62dcccbb",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0aaf6fe22d49989ff765fd28a3c2f6",
            "value": " 3.09GB / 3.09GB, 93.9MB/s  "
          }
        },
        "855015fb93a34503b08016ebcf4eac42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9748f6b870024ac6a5d1fa5b58d55a9a",
            "placeholder": "​",
            "style": "IPY_MODEL_73fdf897095e4d81916126600b1cb866",
            "value": " 3.09GB / 3.09GB            "
          }
        },
        "8b5356452a8343849a959e76c6815e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5996ac4b209474ca1f1e18530e0dcb2",
              "IPY_MODEL_77087f5e92624d158822106e6031c451",
              "IPY_MODEL_855015fb93a34503b08016ebcf4eac42"
            ],
            "layout": "IPY_MODEL_73bcb76d98e84793879a91504933c3c7"
          }
        },
        "8b8fa1d870954e3db10e08d061cf07ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_f4d642a23965459191622f7acd8c69f9"
          }
        },
        "8fcc73d4c7a7439fabc0e91efbcfc300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9748f6b870024ac6a5d1fa5b58d55a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975e235f9caf4adb8fc01e8262d2d12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b38335e9f8c4b24a092bf1b6df0bc58",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_011f6b2593b14eadab1f4f2a5edb3a82",
            "value": 1
          }
        },
        "9d05b8e4797c4f3b8a04fb4184054145": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5f493fc09ef498386a72980abb8cba3",
              "IPY_MODEL_bf92d69172a04066a25a4c300f7514a0",
              "IPY_MODEL_81c97deb048647beab4e5450e9380b31"
            ],
            "layout": "IPY_MODEL_169bb1693d914e6e84b46b21b334bff6"
          }
        },
        "a68155d675474b7c84153852bf825e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5f493fc09ef498386a72980abb8cba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b589d83f2a4599b12a61d94e98a0e2",
            "placeholder": "​",
            "style": "IPY_MODEL_e901cfef3bc14c7397d9f0010a15a470",
            "value": "Processing Files (0 / 1)                : 100%"
          }
        },
        "bf92d69172a04066a25a4c300f7514a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e81869d55545c1a223da69937f28a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f5ecc38f9c34056afebc11cd2ba0751",
            "value": 1
          }
        },
        "bf9f85df75f1454eb40f04db5102e9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c546f6ed3e2f4f12ad4b2aa97c1783e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5996ac4b209474ca1f1e18530e0dcb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c546f6ed3e2f4f12ad4b2aa97c1783e2",
            "placeholder": "​",
            "style": "IPY_MODEL_8fcc73d4c7a7439fabc0e91efbcfc300",
            "value": "  .../qwen2-1.5b-log-classifier-f16.gguf: 100%"
          }
        },
        "e901cfef3bc14c7397d9f0010a15a470": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4d642a23965459191622f7acd8c69f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fb21570567c94da19c3940fde0efc669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e0c9a7240834b8696e92dcf683efb75",
              "IPY_MODEL_975e235f9caf4adb8fc01e8262d2d12a",
              "IPY_MODEL_69fb510685a1486490bd8996440e211f"
            ],
            "layout": "IPY_MODEL_5c1ee615b716443b8855c8ff84431d6f"
          }
        },
        "fd6a1ba75d5741249c08818fd61f301b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd083d2acb634fc29d31bc093d7dc813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ee1f3567f1f4c46873d012aedff8b16",
              "IPY_MODEL_24ef2e926494418d897ab037f7410ba5",
              "IPY_MODEL_a67c601fe7b64d8b99b9542fc053d4ad"
            ],
            "layout": "IPY_MODEL_ba6b991cfce2459f92506310387f56fb"
          }
        },
        "4ee1f3567f1f4c46873d012aedff8b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be78d2448c9c4eb79e14f4761d49c5f9",
            "placeholder": "​",
            "style": "IPY_MODEL_671d3702c76e446f8f11d812fea4e601",
            "value": "Processing Files (0 / 1)                : 100%"
          }
        },
        "24ef2e926494418d897ab037f7410ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3bb09691284ea88fdff5041c28915b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9be2ae82a9be4774ac030f8d0c4b29b4",
            "value": 1
          }
        },
        "a67c601fe7b64d8b99b9542fc053d4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00f2ff84f2444f96bb555953c96840f2",
            "placeholder": "​",
            "style": "IPY_MODEL_fb3b5a7b85a740299287302abe79a1f4",
            "value": "  985MB /  986MB, 60.7MB/s  "
          }
        },
        "ba6b991cfce2459f92506310387f56fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be78d2448c9c4eb79e14f4761d49c5f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671d3702c76e446f8f11d812fea4e601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b3bb09691284ea88fdff5041c28915b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9be2ae82a9be4774ac030f8d0c4b29b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00f2ff84f2444f96bb555953c96840f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb3b5a7b85a740299287302abe79a1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1030038e01a64eab8abb9ffeee322e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2f8be7bef104952a20ac6bb28c6bfbf",
              "IPY_MODEL_9f36ac51148743e2b62e22f55c280a8e",
              "IPY_MODEL_6394a7c28f09401e83b70ead3f853a77"
            ],
            "layout": "IPY_MODEL_6559a5d0439c4a38a09f0ad464670c61"
          }
        },
        "d2f8be7bef104952a20ac6bb28c6bfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445c2bb6334f45b0a69ed9c34191370e",
            "placeholder": "​",
            "style": "IPY_MODEL_891e1eaa8ba641a28d64c19a8817cfa4",
            "value": "New Data Upload                         : 100%"
          }
        },
        "9f36ac51148743e2b62e22f55c280a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c7ee1055614c0198e483ae386d9e9a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96d06d68094a4962874ef06ae83f9b12",
            "value": 1
          }
        },
        "6394a7c28f09401e83b70ead3f853a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_103eedcf7bd14d48b41c201a0dcb5052",
            "placeholder": "​",
            "style": "IPY_MODEL_f43b07b3cf234a60913b358d7eb79338",
            "value": "  797MB /  797MB, 60.7MB/s  "
          }
        },
        "6559a5d0439c4a38a09f0ad464670c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445c2bb6334f45b0a69ed9c34191370e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891e1eaa8ba641a28d64c19a8817cfa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22c7ee1055614c0198e483ae386d9e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "96d06d68094a4962874ef06ae83f9b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "103eedcf7bd14d48b41c201a0dcb5052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43b07b3cf234a60913b358d7eb79338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9b1ff094619421ea7b901358c9c4cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2d20bf33e12437ea4066b280268f20a",
              "IPY_MODEL_6c5357ab3c7f430ba0e49ad1febc1c47",
              "IPY_MODEL_a204b2cfccb6441790cc40aeb45136de"
            ],
            "layout": "IPY_MODEL_0b463277a2fe4752a08568c538caafd5"
          }
        },
        "f2d20bf33e12437ea4066b280268f20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dab92f0ec0b4866a8e4d7347ad66bab",
            "placeholder": "​",
            "style": "IPY_MODEL_ce8c694c1022476f85a7f3a9dc259941",
            "value": "  ...en2-1.5b-log-classifier-Q4_K_M.gguf: 100%"
          }
        },
        "6c5357ab3c7f430ba0e49ad1febc1c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c70187ee264bfba60b5ef0e7858a5e",
            "max": 985673760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b746067d2d6842d4b7eefc43d73c31ca",
            "value": 985411616
          }
        },
        "a204b2cfccb6441790cc40aeb45136de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0fa38a122e64a37b94df8ac84af1c4b",
            "placeholder": "​",
            "style": "IPY_MODEL_9e511939d2884ed0b04df4acb5a16b41",
            "value": "  985MB /  986MB            "
          }
        },
        "0b463277a2fe4752a08568c538caafd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dab92f0ec0b4866a8e4d7347ad66bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce8c694c1022476f85a7f3a9dc259941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95c70187ee264bfba60b5ef0e7858a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b746067d2d6842d4b7eefc43d73c31ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0fa38a122e64a37b94df8ac84af1c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e511939d2884ed0b04df4acb5a16b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd97b06a31684702a4781dd20a177135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eda7f5f926734cb4909f0dbfff03a871",
              "IPY_MODEL_0208eef0a5c54d1b80247a6609f6427e",
              "IPY_MODEL_b184145a1e3f40de9e45449206603249"
            ],
            "layout": "IPY_MODEL_a1b1a285d723454eb76785359be51484"
          }
        },
        "eda7f5f926734cb4909f0dbfff03a871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb4840351d845be8277296e8b587bd5",
            "placeholder": "​",
            "style": "IPY_MODEL_79caa7d5ba704103930c19657db45094",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "0208eef0a5c54d1b80247a6609f6427e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04adbfab563a4e8cb4c683ed94922e36",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5109298cc8e49febed16e1dfc680604",
            "value": 1
          }
        },
        "b184145a1e3f40de9e45449206603249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a952be2fcc6f4a2fa3f30c35e45f175b",
            "placeholder": "​",
            "style": "IPY_MODEL_e35b707785e545bc947578e75f98724a",
            "value": " 1.12GB / 1.12GB, 23.6MB/s  "
          }
        },
        "a1b1a285d723454eb76785359be51484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb4840351d845be8277296e8b587bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79caa7d5ba704103930c19657db45094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04adbfab563a4e8cb4c683ed94922e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a5109298cc8e49febed16e1dfc680604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a952be2fcc6f4a2fa3f30c35e45f175b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35b707785e545bc947578e75f98724a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d52801dd9a04a0d910b405a15df98f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9db0a720e9154993a7a1f8382ddccb73",
              "IPY_MODEL_ebf375ed6c2843aeab49f9127adeeccc",
              "IPY_MODEL_5d9e9ed9600f4c228f0b4f58d39a3b58"
            ],
            "layout": "IPY_MODEL_197d5cb732e945988c249bd0300c90ea"
          }
        },
        "9db0a720e9154993a7a1f8382ddccb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50de80bc8cc487584dcee2cdabac5d6",
            "placeholder": "​",
            "style": "IPY_MODEL_725f0a5c628d4d79a98aa59d5e81be09",
            "value": "New Data Upload                         : 100%"
          }
        },
        "ebf375ed6c2843aeab49f9127adeeccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90e9e4402dc3481d9f8f3a7f2189ce6e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efcb58f091fb4984bdeabd58f8298d84",
            "value": 1
          }
        },
        "5d9e9ed9600f4c228f0b4f58d39a3b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a42f526fbc74dc49045cf14b5370474",
            "placeholder": "​",
            "style": "IPY_MODEL_6e3b5e0dec3d4ab0adf2effd120348d8",
            "value": "  767MB /  767MB, 23.6MB/s  "
          }
        },
        "197d5cb732e945988c249bd0300c90ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b50de80bc8cc487584dcee2cdabac5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725f0a5c628d4d79a98aa59d5e81be09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90e9e4402dc3481d9f8f3a7f2189ce6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "efcb58f091fb4984bdeabd58f8298d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a42f526fbc74dc49045cf14b5370474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e3b5e0dec3d4ab0adf2effd120348d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9495de38f2124d589adc651a19943e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6409acb798fe43548c200f62c16a1039",
              "IPY_MODEL_3a69cc09452b48faa7eeb30a6f038642",
              "IPY_MODEL_e249a795288b439c948928b80e2e812a"
            ],
            "layout": "IPY_MODEL_503eba291702442d9485419a166ad661"
          }
        },
        "6409acb798fe43548c200f62c16a1039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fea2ccf76c94e5c9868a9080350db35",
            "placeholder": "​",
            "style": "IPY_MODEL_8264fca6b24241dca84cda5176bc99d1",
            "value": "  ...en2-1.5b-log-classifier-Q5_K_M.gguf: 100%"
          }
        },
        "3a69cc09452b48faa7eeb30a6f038642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259800acb5ea4f1ea2c198e01e95d50c",
            "max": 1124675616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a969b8e0a64b446eab7b84d16ad6bfae",
            "value": 1124675616
          }
        },
        "e249a795288b439c948928b80e2e812a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d7304e2e5794b85bc8bfda0e85db7b4",
            "placeholder": "​",
            "style": "IPY_MODEL_315e73d44860463eb3afae40e8a89175",
            "value": " 1.12GB / 1.12GB            "
          }
        },
        "503eba291702442d9485419a166ad661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fea2ccf76c94e5c9868a9080350db35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8264fca6b24241dca84cda5176bc99d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "259800acb5ea4f1ea2c198e01e95d50c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a969b8e0a64b446eab7b84d16ad6bfae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d7304e2e5794b85bc8bfda0e85db7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315e73d44860463eb3afae40e8a89175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99642e3d8808478eaaff4e8d13be4d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46263d6c2281469cb849f3ba281bcbbd",
              "IPY_MODEL_a9039d53836942248473b38f5227c4a6",
              "IPY_MODEL_36fe90d5450a41c0a647317393085d73"
            ],
            "layout": "IPY_MODEL_2c0c8d4607574c0ca86576d847d204ab"
          }
        },
        "46263d6c2281469cb849f3ba281bcbbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_140270866d3849cc91816af26a09e77e",
            "placeholder": "​",
            "style": "IPY_MODEL_19b186ce4098436fbb916580caf54ed7",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "a9039d53836942248473b38f5227c4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e64051683f3c45c2ad7643a0a3a40478",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3da183ffeeaf40598d86d522c734012c",
            "value": 1
          }
        },
        "36fe90d5450a41c0a647317393085d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c68fc4fc2b40f98a4b73e6e234da11",
            "placeholder": "​",
            "style": "IPY_MODEL_5d5c32e2aed24fe5836916cd97ee0a32",
            "value": " 1.65GB / 1.65GB, 57.1MB/s  "
          }
        },
        "2c0c8d4607574c0ca86576d847d204ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140270866d3849cc91816af26a09e77e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b186ce4098436fbb916580caf54ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e64051683f3c45c2ad7643a0a3a40478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3da183ffeeaf40598d86d522c734012c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52c68fc4fc2b40f98a4b73e6e234da11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5c32e2aed24fe5836916cd97ee0a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9114414e9abb44d6a73d0faa3ff853e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ee8cb3e34524c8794a2113bdc87533a",
              "IPY_MODEL_f7c35fac8e5b473393e5e5b9d1017ad7",
              "IPY_MODEL_7c0d1d1aeff54e1e888fc53b9c892f19"
            ],
            "layout": "IPY_MODEL_1d970099496e40e98e8873e4fdc49301"
          }
        },
        "1ee8cb3e34524c8794a2113bdc87533a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_156ee39402034e4a83de4358e6c6083b",
            "placeholder": "​",
            "style": "IPY_MODEL_3e41198392274a8798065e430106a6d1",
            "value": "New Data Upload                         : 100%"
          }
        },
        "f7c35fac8e5b473393e5e5b9d1017ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01e07d313f81448489390aa96e137ae2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4afd0fdefc7848aabcc08f1563b7d1c8",
            "value": 1
          }
        },
        "7c0d1d1aeff54e1e888fc53b9c892f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b821c49a931f47959df53fb571371142",
            "placeholder": "​",
            "style": "IPY_MODEL_b1825741773a46029ba1bc6e8890746f",
            "value": " 1.47GB / 1.47GB, 57.1MB/s  "
          }
        },
        "1d970099496e40e98e8873e4fdc49301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "156ee39402034e4a83de4358e6c6083b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e41198392274a8798065e430106a6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01e07d313f81448489390aa96e137ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4afd0fdefc7848aabcc08f1563b7d1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b821c49a931f47959df53fb571371142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1825741773a46029ba1bc6e8890746f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2c617d0c7074ea2917060c7d156d001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26874fb011af48b497e2239a9951f341",
              "IPY_MODEL_6513c4d787df47289e4a8fa021108de1",
              "IPY_MODEL_8877b86e57ae447a97562472d882b465"
            ],
            "layout": "IPY_MODEL_d8a541eee92a446da0553eb348fdb91e"
          }
        },
        "26874fb011af48b497e2239a9951f341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e1bcd7edbf44fd95a211b4dc2857dc",
            "placeholder": "​",
            "style": "IPY_MODEL_c8e797e7ba9a446db656fbecf1a6a67d",
            "value": "  ...qwen2-1.5b-log-classifier-Q8_0.gguf: 100%"
          }
        },
        "6513c4d787df47289e4a8fa021108de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a25a99347a46b8abe5e3208f299900",
            "max": 1646090400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0829df8f57a4495096f4a80ca44c8c12",
            "value": 1646090400
          }
        },
        "8877b86e57ae447a97562472d882b465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2d1ef62c7b34f99ab7e1669f0234c36",
            "placeholder": "​",
            "style": "IPY_MODEL_86e3dd20ba99403485cef3bc2dba5c92",
            "value": " 1.65GB / 1.65GB            "
          }
        },
        "d8a541eee92a446da0553eb348fdb91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e1bcd7edbf44fd95a211b4dc2857dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e797e7ba9a446db656fbecf1a6a67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a25a99347a46b8abe5e3208f299900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0829df8f57a4495096f4a80ca44c8c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2d1ef62c7b34f99ab7e1669f0234c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e3dd20ba99403485cef3bc2dba5c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8e9d4e8e7b944aba0add70258bb0516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c422c54bef1a427392214c6edf30788d",
              "IPY_MODEL_defdf8861e404da583030d66d33c9226",
              "IPY_MODEL_03a87c0aaca34661b79e06f4a1bbafe5"
            ],
            "layout": "IPY_MODEL_83860255962f4392b756470f31432c15"
          }
        },
        "c422c54bef1a427392214c6edf30788d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93e3edd7d9a3475bb5e0c3b36169b135",
            "placeholder": "​",
            "style": "IPY_MODEL_ef4a375c72ff4079a243decfdb7dbecf",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "defdf8861e404da583030d66d33c9226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b068bf0e57974af88aee7c0309412d22",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddea06cf5ed740dca9e29f303f5abb7c",
            "value": 1
          }
        },
        "03a87c0aaca34661b79e06f4a1bbafe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e55de40347e45fcb29c595f7bfa5541",
            "placeholder": "​",
            "style": "IPY_MODEL_c7152704ab4c4e1fad2ff79085ec7356",
            "value": "  986MB /  986MB,  106MB/s  "
          }
        },
        "83860255962f4392b756470f31432c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e3edd7d9a3475bb5e0c3b36169b135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4a375c72ff4079a243decfdb7dbecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b068bf0e57974af88aee7c0309412d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ddea06cf5ed740dca9e29f303f5abb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e55de40347e45fcb29c595f7bfa5541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7152704ab4c4e1fad2ff79085ec7356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b8c11cb4538436e93deb7b84983ecd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d70de06933864b1cb7521dfb8b58326e",
              "IPY_MODEL_3bbb65e59d8441b89956f49a856969f4",
              "IPY_MODEL_071c62f506054079a9384c4222c99511"
            ],
            "layout": "IPY_MODEL_4fc881395e9c4f9795e16b23465795e8"
          }
        },
        "d70de06933864b1cb7521dfb8b58326e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7fbafdecf143a4bd863d768d5986c0",
            "placeholder": "​",
            "style": "IPY_MODEL_f44ebdbfec01422e9a1fcfd5ff980233",
            "value": "New Data Upload                         : "
          }
        },
        "3bbb65e59d8441b89956f49a856969f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b47515426014b14bae5f89cdf22188c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4102751f6e904c06946e07738c1a5474",
            "value": 0
          }
        },
        "071c62f506054079a9384c4222c99511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ab8c8bbc0942758d97c28a79680f5a",
            "placeholder": "​",
            "style": "IPY_MODEL_3f6dc962837d4c2190f9d58e9c5bc965",
            "value": "  0.00B /  0.00B,  0.00B/s  "
          }
        },
        "4fc881395e9c4f9795e16b23465795e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7fbafdecf143a4bd863d768d5986c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44ebdbfec01422e9a1fcfd5ff980233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b47515426014b14bae5f89cdf22188c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4102751f6e904c06946e07738c1a5474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79ab8c8bbc0942758d97c28a79680f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6dc962837d4c2190f9d58e9c5bc965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a6bbf4737f641788b7c4188f7cdfb0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5faede1031d143d5b1d5aabfe938e17c",
              "IPY_MODEL_33e72c8a60f64bc2b56e5b870c5f5684",
              "IPY_MODEL_4e8f8e5af3e342b39f577cce037f1154"
            ],
            "layout": "IPY_MODEL_ef8571f7a39340fea435cc68b2017b8c"
          }
        },
        "5faede1031d143d5b1d5aabfe938e17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88c274a802040ca9f49814a1bb7d998",
            "placeholder": "​",
            "style": "IPY_MODEL_8c19764515ca4c7e999da773838e7c3c",
            "value": "  ...en2-1.5b-log-classifier-Q4_K_M.gguf: 100%"
          }
        },
        "33e72c8a60f64bc2b56e5b870c5f5684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88107ded2bff459fb6b13c13bd12ea37",
            "max": 985673760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a4e848f947f44369425a7f32e6e48e8",
            "value": 985673760
          }
        },
        "4e8f8e5af3e342b39f577cce037f1154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f56b7744fa48a49b7622e91086d682",
            "placeholder": "​",
            "style": "IPY_MODEL_f69ace2927394b44a8bd633d30e32bad",
            "value": "  986MB /  986MB            "
          }
        },
        "ef8571f7a39340fea435cc68b2017b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88c274a802040ca9f49814a1bb7d998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c19764515ca4c7e999da773838e7c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88107ded2bff459fb6b13c13bd12ea37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4e848f947f44369425a7f32e6e48e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88f56b7744fa48a49b7622e91086d682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69ace2927394b44a8bd633d30e32bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08c8c7fb3f3e455a808171d1d134b8d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c81af35b0914a7e8765fef2aed60c84",
              "IPY_MODEL_adaeb7f2a92944769e969dff848f9060",
              "IPY_MODEL_4797f23dd1124445b6ab99216f98f4f3"
            ],
            "layout": "IPY_MODEL_9938f9c5cd08427a813f3a35177b5526"
          }
        },
        "6c81af35b0914a7e8765fef2aed60c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_544516bd6b18415fb871bd391760b3bb",
            "placeholder": "​",
            "style": "IPY_MODEL_4b62d92bd3a74e308de82dc09477fbe0",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "adaeb7f2a92944769e969dff848f9060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0283636eaa2c4f2e8001216e6099fe91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86a17c58db2243f5afeb405e013a1b0d",
            "value": 1
          }
        },
        "4797f23dd1124445b6ab99216f98f4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9a240763da04853a9bec67d20fb7400",
            "placeholder": "​",
            "style": "IPY_MODEL_e491c6c87a234e26ac6abc50008f59cc",
            "value": " 1.12GB / 1.12GB,  145MB/s  "
          }
        },
        "9938f9c5cd08427a813f3a35177b5526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "544516bd6b18415fb871bd391760b3bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b62d92bd3a74e308de82dc09477fbe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0283636eaa2c4f2e8001216e6099fe91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "86a17c58db2243f5afeb405e013a1b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9a240763da04853a9bec67d20fb7400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e491c6c87a234e26ac6abc50008f59cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbc87f1e26834242a390b95350ea140a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f39aa1c84fd4465980811107cea5695",
              "IPY_MODEL_7f9609d8e77c43648a8cc45da423d978",
              "IPY_MODEL_84b2b97266c14b329664a4ef30fe647a"
            ],
            "layout": "IPY_MODEL_2a99d1157be149858c15a1378193930c"
          }
        },
        "3f39aa1c84fd4465980811107cea5695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ba97dc1aadc42bebd107c8dc10f3290",
            "placeholder": "​",
            "style": "IPY_MODEL_f91443cd9c4544f59f4896c6597bf95b",
            "value": "New Data Upload                         : "
          }
        },
        "7f9609d8e77c43648a8cc45da423d978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43d7907de3cf4f3d9b6f25d80c9ce56c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8eb3a5fb56614e63b8a515d0183df0f4",
            "value": 0
          }
        },
        "84b2b97266c14b329664a4ef30fe647a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b021e588e1c49cab692d7accf030448",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c172899b03424aa4237ccfdbcabab2",
            "value": "  0.00B /  0.00B,  0.00B/s  "
          }
        },
        "2a99d1157be149858c15a1378193930c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ba97dc1aadc42bebd107c8dc10f3290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91443cd9c4544f59f4896c6597bf95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43d7907de3cf4f3d9b6f25d80c9ce56c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8eb3a5fb56614e63b8a515d0183df0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b021e588e1c49cab692d7accf030448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c172899b03424aa4237ccfdbcabab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d846bd21c4cb4dfb80c669b8d41b5291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7be8d9688da4c09a39b4d660dcecef8",
              "IPY_MODEL_419fb3007c404b8c9a1a74cb82ca8492",
              "IPY_MODEL_9fcd0ffda1844aa0b171780218b3ee08"
            ],
            "layout": "IPY_MODEL_729f77f633104adea0e99b170ab7b0e5"
          }
        },
        "d7be8d9688da4c09a39b4d660dcecef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a08a347fd24b219ac14a4df4ef94e7",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e711039f264d868b5a5d73f3896165",
            "value": "  ...en2-1.5b-log-classifier-Q5_K_M.gguf: 100%"
          }
        },
        "419fb3007c404b8c9a1a74cb82ca8492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3890db83da5d44b594b51e49b0fff942",
            "max": 1124675616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bacd20155eb4b37a426a5c950639840",
            "value": 1124675616
          }
        },
        "9fcd0ffda1844aa0b171780218b3ee08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8f9f1d6fda44adeb6e2ebdc27c91a39",
            "placeholder": "​",
            "style": "IPY_MODEL_fd53d8c63edb4ab09b6d743dc078e6d5",
            "value": " 1.12GB / 1.12GB            "
          }
        },
        "729f77f633104adea0e99b170ab7b0e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a08a347fd24b219ac14a4df4ef94e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e711039f264d868b5a5d73f3896165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3890db83da5d44b594b51e49b0fff942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bacd20155eb4b37a426a5c950639840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8f9f1d6fda44adeb6e2ebdc27c91a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd53d8c63edb4ab09b6d743dc078e6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4227de293cf4d548664fdb001411b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71b8ba4ccdf2491fabbe44c03528772c",
              "IPY_MODEL_98dd91b945164dce883f80619bbb98ef",
              "IPY_MODEL_c24c988cfffe4e38a6140e9f3b623297"
            ],
            "layout": "IPY_MODEL_a465c4f0c25a458b96b62dd0c68d96f7"
          }
        },
        "71b8ba4ccdf2491fabbe44c03528772c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ddc6645c6f44dbc8d0af49a800b5b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_f928f926b8154a8a9e50315adfe9b3b8",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "98dd91b945164dce883f80619bbb98ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64049984318414a9bd2d4620dddf428",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1440373c747b4fe3b73898cd1de3c756",
            "value": 1
          }
        },
        "c24c988cfffe4e38a6140e9f3b623297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a12d074c5680490aaace82abe9813a4c",
            "placeholder": "​",
            "style": "IPY_MODEL_55b5e0ef35624154ac79c38a477a9cbb",
            "value": " 1.65GB / 1.65GB,  134MB/s  "
          }
        },
        "a465c4f0c25a458b96b62dd0c68d96f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ddc6645c6f44dbc8d0af49a800b5b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f928f926b8154a8a9e50315adfe9b3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64049984318414a9bd2d4620dddf428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1440373c747b4fe3b73898cd1de3c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a12d074c5680490aaace82abe9813a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55b5e0ef35624154ac79c38a477a9cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8ae93b74912470495237a7fbe20a518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1268f268191447ea9661b65123da70b8",
              "IPY_MODEL_71ad96da77974c3bbd906609143360ba",
              "IPY_MODEL_5ec9bb51921c4ee6871c0b7d7a401b22"
            ],
            "layout": "IPY_MODEL_ebaa7d1c7e8b412cb9147efc9ffab0d2"
          }
        },
        "1268f268191447ea9661b65123da70b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa6ab1774a7542a1b643cbf2cd95e3c0",
            "placeholder": "​",
            "style": "IPY_MODEL_de8205ce748f4b06b48513094892167a",
            "value": "New Data Upload                         : "
          }
        },
        "71ad96da77974c3bbd906609143360ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf005a9e3474d25bc2c682dd857586a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4503aa8ffff4de69ff247328b3ec65d",
            "value": 0
          }
        },
        "5ec9bb51921c4ee6871c0b7d7a401b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5336822427954df69d8e5d0676b47c73",
            "placeholder": "​",
            "style": "IPY_MODEL_9d89813dace847b695b1bb4449624691",
            "value": "  0.00B /  0.00B,  0.00B/s  "
          }
        },
        "ebaa7d1c7e8b412cb9147efc9ffab0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6ab1774a7542a1b643cbf2cd95e3c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de8205ce748f4b06b48513094892167a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecf005a9e3474d25bc2c682dd857586a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e4503aa8ffff4de69ff247328b3ec65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5336822427954df69d8e5d0676b47c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d89813dace847b695b1bb4449624691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c944135c074b4741859e00d6bdaac0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbd59d13422c436bac71d470b1fd17b5",
              "IPY_MODEL_e2f5fc9839dc4a0889650c51697d1828",
              "IPY_MODEL_7a020e67c06e4b8e963d0a779ab70ae2"
            ],
            "layout": "IPY_MODEL_d0e58731659c453f92179c2dbef6a1d5"
          }
        },
        "dbd59d13422c436bac71d470b1fd17b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9345f4c9c2ad478fa1a56017fae4aac8",
            "placeholder": "​",
            "style": "IPY_MODEL_0c4a21a6c2f945559fe1f762b6e81754",
            "value": "  ...qwen2-1.5b-log-classifier-Q8_0.gguf: 100%"
          }
        },
        "e2f5fc9839dc4a0889650c51697d1828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_509c3a732b8b4b21bbbb2e6b02225f00",
            "max": 1646090400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cba5b423937467baef903dca256a1dd",
            "value": 1646090400
          }
        },
        "7a020e67c06e4b8e963d0a779ab70ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53cccabc2bec4305a5b0255c300172cd",
            "placeholder": "​",
            "style": "IPY_MODEL_aee7d59a7bc34ecfba64c4a6116b859a",
            "value": " 1.65GB / 1.65GB            "
          }
        },
        "d0e58731659c453f92179c2dbef6a1d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9345f4c9c2ad478fa1a56017fae4aac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c4a21a6c2f945559fe1f762b6e81754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "509c3a732b8b4b21bbbb2e6b02225f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cba5b423937467baef903dca256a1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53cccabc2bec4305a5b0255c300172cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee7d59a7bc34ecfba64c4a6116b859a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7658740947614a21aa0b230fbb69c7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_052da233e89343f6b6907019acddbe37",
              "IPY_MODEL_8ab6ddcf242f4690991aa596aed0ccc0",
              "IPY_MODEL_c19c97e9a9c7489cbc7721c48ccbd6ed"
            ],
            "layout": "IPY_MODEL_d92a36756e5749e6895a21b887236dec"
          }
        },
        "052da233e89343f6b6907019acddbe37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b61682bb9d1b465e82635816f2ca4c32",
            "placeholder": "​",
            "style": "IPY_MODEL_d15d3bdaf61940f7831393ce9a34c888",
            "value": "qwen2-1.5b-log-classifier-Q4_K_M.gguf: 100%"
          }
        },
        "8ab6ddcf242f4690991aa596aed0ccc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fde3c46218f4a6d9f5e74069cffa9fc",
            "max": 985673760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_859f046dd7c04c4f87cec15fea9e541e",
            "value": 985673760
          }
        },
        "c19c97e9a9c7489cbc7721c48ccbd6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f2356bab6d420fadf3d92f16848e68",
            "placeholder": "​",
            "style": "IPY_MODEL_aeab7d05b22a444ba63ac6d1011f4794",
            "value": " 986M/986M [00:18&lt;00:00, 107MB/s]"
          }
        },
        "d92a36756e5749e6895a21b887236dec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61682bb9d1b465e82635816f2ca4c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d15d3bdaf61940f7831393ce9a34c888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fde3c46218f4a6d9f5e74069cffa9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859f046dd7c04c4f87cec15fea9e541e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0f2356bab6d420fadf3d92f16848e68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeab7d05b22a444ba63ac6d1011f4794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91d5e6dce5746508c992661842beb7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c781740f15d40e890a60428a1fa047f",
              "IPY_MODEL_32d7ccb857f84414b5b25e65cb5b652b",
              "IPY_MODEL_e5515495e03540aea4c7e933e2d6bbe8"
            ],
            "layout": "IPY_MODEL_38afc26170ca44a9801647cbc32e20b6"
          }
        },
        "8c781740f15d40e890a60428a1fa047f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe142085bcb248079de34f037b89b7bf",
            "placeholder": "​",
            "style": "IPY_MODEL_7cc21eff528547dcbd0797183c90eef3",
            "value": "Generating train split: "
          }
        },
        "32d7ccb857f84414b5b25e65cb5b652b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55826a1429a940eb9b51eb4ced884da3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b1d0d7985ff48ceba69a674167f2f69",
            "value": 1
          }
        },
        "e5515495e03540aea4c7e933e2d6bbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0b0d7bdd254483a32d9abcc51f949a",
            "placeholder": "​",
            "style": "IPY_MODEL_e38e932e1b3e4615bb58e900e964cf22",
            "value": " 10000/0 [00:00&lt;00:00, 84359.51 examples/s]"
          }
        },
        "38afc26170ca44a9801647cbc32e20b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe142085bcb248079de34f037b89b7bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc21eff528547dcbd0797183c90eef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55826a1429a940eb9b51eb4ced884da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6b1d0d7985ff48ceba69a674167f2f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d0b0d7bdd254483a32d9abcc51f949a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38e932e1b3e4615bb58e900e964cf22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11ce012d66ab4b138d2a0caf091b4402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91662a460eab46a4a6aff44c88fe6128",
              "IPY_MODEL_ed2d904efcf14a43b90abfd9a4482395",
              "IPY_MODEL_5c78b1e42f154eb5ab825eddbbbb8b9c"
            ],
            "layout": "IPY_MODEL_8dfb17cdc04841d2a7bcfd289bc32ce0"
          }
        },
        "91662a460eab46a4a6aff44c88fe6128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24454100e7284d9c989ca418bdd0aeb1",
            "placeholder": "​",
            "style": "IPY_MODEL_5a9cd70455a94e61b70bfa9cee9c9b5d",
            "value": "qwen2-1.5b-log-classifier-f16.gguf: 100%"
          }
        },
        "ed2d904efcf14a43b90abfd9a4482395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_888c6f75148a4c83b8368d6303c0d8d4",
            "max": 3092769120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd1e3d5c0bce4ce6b0ebe5f5827c2b61",
            "value": 3092769120
          }
        },
        "5c78b1e42f154eb5ab825eddbbbb8b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dcbdd1ebb944e5fa944d4fb8bd0d93f",
            "placeholder": "​",
            "style": "IPY_MODEL_a6d0476209a84037a931fab5024c0350",
            "value": " 3.09G/3.09G [03:24&lt;00:00, 80.1MB/s]"
          }
        },
        "8dfb17cdc04841d2a7bcfd289bc32ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24454100e7284d9c989ca418bdd0aeb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a9cd70455a94e61b70bfa9cee9c9b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "888c6f75148a4c83b8368d6303c0d8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1e3d5c0bce4ce6b0ebe5f5827c2b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dcbdd1ebb944e5fa944d4fb8bd0d93f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d0476209a84037a931fab5024c0350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}