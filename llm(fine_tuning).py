# -*- coding: utf-8 -*-
"""LLM(Fine-tuning).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U16h_8wwbYw01pTbuJWuXS6YNwluuH6k
"""

!git clone -b LLMtest https://github.com/deeps-03/SentinelLLM.git

# Commented out IPython magic to ensure Python compatibility.
# %cd SentinelLLM

!pip install -r LLMlogs/requirements.txt

!pip install bitsandbytes

!pip install transformers==4.42.4
!pip install datasets==2.20.0
!pip install peft==0.11.1
!pip install trl==0.9.6
!pip install accelerate==0.33.0
!pip install huggingface_hub

!python LLMlogs/fine_tune.py

# Commented out IPython magic to ensure Python compatibility.
# ‚úÖ Install requirements
!apt-get -qq install cmake build-essential git-lfs
!git lfs install

# ‚úÖ Clone your fine-tuned model repo
!git clone https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier
# %cd qwen2-1.5b-log-classifier

# ‚úÖ Clone llama.cpp (latest)
# %cd /content
!git clone https://github.com/ggml-org/llama.cpp
# %cd llama.cpp

# ‚úÖ Build llama.cpp
!cmake -B build
!cmake --build build -j

# ‚úÖ Convert Hugging Face model ‚Üí GGUF (FP16)
!python3 convert_hf_to_gguf.py \
    /content/qwen2-1.5b-log-classifier \
    --outfile /content/qwen2-1.5b-log-classifier-f16.gguf \
    --model-type qwen2

# ‚úÖ Quantize model (Q4_K_M)
!./build/bin/llama-quantize \
    /content/qwen2-1.5b-log-classifier-f16.gguf \
    /content/qwen2-1.5b-log-classifier-Q4_K_M.gguf \
    Q4_K_M

# ‚úÖ Organize files inside repo
import os, shutil
os.makedirs("/content/qwen2-1.5b-log-classifier/gguf", exist_ok=True)
shutil.move("/content/qwen2-1.5b-log-classifier-f16.gguf", "/content/qwen2-1.5b-log-classifier/gguf/")
shutil.move("/content/qwen2-1.5b-log-classifier-Q4_K_M.gguf", "/content/qwen2-1.5b-log-classifier/gguf/")

# ‚úÖ Push GGUF files back to Hugging Face
# %cd /content/qwen2-1.5b-log-classifier
!git lfs track "*.gguf"
!git add .
!git commit -m "Added GGUF (f16 + Q4_K_M) versions"
!git push

!pip install mistral_common

!cd /content/llama.cpp && python3 convert_hf_to_gguf.py /content/qwen2-1.5b-log-classifier --outfile /content/qwen2-1.5b-log-classifier-f16.gguf

!./content/llama.cpp/build/bin/llama-quantize /content/qwen2-1.5b-log-classifier-f16.gguf /content/qwen2-1.5b-log-classifier-Q4_K_M.gguf Q4_K_M
!./content/llama.cpp/build/bin/llama-quantize /content/qwen2-1.5b-log-classifier-f16.gguf /content/qwen2-1.5b-log-classifier-Q5_K_M.gguf Q5_K_M

!mkdir -p /content/qwen2-1.5b-log-classifier/gguf
!mv /content/qwen2-1.5b-log-classifier-f16.gguf /content/qwen2-1.5b-log-classifier/gguf/
!mv /content/qwen2-1.5b-log-classifier-Q4_K_M.gguf /content/qwen2-1.5b-log-classifier/gguf/
!mv /content/qwen2-1.5b-log-classifier-Q5_K_M.gguf /content/qwen2-1.5b-log-classifier/gguf/

!cd /content/qwen2-1.5b-log-classifier && git lfs track "*.gguf" && git add . && git commit -m "Added GGUF models" && git push

from huggingface_hub import login
login()

import os

for root, dirs, files in os.walk("/content"):
    for f in files:
        if f.endswith(".gguf"):
            print(os.path.join(root, f))

from huggingface_hub import login, upload_file

# Login with your token
login("hf token")

# Path to your model file
local_file = "/content/qwen2-1.5b-log-classifier/gguf/qwen2-1.5b-log-classifier-f16.gguf"
path_in_repo = "qwen2-1.5b-log-classifier-f16.gguf"  # filename on HF

# Upload the file
upload_file(
    path_or_fileobj=local_file,
    path_in_repo=path_in_repo,
    repo_id="Deeps03/qwen2-1.5b-log-classifier",
    repo_type="model"
)

print("‚úÖ Upload complete!")

# Commented out IPython magic to ensure Python compatibility.
# Install git-lfs to handle large files
!apt-get install -y git-lfs
!git lfs install

# Clone your repo
!git clone https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier
# %cd qwen2-1.5b-log-classifier

# Verify your f16 file is there
ls -lh

# Commented out IPython magic to ensure Python compatibility.
# Go to /content
# %cd /content

# Clone llama.cpp if not already there
!git clone https://github.com/ggerganov/llama.cpp.git
# %cd llama.cpp

# Build llama.cpp with CMake
!cmake -B build
!cmake --build build -j

# Now quantize your f16 model into Q4_K_M
!./build/bin/llama-quantize \
    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf \
    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q4_K_M.gguf Q4_K_M

# And into Q5_K_M
!./build/bin/llama-quantize \
    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-f16.gguf \
    /content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q5_K_M.gguf Q5_K_M

from huggingface_hub import upload_file

token = "hf token"
repo_id = "Deeps03/qwen2-1.5b-log-classifier"

files_to_upload = [
    "/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q4_K_M.gguf",
    "/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q5_K_M.gguf"
]

for file_path in files_to_upload:
    upload_file(
        path_or_fileobj=file_path,
        path_in_repo="gguf/" + file_path.split("/")[-1],
        repo_id=repo_id,
        repo_type="model",
        token=token,
        commit_message=f"Added {file_path.split('/')[-1]} quantized version"
    )
    print(f"‚úÖ Uploaded {file_path}")

# Commented out IPython magic to ensure Python compatibility.
# === Step 0: Reset environment ===
# %cd /content
!rm -rf llama.cpp qwen2-1.5b-log-classifier

# === Step 1: Clone your repo with f16 file ===
!git lfs install
!git clone https://huggingface.co/Deeps03/qwen2-1.5b-log-classifier

# === Step 2: Clone llama.cpp and build ===
!git clone https://github.com/ggerganov/llama.cpp.git
# %cd /content/llama.cpp
!mkdir build
# %cd build
!cmake ..
!cmake --build . --config Release -j 8

# === Step 3: Go back to your repo folder ===
# %cd /content/qwen2-1.5b-log-classifier

# === Step 4: Quantize Q8_0 ===
!/content/llama.cpp/build/bin/llama-quantize \
    qwen2-1.5b-log-classifier-f16.gguf \
    qwen2-1.5b-log-classifier-Q8_0.gguf \
    Q8_0

# === Step 5: Configure Git identity ===
!git config --global user.email "deepaksuresh698@gmail.com"
!git config --global user.name "Deepak"

# === Step 6: Push quantized model to HF ===
!git lfs track "*.gguf"
!git add qwen2-1.5b-log-classifier-Q8_0.gguf
!git commit -m "Add Q8_0 quantization"
!git push

from huggingface_hub import HfApi, HfFolder, upload_file

# Your repo and access token
repo_id = "Deeps03/qwen2-1.5b-log-classifier"
token = "hf token"

# Save the token
HfFolder.save_token(token)
api = HfApi()

# Local file path (make sure it exists after quantization)
local_file = "/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q8_0.gguf"

# Path inside repo
path_in_repo = "gguf/qwen2-1.5b-log-classifier-Q8_0.gguf"

# Upload file to repo
upload_file(
    path_or_fileobj=local_file,
    path_in_repo=path_in_repo,
    repo_id=repo_id,
    token=token
)

print(f"‚úÖ Uploaded {local_file} to https://huggingface.co/{repo_id}/blob/main/{path_in_repo}")

from huggingface_hub import HfApi, HfFolder, upload_file, delete_folder
import os

repo_id = "Deeps03/qwen2-1.5b-log-classifier"
token = "hf token"

HfFolder.save_token(token)
api = HfApi()

gguf_dir = "/content/qwen2-1.5b-log-classifier/gguf"
for f in os.listdir(gguf_dir):
    local_path = os.path.join(gguf_dir, f)
    upload_file(
        path_or_fileobj=local_path,
        path_in_repo=f,  # root of repo
        repo_id=repo_id,
        token=token
    )
    print(f"‚úÖ Uploaded {f} to root of {repo_id}")

# Delete gguf folder from repo
delete_folder(
    repo_id=repo_id,
    path_in_repo="gguf",
    token=token,
    commit_message="Remove gguf folder after moving files to root"
)
print("üóëÔ∏è Removed old gguf folder")

from huggingface_hub import upload_file

repo_id = "Deeps03/qwen2-1.5b-log-classifier"
token = "hf token"

# Local path of your file in root
local_file = "/content/qwen2-1.5b-log-classifier/qwen2-1.5b-log-classifier-Q8_0.gguf"

upload_file(
    path_or_fileobj=local_file,
    path_in_repo="qwen2-1.5b-log-classifier-Q8_0.gguf",  # filename in repo root
    repo_id=repo_id,
    token=token
)

print("‚úÖ Uploaded qwen2-1.5b-log-classifier-Q8_0.gguf to repo root")

!pip install transformers datasets scikit-learn

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_name = "Deeps03/qwen2-1.5b-log-classifier"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",        # handles GPU/CPU automatically
    torch_dtype=torch.float16 # saves VRAM
)

# No `device=` here
clf_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer
)

!pip install llama-cpp-python huggingface_hub datasets scikit-learn matplotlib

import random
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from datasets import load_dataset
from llama_cpp import Llama
from huggingface_hub import hf_hub_download

# ----------------------------
# 1. Download GGUF model from Hugging Face
# ----------------------------
model_path = hf_hub_download(
    repo_id="Deeps03/qwen2-1.5b-log-classifier",  # your repo
    filename="qwen2-1.5b-log-classifier-Q4_K_M.gguf"  # pick Q4, Q5, or Q8
)

llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,
    n_ctx=4096,
    seed=42
)

# ----------------------------
# 2. Load dataset (small subset)
# ----------------------------
dataset = load_dataset("json", data_files="/content/generated_logs.jsonl")
test_data = dataset["train"].train_test_split(test_size=0.2, seed=42)["test"]

small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs

# ----------------------------
# 3. Inference loop
# ----------------------------
y_true, y_pred = [], []

for ex in small_test:
    prompt = f"""<|im_start|>system
You are an expert log analysis assistant. Your task is to classify log messages into one of:
'incident', 'preventive_action', or 'normal'.
Provide only the classification word.
<|im_end|>
<|im_start|>user
Classify the following log message:
Log Entry: {ex['message']}
<|im_end|>
<|im_start|>assistant
"""

    output = llm(
        prompt,
        max_tokens=5,
        stop=["<|im_end|>", "\n"],
        echo=False
    )

    # Extract model response
    pred = output["choices"][0]["text"].strip().lower()

    # fallback if model returns junk
    if pred not in ["incident", "preventive_action", "normal"]:
        pred = "normal"

    y_pred.append(pred)
    y_true.append(ex["label"].lower())

# ----------------------------
# 4. Confusion Matrix
# ----------------------------
labels = ["incident", "preventive_action", "normal"]
cm = confusion_matrix(y_true, y_pred, labels=labels)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap="Blues", xticks_rotation=45)
plt.title("Confusion Matrix - Qwen2 Log Classifier (Q4_K_M)")
plt.show()

import random
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from datasets import load_dataset
from llama_cpp import Llama
from huggingface_hub import hf_hub_download

# ----------------------------
# 1. Download GGUF model from Hugging Face
# ----------------------------
model_path = hf_hub_download(
    repo_id="Deeps03/qwen2-1.5b-log-classifier",  # your repo
    filename="qwen2-1.5b-log-classifier-Q5_K_M.gguf"  # pick Q4, Q5, or Q8
)

llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,
    n_ctx=4096,
    seed=42
)

# ----------------------------
# 2. Load dataset (small subset)
# ----------------------------
dataset = load_dataset("json", data_files="/content/generated_logs.jsonl")
test_data = dataset["train"].train_test_split(test_size=0.2, seed=42)["test"]

small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs

# ----------------------------
# 3. Inference loop
# ----------------------------
y_true, y_pred = [], []

for ex in small_test:
    prompt = f"""<|im_start|>system
You are an expert log analysis assistant. Your task is to classify log messages into one of:
'incident', 'preventive_action', or 'normal'.
Provide only the classification word.
<|im_end|>
<|im_start|>user
Classify the following log message:
Log Entry: {ex['message']}
<|im_end|>
<|im_start|>assistant
"""

    output = llm(
        prompt,
        max_tokens=5,
        stop=["<|im_end|>", "\n"],
        echo=False
    )

    # Extract model response
    pred = output["choices"][0]["text"].strip().lower()

    # fallback if model returns junk
    if pred not in ["incident", "preventive_action", "normal"]:
        pred = "normal"

    y_pred.append(pred)
    y_true.append(ex["label"].lower())

# ----------------------------
# 4. Confusion Matrix
# ----------------------------
labels = ["incident", "preventive_action", "normal"]
cm = confusion_matrix(y_true, y_pred, labels=labels)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap="Blues", xticks_rotation=45)
plt.title("Confusion Matrix - Qwen2 Log Classifier (Q5_K_M)")
plt.show()

import random
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from datasets import load_dataset
from llama_cpp import Llama
from huggingface_hub import hf_hub_download

# ----------------------------
# 1. Download GGUF model from Hugging Face
# ----------------------------
model_path = hf_hub_download(
    repo_id="Deeps03/qwen2-1.5b-log-classifier",  # your repo
    filename="qwen2-1.5b-log-classifier-Q8_0.gguf"  # pick Q4, Q5, or Q8
)

llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,
    n_ctx=4096,
    seed=42
)

# ----------------------------
# 2. Load dataset (small subset)
# ----------------------------
dataset = load_dataset("json", data_files="/content/generated_logs.jsonl")
test_data = dataset["train"].train_test_split(test_size=0.2, seed=42)["test"]

small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs

# ----------------------------
# 3. Inference loop
# ----------------------------
y_true, y_pred = [], []

for ex in small_test:
    prompt = f"""<|im_start|>system
You are an expert log analysis assistant. Your task is to classify log messages into one of:
'incident', 'preventive_action', or 'normal'.
Provide only the classification word.
<|im_end|>
<|im_start|>user
Classify the following log message:
Log Entry: {ex['message']}
<|im_end|>
<|im_start|>assistant
"""

    output = llm(
        prompt,
        max_tokens=5,
        stop=["<|im_end|>", "\n"],
        echo=False
    )

    # Extract model response
    pred = output["choices"][0]["text"].strip().lower()

    # fallback if model returns junk
    if pred not in ["incident", "preventive_action", "normal"]:
        pred = "normal"

    y_pred.append(pred)
    y_true.append(ex["label"].lower())

# ----------------------------
# 4. Confusion Matrix
# ----------------------------
labels = ["incident", "preventive_action", "normal"]
cm = confusion_matrix(y_true, y_pred, labels=labels)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap="Blues", xticks_rotation=45)
plt.title("Confusion Matrix - Qwen2 Log Classifier (Q8_0)")
plt.show()

import random
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from datasets import load_dataset
from llama_cpp import Llama
from huggingface_hub import hf_hub_download

# ----------------------------
# 1. Download GGUF model from Hugging Face
# ----------------------------
model_path = hf_hub_download(
    repo_id="Deeps03/qwen2-1.5b-log-classifier",  # your repo
    filename="qwen2-1.5b-log-classifier-f16.gguf"  # pick Q4, Q5, or Q8
)

llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,
    n_ctx=4096,
    seed=42
)

# ----------------------------
# 2. Load dataset (small subset)
# ----------------------------
dataset = load_dataset("json", data_files="/content/generated_logs.jsonl")
test_data = dataset["train"].train_test_split(test_size=0.2, seed=42)["test"]

small_test = test_data.shuffle(seed=42).select(range(500))  # 500 logs

# ----------------------------
# 3. Inference loop
# ----------------------------
y_true, y_pred = [], []

for ex in small_test:
    prompt = f"""<|im_start|>system
You are an expert log analysis assistant. Your task is to classify log messages into one of:
'incident', 'preventive_action', or 'normal'.
Provide only the classification word.
<|im_end|>
<|im_start|>user
Classify the following log message:
Log Entry: {ex['message']}
<|im_end|>
<|im_start|>assistant
"""

    output = llm(
        prompt,
        max_tokens=5,
        stop=["<|im_end|>", "\n"],
        echo=False
    )

    # Extract model response
    pred = output["choices"][0]["text"].strip().lower()

    # fallback if model returns junk
    if pred not in ["incident", "preventive_action", "normal"]:
        pred = "normal"

    y_pred.append(pred)
    y_true.append(ex["label"].lower())

# ----------------------------
# 4. Confusion Matrix
# ----------------------------
labels = ["incident", "preventive_action", "normal"]
cm = confusion_matrix(y_true, y_pred, labels=labels)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap="Blues", xticks_rotation=45)
plt.title("Confusion Matrix - Qwen2 Log Classifier (F16)")
plt.show()